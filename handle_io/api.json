[
  {
    "title": "Using qKnowledgeGradient with SaasFullyBayesianSingleTaskGP gives shape mismatch error",
    "link": "https://stackoverflow.com/questions/79716022/using-qknowledgegradient-with-saasfullybayesiansingletaskgp-gives-shape-mismatch",
    "tags": [
      "python",
      "debugging",
      "data-science",
      "bayesian",
      "botorch"
    ],
    "body": "<p>I\u2019m trying to use qKnowledgeGradient with a fully Bayesian SAAS (Sparse Axis Aligned Subspace) GP  (Gaussian Process) (using SaasFullyBayesianSingleTaskGP) in BoTorch. I'm doing so by writing a new class that inherits from both SaasFullyBayesianSingleTaskGP and FantasizeMixin. Then, I override the fantasize() method to define how fantasy data is generated for this model. I have used 256 samples, warmup of 512, thinning of 1, num_fantasies=2. However, on running the code, I keep getting a shape mismatch error even with raw_samples=1 and num_restarts=1. The error looks like this:</p>\n<pre><code>RuntimeError: shape '[2, 1, 16, 1]' is invalid for input of size 64\n</code></pre>\n<p>I created a custom SAAS GP by inheriting from both SaasFullyBayesianSingleTaskGP and FantasizeMixin, and I overrode the fantasize() method.  I then attempted to use this model with qKnowledgeGradient by setting num_fantasies=2 and reducing raw_samples and num_restarts to 1 (so only a single t\u2011batch is used).  I expected the acquisition to evaluate successfully and produce a candidate point, but instead, KG fails with the broadcast/reshape error above.</p>\n<p>The error occurs regardless of whether I use the default KG (Knowledge Gradient) implementation or a custom KG that loops over the batch dimension and manually averages over the ensemble and I haven\u2019t been able to eliminate it by changing collapsing batch dimensions also. I even printed the tensor dimension and seems okay to me.</p>\n<p>Below is a minimal version of my code to reproduce the issue.</p>\n<p>Reproducible Minimal Example\nBranin function embedded in 100D</p>\n<pre><code>lb = np.hstack((-5 * np.ones(50), 0 * np.ones(50)))\nub = np.hstack((10 * np.ones(50), 15 * np.ones(50)))\n\ndef branin100(x):\n    assert (x &lt;= ub).all() and (x &gt;= lb).all()\n    x1, x2 = x[19], x[64]\n    t1 = x2 - 5.1 / (4 * math.pi ** 2) * x1 ** 2 + 5 / math.pi * x1 - 6\n    t2 = 10 * (1 - 1 / (8 * math.pi)) * np.cos(x1)\n    return t1 ** 2 + t2 + 10\n</code></pre>\n<p>SAAS GP with custom fantasize() method</p>\n<pre><code>class SaasFullyBayesianSingleTaskGPWithFantasy(SaasFullyBayesianSingleTaskGP, FantasizeMixin):\n    def fantasize(\n        self,\n        X: torch.Tensor,\n        sampler: Optional[MCSampler] = None,\n        num_fantasies: int = 2,\n        **kwargs,\n    ) -&gt; Model:\n        if sampler is None:\n            sampler = SobolQMCNormalSampler(\n                sample_shape=torch.Size([num_fantasies]),\n                collapse_batch_dims=True,\n            )\n        X = torch.as_tensor(\n            X, dtype=self.train_inputs[0].dtype, device=self.train_inputs[0].device\n        )\n        return FantasizeMixin.fantasize(self, X, sampler=sampler, **kwargs)\n</code></pre>\n<p>Running SAASBO with KG</p>\n<pre><code>\ndef run_saasbo_botorch():\n    torch.manual_seed(0)\n    dtype = torch.double\n    device = &quot;cpu&quot;\n    dim = 100\n    lb_torch = torch.zeros(dim, dtype=dtype)\n    ub_torch = torch.ones(dim, dtype=dtype)\n    bounds = torch.stack([lb_torch, ub_torch])\n\n    def f(x): return branin100(x)\n\n    # Initial Sobol samples\n    sobol = SobolEngine(dim, scramble=True, seed=0)\n    X = sobol.draw(4).to(dtype=dtype)  # 4 initial points\n    Y = torch.tensor(\n        [f(lb + (ub - lb) * x.cpu().numpy()) for x in X],\n        dtype=dtype\n    ).unsqueeze(-1)\n\n    train_Y = (Y - Y.mean()) / Y.std()\n\n    # Fit SAAS GP\n    model = SaasFullyBayesianSingleTaskGPWithFantasy(X, train_Y)\n    fit_fully_bayesian_model_nuts(\n        model, warmup_steps=512, num_samples=256, thinning=16\n    )\n\n    # Define posterior transform\n    weights = torch.ones(2, dtype=dtype) / 2\n    post_tf = ScalarizedPosteriorTransform(weights=weights)\n\n    # Define KG acquisition\n    qkg = qKnowledgeGradient(\n        model=model,\n        num_fantasies=2,\n        current_value=train_Y.min(),\n        posterior_transform=post_tf,\n    )\n\n    # Optimize acquisition\n    candidate, _ = optimize_acqf(\n        acq_function=qkg,\n        bounds=bounds,\n        q=1,\n        raw_samples=1,\n        num_restarts=1,\n    )\n\nrun_saasbo_botorch()\n</code></pre>\n<p>Error</p>\n<pre><code>RuntimeError: shape '[2, 1, 16, 1]' is invalid for input of size 64\n</code></pre>\n<p>I don't understand why I keep getting this error and where it is coming from. Any guidance on what might be causing this and how to properly structure the fantasy model in this context would be greatly appreciated!</p>\n<p>Thanks in advance.</p>\n<p>EDIT: I overrode condition_on_observations and changed num fantasies to 64,( code below ) but now I get another error :</p>\n<pre><code>Output shape not equal to that of weights. Output shape is 1 and weights are torch.Size([64]\n</code></pre>\n<p>Code for condition_on_observation -:</p>\n<pre><code>def condition_on_observations(self, X: torch.Tensor, Y: torch.Tensor, **kwargs):\nmodel_batch_ndim = len(self.batch_shape)\n\n    if X.ndim == 2 and Y.ndim == 2:\n        X = X.repeat(self.batch_shape + (1, 1)).contiguous()\n        Y = Y.repeat(self.batch_shape + (1, 1)).contiguous()\n        return super().condition_on_observations(X, Y, **kwargs)\n\n    \n    start_idx = Y.ndim - (2 + model_batch_ndim)\n    model_batch_indices = list(range(start_idx, start_idx + model_batch_ndim))\n    extra_indices = list(range(0, start_idx))\n    remaining_indices = list(range(start_idx + model_batch_ndim, Y.ndim - 2))\n    permute_order = model_batch_indices + extra_indices + remaining_indices + [Y.ndim - 2, Y.ndim - 1]\n    Y_perm = Y.permute(*permute_order).contiguous()\n    \n\n    \n    if X.shape[:model_batch_ndim] != self.batch_shape:\n        X = X.expand(self.batch_shape + X.shape[-2:])  \n\n   \n    extra_dims = len(extra_indices) + len(remaining_indices)\n    for _ in range(extra_dims):\n        X = X.unsqueeze(model_batch_ndim)\n\n    \n    expand_shape = list(X.shape)\n    for i in range(extra_dims):\n        expand_shape[model_batch_ndim + i] = Y_perm.shape[model_batch_ndim + i]\n    X_expanded = X.expand(*expand_shape).contiguous()\n\n    \n    flat_size = int(torch.tensor(Y_perm.shape[model_batch_ndim:-1]).prod())\n    X_flat = X_expanded.reshape(*X_expanded.shape[:model_batch_ndim], flat_size, X_expanded.shape[-1]).clone()\n    Y_flat = Y_perm.reshape(*Y_perm.shape[:model_batch_ndim], flat_size, Y_perm.shape[-1]).clone()\n\n    return super().condition_on_observations(X_flat, Y_flat, **kwargs)\n</code></pre>\n",
    "is_answered": false,
    "view_count": 88,
    "answer_count": 0
  },
  {
    "title": "Stratefied vs Random Splitting on highly categotical datasets",
    "link": "https://stackoverflow.com/questions/79694850/stratefied-vs-random-splitting-on-highly-categotical-datasets",
    "tags": [
      "machine-learning",
      "split",
      "data-science"
    ],
    "body": "<p>I am working on a machine learning model on a survey dataset with highly categorical dataset, each feature <strong>(12 features)</strong> has been bucketized very sensitively depending on the results and domain intuition.</p>\n<p>I am spending time on the dataset splitting decision. I think stratefied splitting would be good to ensure equal distribution of categorical variables. Although the number of featuresa are high so I'd like to hear some input on whether Stratified splitting is best or normal train test split would do.</p>\n<p>Haven't tried it anything yet, I am doing some research and circling back to stratefied sampling concepts in ML books.</p>\n",
    "is_answered": true,
    "view_count": 38,
    "answer_count": 1
  },
  {
    "title": "In &quot;statsmodels&quot; Python library, why does z-test not require Population standard deviation?",
    "link": "https://stackoverflow.com/questions/79693109/in-statsmodels-python-library-why-does-z-test-not-require-population-standard",
    "tags": [
      "python",
      "statistics",
      "data-science",
      "statsmodels",
      "python-hypothesis"
    ],
    "body": "<p>So to execute a z-test in python, I have seen people doing:</p>\n<pre><code>from statsmodels.stats.weightstats import ztest as ztest\n# IQ levels of 20 patients post medication\ndata = [88, 92, 94, 94, 96, 97, 97, 97, 99, 99,\n            105, 109, 109, 109, 110, 112, 112, 113, 114, 115]\n\nztest(data, value=100) # value is mean\n</code></pre>\n<p>Why is the Population standard deviation (std) not required in the argument? How does the library apply the z-test formula without getting the population std?</p>\n",
    "is_answered": true,
    "view_count": 74,
    "answer_count": 2
  },
  {
    "title": "Sharing Stored Cache After Reading CSV&#39;s List Using Targets in R",
    "link": "https://stackoverflow.com/questions/79687409/sharing-stored-cache-after-reading-csvs-list-using-targets-in-r",
    "tags": [
      "r",
      "data-science",
      "targets-r-package"
    ],
    "body": "<p>I have 760 large csv files on which i perform different algorithms on each one through my projects.</p>\n<p>Until now i performed the algorithms with Future_map from the furrr library in R.\nI started to work with the Targets library in R, and i want to utilize this library to load faster the large csv's files (they are remained unchanged).</p>\n<p>i created a project on which i just read the files (here ReadData is just a custom function which uses fread from data.table)</p>\n<pre><code>    list(\n  tar_files_input(csv_files ,list.files(dirPath , full.names = T) ,\n             format = &quot;file&quot;),\n  tar_target(DataTables , ReadData(csv_files) , pattern = map(csv_files) , error = &quot;continue&quot;) \n)\n</code></pre>\n<p>This code created 761 files in the objects directory under the _targets directory, 1 file of the csv_files_files and another 760 files with the DataTables files, each with its own unique ID appended to the name.</p>\n<p>I tried in a different project to read those files (after changing the store to _targets path), however i cant find the way to do so. i tried tar_read, tar_files , tar_target with the algorithm in the next stage in the pipeline and pattern with map on the files, but i cant get the correct way to do so, for example:</p>\n<pre><code>list(\n  tar_target(\n    Data,\n    tar_read(DataTables, store = _TargetsPath ),\n    pattern = map(DataTables),\n  ),\n  tar_target(\n    AlgoOutput,\n    MyAlgo(Data),\n    pattern = map(Data)\n  )\n)\n</code></pre>\n<p>Anyone knows how to use another projects csv's files after reading them using targets in order to read them faster in another project?</p>\n<p>Thanks</p>\n",
    "is_answered": false,
    "view_count": 52,
    "answer_count": 0
  },
  {
    "title": "Identify a column cell and perform subtraction with the same column",
    "link": "https://stackoverflow.com/questions/79684073/identify-a-column-cell-and-perform-subtraction-with-the-same-column",
    "tags": [
      "python",
      "pandas",
      "data-science"
    ],
    "body": "<p>This is just a small example DataFrame, but in reality, I\u2019m working with a much larger and uneven dataset. I need to identify the data in pattern to perform subtraction within the same column. Please take a look at my attempt and the expected result below.</p>\n<pre><code>df = pd.DataFrame([{'col1': &quot;&quot;, 'col2': &quot;&quot;}, {'col1': 10.0, 'col2': 'A'}, {'col1': 20.0, 'col2': 'D'}, {'col1': &quot;&quot;, 'col2': &quot;&quot;}, {'col1': &quot;&quot;, 'col2': &quot;&quot;}, {'col1': &quot;&quot;, 'col2': &quot;&quot;}, {'col1': 40.0, 'col2': 'W'}, {'col1': 10.0, 'col2': 'E'}, {'col1': 15.0, 'col2': 'R'}, {'col1': &quot;&quot;, 'col2': &quot;&quot;}, {'col1': &quot;&quot;, 'col2': &quot;&quot;}, {'col1': 5.0, 'col2': 'F'}, {'col1': 10.0, 'col2': 'H'}, {'col1': 15.0, 'col2': 'U'}, {'col1': 11.0, 'col2': 'T'}])\n\nprint (df)\n\n    col1 col2\n0    NaN  NaN\n1   10.0    A\n2   20.0    D\n3    NaN  NaN\n4    NaN  NaN\n5    NaN  NaN\n6    2.0    W\n7   10.0    E\n8   15.0    R\n9    NaN  NaN\n10   NaN  NaN\n11   5.0    F\n12  10.0    H\n13   9.0    U\n14  11.0    T\n</code></pre>\n<p>Need to identify the data to perform subtraction as shown below.\nAdditional Column &quot;IdentifySub&quot;</p>\n<pre><code>    col1 col2 IdentifySub\n0    NaN  NaN         NaN\n1   10.0    A         1.0\n2   20.0    D         2.0\n3    NaN  NaN         NaN\n4    NaN  NaN         NaN\n5    NaN  NaN         NaN\n6    2.0    W         1.0\n7   10.0    E         2.0\n8   15.0    R         3.0\n9    NaN  NaN         NaN\n10   NaN  NaN         NaN\n11   5.0    F         1.0\n12  10.0    H         2.0\n13   9.0    U         3.0\n14  11.0    T         4.0\n</code></pre>\n<p>Perform Subtraction from even to odd number only.</p>\n<pre><code>    col1 col2 IdentifySub  Result\n0    NaN  NaN         NaN     NaN\n1   10.0    A         1.0     NaN\n2   20.0    D         2.0    10.0\n3    NaN  NaN         NaN     NaN\n4    NaN  NaN         NaN     NaN\n5    NaN  NaN         NaN     NaN\n6    2.0    W         1.0     NaN\n7   10.0    E         2.0     8.0\n8   15.0    R         3.0     NaN\n9    NaN  NaN         NaN     NaN\n10   NaN  NaN         NaN     NaN\n11   5.0    F         1.0     NaN\n12  10.0    H         2.0     5.0\n13   9.0    U         3.0     NaN\n14  11.0    T         4.0     2.0\n\n</code></pre>\n<p>Please see image to understand math of output:</p>\n<p><a href=\"https://i.sstatic.net/UDGUO6mE.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/UDGUO6mE.png\" alt=\"\" /></a></p>\n",
    "is_answered": true,
    "view_count": 71,
    "answer_count": 1
  },
  {
    "title": "Normalizing large free text input data",
    "link": "https://stackoverflow.com/questions/79681718/normalizing-large-free-text-input-data",
    "tags": [
      "python",
      "machine-learning",
      "nlp",
      "data-science",
      "spell-checking"
    ],
    "body": "<p>Here is the case, i have data coming from free text input source, it is related to the degree and education level of the users\nThere are many typos, many crazy stuff going on in the data, there are over 10m records, the data could be anything from typos, some their industry, some have their GPA, some even their tech stack, some data coming in English, some in Spanish</p>\n<p>I want to categorize the data by adding a new column with the official degree name to each line</p>\n<p>This is sample data</p>\n<pre><code>id;name;cluster;;\n1;Licenciatura direccion y administracion de empresas, Auditing;99;;\n2;Lda. en Traducci\u00f3n e Interpretaci\u00f3n;99;;\n3;Licence, Communication, g\u00e9n\u00e9ral, Major de promo 2018 - Cambridge C1 - DELE B2 - Russe A2 (Institut Poushkine);99;;\n4;T\u00e9cnico en Marketing y Comercio Internacional, Negocios internacionales/Comercio internacional;99;;\n5;Curso Monogr\u00e1fico sobre Impuesto de Sociedades., Derecho financiero y tributario;99;;\n6;M\u00c0STER LIDERATGE I GESTI\u00d3 CURES INFERMERIA. LINEA INVESTIGADORA., CI\u00c8NCIES DE LA SALUT.;99;;\n7;M\u00c0STER EN GESTI\u00d3 I ADMINISTRACI\u00d3 CURES INFERMERIA, CI\u00c8NCIES DE LA SALUT;99;;\n8;Grado en Interpretaci\u00f3n Musical, Music;99;;\n9;M\u00f3dulo Superior Educaci\u00f3n Infantil, Educaci\u00f3n infantil, Notable;99;;\n10;Nivel 1 Entrenador f\u00fatbol sala, Deportes y ejercicio f\u00edsico;99;;\n11;Formado en Ingenieria Industrial, Ingenier\u00eda industrial;99;;\n12;C.E.S-I y C.E.S-II, Educaci\u00f3n secundaria;99;;\n13;M\u00f3dulo Superior Educaci\u00f3n Infantil, Early Childhood Education and Teaching, Notable;99;;\n14;Nivel 1 Entrenador f\u00fatbol sala, Sports and Exercise;99;;\n15;Formado en Ingenieria Industrial, Industrial Engineering;99;;\n16;C.E.S-I y C.E.S-II, Secondary Education and Teaching;99;;\n17;M\u00e1ster profesional \u201cGesti\u00f3n y Control de la Seguridad en la Industria Alimentaria\u201d, Seguridad alimentaria;99;;\n18;Master, Sociedad de la Informacion y el Conocimiento;99;;\n</code></pre>\n<p>The goal output is:</p>\n<pre><code>id;name;cluster;category\n1;Licenciatura direccion y administracion de empresas, Auditing;99;licence in business administration;\n2;Lda. en Traducci\u00f3n e Interpretaci\u00f3n;99;licence in translation;\n3;Licence, Communication, g\u00e9n\u00e9ral, Major de promo 2018 - Cambridge C1 - DELE B2 - Russe A2 (Institut Poushkine);99;Major in communication;\n4;T\u00e9cnico en Marketing y Comercio Internacional, Negocios internacionales/Comercio internacional;99;Degree in marketing;\n5;Curso Monogr\u00e1fico sobre Impuesto de Sociedades., Derecho financiero y tributario;99;course in monography;\n6;M\u00c0STER LIDERATGE I GESTI\u00d3 CURES INFERMERIA. LINEA INVESTIGADORA., CI\u00c8NCIES DE LA SALUT.;99;masters in leaderchip;\n7;M\u00c0STER EN GESTI\u00d3 I ADMINISTRACI\u00d3 CURES INFERMERIA, CI\u00c8NCIES DE LA SALUT;99;master in business administration;\n</code></pre>\n",
    "is_answered": true,
    "view_count": 86,
    "answer_count": 1
  },
  {
    "title": "Randomized Search CV with Neural Network",
    "link": "https://stackoverflow.com/questions/79669291/randomized-search-cv-with-neural-network",
    "tags": [
      "python",
      "machine-learning",
      "deep-learning",
      "neural-network",
      "data-science"
    ],
    "body": "<p>I want to tune a neural network but since i would be doing a lot of test runs i decided to use Randomized search CV</p>\n<p>I first started by freeing up some space.</p>\n<pre><code># Reseting layers and freeing up some space\ntf.keras.backend.clear_session()\n</code></pre>\n<p>I then created a function to hold my neural network model.</p>\n<pre><code>def create_model(learning_rate=0.02):\n  model = Sequential([\n    Dense(128, activation='relu', input_shape=(23,)), #Input Layer\n    Dense(64, activation='relu'), #Hidden Layer\n    Dense(7, activation='softmax') #Output Layer\n    ])\n  # Compiling Model\n  from tensorflow.keras.optimizers import Adam\n  model.compile(optimizer=Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy', map_3])\n  \n  return model\n</code></pre>\n<p>I wanted an early stopping parameter so I created a variable to hold that.</p>\n<pre><code># Early Stopping\nfrom tensorflow.keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor = 'val_map_3',\n                               patience = 5,\n                               mode = 'max',\n                               restore_best_weights=True)\n</code></pre>\n<p>I then define my parameter dictionary for my Randomized Search CV.</p>\n<pre><code># Setting Parameters for Randomized Search CV\nparam_dict = {'epochs': [20, 40, 60, 80, 100, 120, 150],\n              'batch_size': [32, 64, 128, 256, 512, 1024],\n              'learning_rate': [0.04, 0.06, 0.08, 0.1, 0.2, 0.5, 1.0, 2.0]}\n</code></pre>\n<p>I used Scikeras to wrap my neural network.</p>\n<pre><code>from scikeras.wrappers import KerasClassifier\ntunned_model = KerasClassifier(\n    model=create_model,\n    epochs=param_dict['epochs'],\n    batch_size=param_dict['batch_size'],\n    learning_rate=param_dict['learning_rate'], verbose=1)\n</code></pre>\n<p>Then called RandomizedSearchCV and fit my data and i got an error.</p>\n<pre><code># Loading Randomized Search CV\nrandom_search = RandomizedSearchCV(tunned_model,  param_distributions=param_dict, n_iter= 20, scoring='accuracy', verbose=2, cv=5, n_jobs=1, refit=True)\n\n# Fitting Neural Network\nrandom_search.fit(\n    X_train,\n    y_train,\n    validation_data=(X_val, y_val),\n    callbacks=[early_stopping]\n)\n\nOutput:\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n&lt;ipython-input-40-1796011897&gt; in &lt;cell line: 0&gt;()\n      3 \n      4 # Fitting Neural Network\n----&gt; 5 random_search.fit(\n      6     X_train,\n      7     y_train,\n\n4 frames\n/usr/local/lib/python3.11/dist-packages/sklearn/base.py in __sklearn_tags__(self)\n    538 \n    539     def __sklearn_tags__(self):\n--&gt; 540         tags = super().__sklearn_tags__()\n    541         tags.estimator_type = &quot;classifier&quot;\n    542         tags.classifier_tags = ClassifierTags()\n\nAttributeError: 'super' object has no attribute '__sklearn_tags__'\n</code></pre>\n<p>I have done my best to figure out what the problem is and nothing seems to work. I need help with the code.</p>\n",
    "is_answered": false,
    "view_count": 49,
    "answer_count": 1
  },
  {
    "title": "Pandas dropna(how=&quot;all&quot;, inplace=True) doesn&#39;t work",
    "link": "https://stackoverflow.com/questions/79665899/pandas-dropnahow-all-inplace-true-doesnt-work",
    "tags": [
      "python",
      "pandas",
      "dataframe",
      "data-science"
    ],
    "body": "<p><code>df.dropna(how=&quot;all&quot;, inplace=True)</code> does not remove all full <code>NaN</code> columns/rows that are in my dataframe.</p>\n<p>I created a matrix of every distance between the values in a list.</p>\n<pre><code>     0        1        2        3        4\n0    0.000000,1.003090,7.006241,8.985008,9.012505\n1    1.003090,0.000000,6.003151,7.981918,8.009415\n2    7.006241,6.003151,0.000000,1.978767,2.006264\n3    8.985008,7.981918,1.978767,0.000000,0.027496\n</code></pre>\n<p>I then filtered this list by values that don't fit within a specific interval and turned the values that don't fit into <code>NaN</code>s.</p>\n<pre><code>     0        1        2        3        4\n0    Nan     ,Nan     ,7.006241,8.985008,9.012505\n1    Nan     ,Nan     ,6.003151,7.981918,8.009415\n2    7.006241,6.003151,Nan     ,Nan     ,2.006264\n3    8.985008,7.981918,1.978767,Nan     ,Nan\n</code></pre>\n<p>Additionally I turned the lower half (diagonally) to <code>Nans</code> because it is essentially the same as the upper half (diagonally).</p>\n<pre><code>     0        1        2        3        4\n0    Nan     ,Nan     ,7.006241,8.985008,9.012505\n1    Nan     ,Nan     ,6.003151,7.981918,8.009415\n2    Nan     ,Nan     ,Nan     ,Nan     ,2.006264\n3    Nan     ,Nan     ,Nan     ,Nan     ,Nan\n</code></pre>\n<p>Now I am trying to remove all columns and rows that are fully turned into <code>Nans</code> with:</p>\n<pre><code>df.dropna(how=&quot;all&quot;, inplace=True)\n</code></pre>\n<p>but it only removes like 6 out of 100 rows that are full of <code>Nans</code> :,(.</p>\n<p>This is my full code:</p>\n<pre><code>mz_array = [1,2,3,4,5,6,7] # my list with data, actually 603 long.\ndistance_matrix = []\nfor column, i in zip(mz_array, range(len(mz_array))):\n    distance_matrix.append([])\n    for row in mz_array:\n        distance_matrix[i].append(np.abs(row - column))\n\ndf_dm = pd.DataFrame(distance_matrix)\ndf_dm.columns = df_dm.columns.map(str)\n\nfor i in range(len(mz_array)):\n    for j in range(len(mz_array)):\n        if j &lt;= i:\n            df_dm.iat[i,j] = None\n        if df_dm.iat[i,j] &lt; 55 or df_dm.iat[i,j] &gt; 480:\n            df_dm.iat[i,j] = None\n\ndf_dm.dropna(how=&quot;all&quot;, inplace=True)\n</code></pre>\n",
    "is_answered": false,
    "view_count": 110,
    "answer_count": 1
  },
  {
    "title": "How to use custom category names with YOLO-World for zero-shot object detection?",
    "link": "https://stackoverflow.com/questions/79662036/how-to-use-custom-category-names-with-yolo-world-for-zero-shot-object-detection",
    "tags": [
      "pytorch",
      "artificial-intelligence",
      "data-science",
      "object-detection",
      "yolo"
    ],
    "body": "<p>I'm currently experimenting with the <a href=\"https://github.com/AILab-CVC/YOLO-World\" rel=\"nofollow noreferrer\">YOLO-World</a> zero-shot object detection model and I want to use it to detect a <strong>custom set of categories</strong> not found in the default COCO or ImageNet label sets.</p>\n<p>I've gone through the provided examples and noticed that categories like <code>&quot;dog&quot;, &quot;car&quot;, &quot;person&quot;</code> work out-of-the-box, but when I try to use custom phrases like <code>&quot;medical mask&quot;</code> or <code>&quot;screwdriver&quot;</code>, the model either misses them or gives low confidence scores.</p>\n<p><strong>How do I properly define and pass custom text prompts</strong> for novel categories in YOLO-World?</p>\n<ol>\n<li><p>Is there a recommended way to <strong>format multi-word labels</strong> (e.g., &quot;safety helmet&quot;, &quot;coffee mug&quot;) to improve detection accuracy?</p>\n</li>\n<li><p>Does YOLO-World require any <strong>fine-tuning or prompt engineering</strong> for completely unseen categories?</p>\n</li>\n<li><p>Is there any <strong>vocabulary limitation or embedding fallback</strong> mechanism that I should be aware of when specifying rare words?</p>\n</li>\n</ol>\n<ul>\n<li><p>Edited the <code>args.categories</code> in <code>demo.py</code> to include custom words</p>\n</li>\n<li><p>Ran the model with <code>--task zeroshot</code> and <code>--text_prompt</code> arguments</p>\n</li>\n<li><p>Verified that the model works fine on COCO-style prompts</p>\n</li>\n</ul>\n<p>But the model still struggles with some uncommon categories.</p>\n<p>Any suggestions, insights, or working examples would be highly appreciated!</p>\n",
    "is_answered": false,
    "view_count": 55,
    "answer_count": 0
  },
  {
    "title": "Need Assistance with Fine-tuning my Content Moderation Neural Network with RoBERTa. (Dataset: Jigsaw)",
    "link": "https://stackoverflow.com/questions/79658001/need-assistance-with-fine-tuning-my-content-moderation-neural-network-with-rober",
    "tags": [
      "python",
      "pytorch",
      "artificial-intelligence",
      "data-science"
    ],
    "body": "<p>I can't quite figure out how to fine-tune my neural network with jigsaw and make it a multi-classifier rather than a binary-classifier (multiple types of output rather than just positive and negative) and need assistance with doing so.</p>\n<p>(With PyTorch).</p>\n<p>Here is the source code:</p>\n<pre><code>from transformers import AutoTokenizer\nfrom transformers import AutoModelForSequenceClassification\nfrom scipy.special import softmax\nimport nltk\nfrom nltk.tokenize import word_tokenize\n\nnltk.download('punkt')\n\nexample = &quot;Dude, that's amazing!&quot;\n\ntokens = word_tokenize(example, preserve_line=True)\n\nMODEL = &quot;cardiffnlp/twitter-roberta-base-offensive&quot;\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL)\n\ndef evaluate_text(text):\n    encoded_text = tokenizer(text, return_tensors='pt')\n    output = model(**encoded_text)\n    scores = output[0][0].detach().numpy()\n    scores = softmax(scores)\n\n    neg_value = float(scores[1])\n    pos_value = float(scores[0])\n\n    print(&quot;Negative value:&quot;, neg_value)\n    print(&quot;Positive value:&quot;, pos_value)\n\nevaluate_text(example)\n</code></pre>\n<p>Well, I really just need assistance rather than just resolving an issue.</p>\n",
    "is_answered": false,
    "view_count": 32,
    "answer_count": 1
  },
  {
    "title": "Future Looking Target Variables in Light GBM",
    "link": "https://stackoverflow.com/questions/79657687/future-looking-target-variables-in-light-gbm",
    "tags": [
      "machine-learning",
      "data-science",
      "xgboost",
      "algorithmic-trading"
    ],
    "body": "<p>I am working on a ML model using LightGBM classifier with a task to maximise a value( PnL) for a quantitative strategy.\nI am entering a trade every Monday and can hold till Friday close.\nI want to create labels to maximise optimum exit within the week</p>\n<p>I am using the target variable as difference of current rolling pnl vs projected pnl till Friday\nlike (current_close/monday_open)-1 = current pnl\nAnd than (current_close/friday_close ) = projected pnl\nso in a nutshell i am using future leakage for the target variable.\nis this approach justified?</p>\n<p>Basically if current_pnl&lt;future_pnl i am marking the label as '1'\n(means more juice left in the trade, else 0).</p>\n<p>is this a good way to use ML?</p>\n<p>I have a bunch of 50 features and than this forward looking Target Variable\nWhat should be a good, simple target variable?\nI am worried as my target feature is forward looking (future-leakage).</p>\n",
    "is_answered": false,
    "view_count": 21,
    "answer_count": 1
  },
  {
    "title": "How can I scrape content that&#39;s loaded dynamically on Sainsbury&#39;s product pages?",
    "link": "https://stackoverflow.com/questions/79656070/how-can-i-scrape-content-thats-loaded-dynamically-on-sainsburys-product-pages",
    "tags": [
      "html",
      "web-scraping",
      "beautifulsoup",
      "scrapy",
      "data-science"
    ],
    "body": "<p>Trying to build a scraper that extracts nutritional information from each product page on Sainsbury (for eg, scraping energy values out of <a href=\"https://www.sainsburys.co.uk/gol-ui/product/sainsburys-british-filtered-whole-milk-2l\" rel=\"nofollow noreferrer\">https://www.sainsburys.co.uk/gol-ui/product/sainsburys-british-filtered-whole-milk-2l</a>).</p>\n<p>The nutritional information, however, only shows up on the html after pressing the dropdown button so Beautiful Soup is no help. For 1000+ products, Selenium will take too much time. Is there a way I can access that data?</p>\n<p>Any help would be lovely!</p>\n<p>Tried Beautiful Soup (content is loaded dynamically tho), Selenium (taking too much time), Scrapy (getting a 403 error whenever scrapy shell fetches the site)</p>\n",
    "is_answered": false,
    "view_count": 65,
    "answer_count": 2
  },
  {
    "title": "Is it possible to fine-tune an LLM using LoRA on AWS ECS Fargate?",
    "link": "https://stackoverflow.com/questions/79639986/is-it-possible-to-fine-tune-an-llm-using-lora-on-aws-ecs-fargate",
    "tags": [
      "amazon-web-services",
      "data-science",
      "large-language-model",
      "aws-fargate"
    ],
    "body": "<p>I\u2019m trying to fine-tune a lightweight LLM (like TinyLlama) using LoRA for a small custom dataset, but I'm facing two major issues:</p>\n<p>Problem 1:\nWhen I fine-tune the model locally, I\u2019m unable to push the fine-tuned model folder (checkpoints, adapter weights, etc.) to GitHub due to its large size. As a result, I can\u2019t trigger our CI/CD workflow, which depends on the GitHub repo.</p>\n<p>Problem 2:\nOur local system doesn\u2019t have a GPU, and CPU-only fine-tuning is very slow and impractical.</p>\n<p>What I want:\nI want to know if it\u2019s possible to fine-tune the model directly on AWS ECS Fargate, so I can:</p>\n<p>Additional Info:\nModel: TinyLlama (1.1B), LoRA-based fine-tuning with HuggingFace peft</p>\n<p>Dataset: Small Q&amp;A custom dataset</p>\n<p>Dependencies: transformers, peft, bitsandbytes, etc.\nAvoid pushing large models to GitHub</p>\n",
    "is_answered": false,
    "view_count": 41,
    "answer_count": 1
  },
  {
    "title": "How can I filter out spikes in Kusto Engine data but also create a new baseline value if the spike value persists?",
    "link": "https://stackoverflow.com/questions/79619491/how-can-i-filter-out-spikes-in-kusto-engine-data-but-also-create-a-new-baseline",
    "tags": [
      "statistics",
      "data-science",
      "kql",
      "azure-data-explorer"
    ],
    "body": "<p>So my data is totaliser values of Engine Hours, but the values will increase/decrease by an order of magnitude for a few timestamps and then return to a real value. I want to filter out these anomalies/spikes but I also want to cater for times when the Engine may have been replaced and there will be a new baseline. Ultimately I'm going to be doing a delta (max-min) type calculation on this data per time period bin, so I want to partition by any new engine/baseline event. I'm thinking I could have a baseline set if there are 5 consecutive values which are much higher/lower than the current baseline. I've tried a few things to filter out the spikes like series_outliers but even with tuning, I still get spikes or I end up filtering good data. I've tried the scan function and doing a delta on the time to compare to the engine hours delta, but I'm not sure how to achieve the new baseline functionality. Here is the code I was working on:</p>\n<pre><code>datatable (['time']: datetime, stored_float: real) [\n    datetime(2025-01-01T04:40:49.000Z),real(1178.9),\n    datetime(2025-01-19T12:28:15.000Z),real(1530),\n    datetime(2025-01-19T20:41:31.000Z),real(1536.7),\n    datetime(2025-01-20T04:42:55.000Z),real(1543.8),\n    datetime(2025-01-20T05:38:55.000Z),real(154), //Negative Spike\n    datetime(2025-01-20T12:35:50.000Z),real(1550.3),\n    datetime(2025-01-20T20:40:43.000Z),real(1557),\n    datetime(2025-01-21T08:54:32.000Z),real(15682), //Positive Spike\n    datetime(2025-01-21T12:40:32.000Z),real(1571.8),\n    datetime(2025-01-22T10:39:52.000Z),real(15889), //Positive Spike\n    datetime(2025-01-22T12:37:06.000Z),real(1590.8),\n    datetime(2025-01-22T21:44:36.000Z),real(1594.8),\n    datetime(2025-01-23T13:07:22.000Z),real(1607.2),\n    datetime(2025-01-24T13:07:22.000Z),real(16007.2), //New Baseline\n    datetime(2025-01-25T13:07:22.000Z),real(16017.2),\n    datetime(2025-01-26T13:07:22.000Z),real(16027.2),\n    datetime(2025-01-27T13:07:22.000Z),real(16037.2),\n    datetime(2025-01-28T13:07:22.000Z),real(16047.2),\n    datetime(2025-01-29T13:07:22.000Z),real(16057.2)\n]\n| order by ['time'] asc\n//| extend timeDelta=(['time']-prev(['time']))/1h,engineDelta=stored_float-prev(stored_float)\n| scan with_match_id=session_id declare(timeDelta:real,engineDelta:real,baselineTime:datetime,baselineEngine:real) with \n(\n    step s1: true =&gt; timeDelta=(['time']-s1.['time'])/1h,engineDelta=stored_float-s1.stored_float,baselineTime=s1.['time'],baselineEngine=s1.stored_float;\n    step s2: abs(s1.engineDelta)&gt;s1.timeDelta =&gt; timeDelta=(['time']-s1.baselineTime)/1h,engineDelta=stored_float-s1.baselineEngine,baselineTime=s1.baselineTime,baselineEngine=s1.baselineEngine;\n    step s3: abs(s2.engineDelta)&gt;s2.timeDelta =&gt; timeDelta=(['time']-s2.baselineTime)/1h,engineDelta=stored_float-s2.baselineEngine,baselineTime=s2.baselineTime,baselineEngine=s2.baselineEngine;\n    step s4: abs(s3.engineDelta)&gt;s3.timeDelta =&gt; timeDelta=(['time']-s1.['time'])/1h,engineDelta=stored_float-s1.stored_float,baselineTime=s1.['time'],baselineEngine=s1.stored_float;\n)\n</code></pre>\n<p><strong>EDIT:</strong> Expected Results (before filtering for anomalies)</p>\n<p>timeDelta=time-baselineTime</p>\n<p>engineDelta=stored_float-baselineEngine</p>\n<p>anomaly=abs(engineDelta)&gt;timeDelta</p>\n<p><a href=\"https://i.sstatic.net/V0OI2Jyt.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/V0OI2Jyt.png\" alt=\"enter image description here\" /></a></p>\n",
    "is_answered": false,
    "view_count": 88,
    "answer_count": 1
  },
  {
    "title": "How do I do a specific aggregation on a table based on row column values on another table (SQL)?",
    "link": "https://stackoverflow.com/questions/79616449/how-do-i-do-a-specific-aggregation-on-a-table-based-on-row-column-values-on-anot",
    "tags": [
      "python",
      "sql",
      "data-science",
      "duckdb",
      "data-engineering"
    ],
    "body": "<p>I have loaded two fact tables <code>CDI</code> and <code>Population</code> and a couple dimension tables in DuckDB. I did joins on the <code>CDI</code> fact table and its respective dimension tables which yields a snippet of the table below\n<a href=\"https://i.sstatic.net/Qx35MfnZ.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/Qx35MfnZ.png\" alt=\"CDI table merged\" /></a></p>\n<p>And below is the <code>Population</code> fact table merged with its other dimension tables yielding this snippet below\n<a href=\"https://i.sstatic.net/rUtTYGEk.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/rUtTYGEk.png\" alt=\"Population table merged\" /></a>\n<a href=\"https://i.sstatic.net/A9XTlS8J.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/A9XTlS8J.png\" alt=\"enter image description here\" /></a></p>\n<p>Now what I want to basically do is filter out the <code>Population</code> table based only on the values of this particular row of the <code>CDI</code> table. In this case the current row outlined in green will somehow do this query</p>\n<pre><code>SELECT Year, SUM(Population) AS TotalPopulation\n    FROM Population\n    WHERE (Year BETWEEN 2018 AND 2018) AND\n    (Age BETWEEN 18 AND 85) AND\n    State = 'Pennsylvania' AND\n    Sex IN ('Male', 'Female') AND\n    Ethnicity IN ('Multiracial') AND\n    Origin IN ('Not Hispanic')\n    GROUP BY Year\n    ORDER BY Year ASC\n</code></pre>\n<p>This query aggregates the <code>Population</code> column values based on the row values of the <code>CDI</code> table. What I'm just at a loss in trying to implement is doing this aggregation operation for all row values in the <code>CDI</code> table. Here is a full visualization of what I'm trying to do.\n<a href=\"https://i.sstatic.net/YGYud5x7.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/YGYud5x7.png\" alt=\"enter image description here\" /></a></p>\n<p>How would I implement this type of varying filtering aggregation based on each row column values of the <code>CDI</code> table? I'm using DuckDB as the OLAP DB here so ANSI SQL is what I'm trying to use to implement this task. Could it be possible only using this kind of SQL?</p>\n",
    "is_answered": true,
    "view_count": 112,
    "answer_count": 1
  },
  {
    "title": "Encountering an error when fitting an ensemble stack of classification models to test data in purr::map() function",
    "link": "https://stackoverflow.com/questions/79609679/encountering-an-error-when-fitting-an-ensemble-stack-of-classification-models-to",
    "tags": [
      "machine-learning",
      "data-science",
      "tidymodels"
    ],
    "body": "<p>I encounter this error when trying to fit a tuned model to my test data:</p>\n<pre><code>Error in `purrr::map()`:\n\u2139 In index: 10.\n\u2139 With name: umap_neural_network_08_1.\nCaused by error in `.External()`:\n! NULL value passed as symbol address\nBacktrace:\n  1. camp_test %&gt;% bind_cols(final_model %&gt;% predict(new_data = camp_test))\n 12. stacks:::predict_members_classification(...)\n 21. purrr::map(...)\n 22. purrr:::map_(&quot;list&quot;, .x, .f, ..., .progress = .progress)\n 27. workflows:::predict.workflow(.x[[i]], ...)\n     ...\n 35. recipes:::bake.recipe(object = rec, new_data = new_data)\n 37. embed:::bake.step_umap(step, new_data = new_data)\n 40. uwot::umap_transform(model = object$object, X = new_data[, col_names])\n 41. uwot:::all_nn_indices_are_loaded(model)\n 42. rcppannoy$getNTrees()\n</code></pre>\n<p>This is occurring after tuning and fitting my classification model. It continues to occur when I remove the identified model from the race results,  as it applies to several models it would seem.</p>\n<p>In my candidate models there are some which apply umap or pls dimensionality reduction, as the data set was very wide.\nHowever, this issue is not isolated to such models.\nIt did not occur with untuned candidate models. I.e. when I ran the same workflow without hyper tuning my candidates.\nI have not preprocessed my test data as the ensemble has the workflow within.</p>\n<p>Any pointers would be helpful on how to resolve this issue.</p>\n<p>Cheers!</p>\n<p>Full code:</p>\n<pre><code>#setting up specs\nlog_spec &lt;- multinom_reg(penalty=tune(), mixture = tune()) %&gt;%\n  set_engine(&quot;glmnet&quot;) %&gt;%\n  set_mode(&quot;classification&quot;)\nlog_param &lt;- extract_parameter_set_dials(log_spec) \ntune_grid_log &lt;- grid_latin_hypercube(log_param, size = 20, original = FALSE)\n\nnnet_spec &lt;- \n   mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %&gt;% \n   set_engine(&quot;nnet&quot;, MaxNWts = 2600) %&gt;% \n   set_mode(&quot;classification&quot;)\nnnet_param &lt;- extract_parameter_set_dials(nnet_spec) \ntune_grid_nnet &lt;- grid_latin_hypercube(nnet_param, size = 20, original = FALSE)\n\nsvm_r_spec &lt;- \n   svm_rbf(cost = tune(), rbf_sigma = tune()) %&gt;% \n   set_engine(&quot;kernlab&quot;) %&gt;% \n   set_mode(&quot;classification&quot;)\nsvm_param &lt;- extract_parameter_set_dials(svm_r_spec) \ntune_grid_svm &lt;- grid_latin_hypercube(svm_param, size = 20, original = FALSE)\n\nknn_spec &lt;- \n   nearest_neighbor(neighbors = tune(), dist_power = tune(), weight_func = tune()) %&gt;% \n   set_engine(&quot;kknn&quot;) %&gt;% \n   set_mode(&quot;classification&quot;)\nknn_param &lt;- extract_parameter_set_dials(knn_spec) \ntune_grid_knn &lt;- grid_latin_hypercube(knn_param, size = 20, original = FALSE)\n\nknn_spec_umap &lt;- \n   nearest_neighbor(dist_power = tune(), weight_func = tune()) %&gt;% \n   set_engine(&quot;kknn&quot;) %&gt;% \n   set_mode(&quot;classification&quot;)\nknn_param_umap &lt;- extract_parameter_set_dials(knn_spec_umap) \ntune_grid_knn_umap &lt;- grid_latin_hypercube(knn_param_umap, size = 20, original = FALSE)\n\n#bagged CART\nbag_cart_spec &lt;- \n   bag_tree(cost_complexity = tune(), min_n = tune(), tree_depth = tune()) %&gt;% \n   set_engine(&quot;rpart&quot;, times = 50L) %&gt;% \n   set_mode(&quot;classification&quot;)\ncart_param &lt;- extract_parameter_set_dials(bag_cart_spec) \ntune_grid_cart &lt;- grid_latin_hypercube(cart_param, size = 20, original = FALSE)\n\n#random Forest\nrf_spec &lt;- \n   rand_forest(mtry = tune(), min_n = tune(), trees = 500) %&gt;% \n   set_engine(&quot;ranger&quot;) %&gt;% \n   set_mode(&quot;classification&quot;)\nrf_param &lt;- extract_parameter_set_dials(rf_spec) %&gt;% finalize(camp_train)\ntune_grid_rf &lt;- grid_latin_hypercube(rf_param, size = 20, original = FALSE)\n\nxg_spec &lt;- \n   boost_tree(tree_depth = tune(), learn_rate = tune(), loss_reduction = tune(), min_n = tune(), sample_size = tune(), trees = tune()) %&gt;% \n   set_engine(&quot;xgboost&quot;) %&gt;% \n   set_mode(&quot;classification&quot;)\nxg_param &lt;- extract_parameter_set_dials(xg_spec)\ntune_grid_xg &lt;- grid_latin_hypercube(xg_param, size = 20, original = FALSE)\n\nc5_spec &lt;- C5_rules(trees=tune(), min_n=tune()) %&gt;% \n  set_engine(&quot;C5.0&quot;) %&gt;% \n  set_mode(&quot;classification&quot;)\nc5_param &lt;- extract_parameter_set_dials(c5_spec)\ntune_grid_c5 &lt;- grid_latin_hypercube(c5_param, size = 20, original = FALSE)\n\nfda_spec &lt;-\n  discrim_flexible(prod_degree = tune()) %&gt;%\n  set_engine('earth')\nfda_param &lt;- extract_parameter_set_dials(fda_spec)\ntune_grid_fda &lt;- grid_latin_hypercube(fda_param, size = 20, original = FALSE)\n\nrda_spec &lt;-\n  discrim_regularized(frac_common_cov = tune(), frac_identity = tune()) %&gt;%\n  set_engine('klaR')\nrda_param &lt;- extract_parameter_set_dials(rda_spec)\ntune_grid_rda &lt;- grid_latin_hypercube(rda_param, size = 20, original = FALSE)\n\nbayes_spec &lt;-\n  naive_Bayes(smoothness = tune(), Laplace = tune()) %&gt;%\n  set_engine('klaR')\nbayes_param &lt;- extract_parameter_set_dials(bayes_spec)\ntune_grid_bayes &lt;- grid_latin_hypercube(bayes_param, size = 20, original = FALSE)\n</code></pre>\n<pre><code>#setting up split using beans\nbean_split &lt;- initial_validation_split(campy.train, strata = Source, prop = c(0.75, 0.125))\nbean_train &lt;- training(bean_split)\nbean_test &lt;- testing(bean_split)\nbean_validation &lt;- validation(bean_split)\n</code></pre>\n<pre><code>#setting up recipe and workflows\nbean_recipe &lt;- \n  recipe(Source ~ ., camp_train) %&gt;%\n  step_zv(all_numeric_predictors()) %&gt;%\n  step_orderNorm(all_numeric_predictors()) %&gt;%\n  step_normalize(all_numeric_predictors())\n\npls_recipe &lt;- \n  bean_recipe %&gt;% \n  step_pls(all_numeric_predictors(), outcome = &quot;Source&quot;, num_comp = tune())\n\numap_recipe &lt;-\n  bean_recipe %&gt;%\n  step_umap(\n    all_numeric_predictors(),\n    outcome = &quot;Source&quot;,\n    num_comp = tune(),\n    neighbors = tune(),\n    min_dist = tune()\n  )\n\nsimple_proc &lt;- \n   workflow_set(\n      preproc = list(simple = class_recipe), \n      models = list(RF = rf_spec, KNN = knn_spec, logistic = log_spec, neural_network = nnet_spec, SVM_radial = svm_r_spec, Bagged_CART = bag_cart_spec, XGBoost = xg_spec, C5 = c5_spec, fda = fda_spec, rda = rda_spec, bayes = bayes_spec)\n   ) %&gt;% option_add(grid=tune_grid_rf, id = 'simple_RF') %&gt;% \n  option_add(grid = tune_grid_knn, id = 'simple_KNN') %&gt;%\n  option_add(grid = tune_grid_log, id = 'simple_logistic') %&gt;%\n  option_add(grid = tune_grid_nnet, id = 'simple_neural_network') %&gt;%\n  option_add(grid = tune_grid_svm, id = 'simple_SVM_radial') %&gt;%\n  option_add(grid = tune_grid_cart, id = 'simple_Bagged_CART') %&gt;%\n  option_add(grid = tune_grid_xg, id = 'simple_XGBoost') %&gt;%\n  option_add(grid = tune_grid_c5, id = 'simple_C5') %&gt;%\n  option_add(grid = tune_grid_fda, id = 'simple_fda') %&gt;%\n  option_add(grid = tune_grid_rda, id = 'simple_rda') %&gt;%\n  option_add(grid = tune_grid_bayes, id = 'simple_bayes') \n  \n#basic pre-processing\nbasic_proc &lt;- \n   workflow_set(\n      preproc = list(basic = bean_recipe), \n      models = list(RF = rf_spec, KNN = knn_spec, logistic = log_spec, neural_network = nnet_spec, SVM_radial = svm_r_spec, Bagged_CART = bag_cart_spec, XGBoost = xg_spec, C5 = c5_spec, fda = fda_spec, rda = rda_spec, bayes = bayes_spec)\n   ) %&gt;% option_add(grid=tune_grid_rf, id = 'basic_RF') %&gt;% \n  option_add(grid = tune_grid_knn, id = 'basic_KNN') %&gt;%\n  option_add(grid = tune_grid_log, id = 'basic_logistic') %&gt;%\n  option_add(grid = tune_grid_nnet, id = 'basic_neural_network') %&gt;%\n  option_add(grid = tune_grid_svm, id = 'basic_SVM_radial') %&gt;%\n  option_add(grid = tune_grid_cart, id = 'basic_Bagged_CART') %&gt;%\n  option_add(grid = tune_grid_xg, id = 'basic_XGBoost') %&gt;%\n  option_add(grid = tune_grid_c5, id = 'basic_C5') %&gt;%\n  option_add(grid = tune_grid_fda, id = 'basic_fda') %&gt;%\n  option_add(grid = tune_grid_rda, id = 'basic_rda') %&gt;%\n  option_add(grid = tune_grid_bayes, id = 'basic_bayes')\n\n#partial least squares\npls_proc &lt;- workflow_set(\n      preproc = list(pls = pls_recipe), \n      models = list(KNN = knn_spec, logistic = log_spec, neural_network = nnet_spec, SVM_radial = svm_r_spec, Bagged_CART = bag_cart_spec, XGBoost = xg_spec, C5 = c5_spec, fda = fda_spec, rda = rda_spec, bayes = bayes_spec)\n   ) %&gt;% option_add(grid = tune_grid_knn, id = 'pls_KNN') %&gt;%\n  option_add(grid = tune_grid_log, id = 'pls_logistic') %&gt;%\n  option_add(grid = tune_grid_nnet, id = 'pls_neural_network') %&gt;%\n  option_add(grid = tune_grid_svm, id = 'pls_SVM_radial') %&gt;%\n  option_add(grid = tune_grid_cart, id = 'pls_Bagged_CART') %&gt;%\n  option_add(grid = tune_grid_xg, id = 'pls_XGBoost') %&gt;%\n  option_add(grid = tune_grid_c5, id = 'pls_C5') %&gt;%\n  option_add(grid = tune_grid_fda, id = 'pls_fda') %&gt;%\n  option_add(grid = tune_grid_rda, id = 'pls_rda') %&gt;%\n  option_add(grid = tune_grid_bayes, id = 'pls_bayes')\n\n#umap\numap_proc &lt;- \n   workflow_set(\n     preproc = list(umap = umap_recipe), \n     models = list(KNN = knn_spec_umap, logistic = log_spec, neural_network = nnet_spec, SVM_radial = svm_r_spec, Bagged_CART = bag_cart_spec, XGBoost = xg_spec, C5 = c5_spec, fda = fda_spec, rda = rda_spec, bayes = bayes_spec)\n) %&gt;% option_add(grid = tune_grid_knn_umap, id = 'umap_KNN') %&gt;%\n  option_add(grid = tune_grid_log, id = 'umap_logistic') %&gt;%\n  option_add(grid = tune_grid_nnet, id = 'umap_neural_network') %&gt;%\n  option_add(grid = tune_grid_svm, id = 'umap_SVM_radial') %&gt;%\n  option_add(grid = tune_grid_cart, id = 'umap_Bagged_CART') %&gt;%\n  option_add(grid = tune_grid_xg, id = 'umap_XGBoost') %&gt;%\n  option_add(grid = tune_grid_c5, id = 'umap_C5') %&gt;%\n  option_add(grid = tune_grid_fda, id = 'umap_fda') %&gt;%\n  option_add(grid = tune_grid_rda, id = 'umap_rda') %&gt;%\n  option_add(grid = tune_grid_bayes, id = 'umap_bayes')\n\nall_workflows &lt;- \n   bind_rows(simple_proc, basic_proc, pls_proc, umap_proc) \nall_workflows\n</code></pre>\n<pre><code>#tuning\nplan(multisession, workers = 8)\n\nbeans_race &lt;- all_workflows %&gt;%\n  workflow_map(\n    &quot;tune_race_anova&quot;,  #turn off for bagged cartesian models\n    verbose = TRUE,\n    seed = 21,\n    resamples = camp_folds,\n    grid = 20,\n    metrics = metric_set(accuracy, roc_auc),\n    control = race_ctrl\n  )\n\nrankings &lt;- \n  rank_results(beans_race, select_best = TRUE) %&gt;% \n  mutate(method = map_chr(wflow_id, ~ str_split(.x, &quot;_&quot;, simplify = TRUE)[1])) \n</code></pre>\n<pre><code>#stacking\ncamp_stack &lt;- \n  stacks() %&gt;%\n  add_candidates(beans_race)\n\nensemble &lt;- blend_predictions(camp_stack)\n\nautoplot(ensemble, &quot;weights&quot;) +\n  geom_text(aes(x = weight + 0.01, label = model), hjust = 0) + \n  theme(legend.position = &quot;none&quot;) +\n  lims(x = c(0, 0.6)) + labs(title = 'Models by stacking coefficient with penalty = 0.001')\n\nfinal_model &lt;- fit_members(ensemble) \n\nfinal_results &lt;- camp_test %&gt;% \n  bind_cols(final_model %&gt;% predict(new_data = camp_test))# %&gt;%\n  bind_cols(final_model %&gt;% predict(new_data = camp_test, type = &quot;prob&quot;)) \n</code></pre>\n",
    "is_answered": false,
    "view_count": 41,
    "answer_count": 0
  },
  {
    "title": "How to set a fixed random state in RandomizedSearchCV?",
    "link": "https://stackoverflow.com/questions/79603555/how-to-set-a-fixed-random-state-in-randomizedsearchcv",
    "tags": [
      "python",
      "scikit-learn",
      "data-science"
    ],
    "body": "<p>I'm using RandomizedSearchCV with RandomForestClassifier in scikit-learn. I want to make sure my results are reproducible across runs. Where should I set the random_state\u2014in the classifier, in RandomizedSearchCV, or both?</p>\n<p>Example code:</p>\n<pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\nclf = RandomForestClassifier()\nsearch = RandomizedSearchCV(clf, param_distributions=params, n_iter=10)\n</code></pre>\n<p>What's the best practice to ensure consistent results?</p>\n",
    "is_answered": true,
    "view_count": 32,
    "answer_count": 1
  },
  {
    "title": "Smoothening of data in excel based on certain rules",
    "link": "https://stackoverflow.com/questions/79588450/smoothening-of-data-in-excel-based-on-certain-rules",
    "tags": [
      "excel",
      "data-science",
      "non-linear-regression"
    ],
    "body": "<p>I have following example data</p>\n<p><a href=\"https://i.sstatic.net/0kUm7btC.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/0kUm7btC.png\" alt=\"enter image description here\" /></a></p>\n<p>The Figures depend on 3 parameters: X, Y &amp; Rank. Out of these we are making 4 buckets. High X High Y, Low X Low Y, High X, Low Y and Low X, High Y.  Then we are putting the values in each bucket and classifying them in Ranks. So for example, -22.32 is the average of all the records in Low X, Low Y bucket with Rank as 1 and -8.67 for Rank2 and so on.</p>\n<p>Now we have following rules for the ideal distribution</p>\n<p>For the same Rank :</p>\n<ul>\n<li>Low X High Y Figure should be greater than Low X Low Y Figure</li>\n<li>High X High Y Figure should be greater than High X Low Y Figure</li>\n<li>High X Low Y Figure should be greater than Low X Low Y Figure</li>\n<li>High X High Y Figure should be greater than High X High Y Figure</li>\n</ul>\n<p>and for the same bucket :\nHigh Rank Figure should be greater than Low Rank Figure in a bucket</p>\n<p>As you can see, the data confirms fairly well but there are 2 outliers :\nHigh X, High Y, Rank 2 (marked in red) and Low X, Rank 3 (marked in orange)</p>\n<p>If lets say -4.27 becomes -0.4 and -1.51 becomes -1.8 (or -1.79 becomes -1.50) the data will start confirming with all rules.</p>\n<p>My question is that is there any automated way to do this in excel. I have tried polynomial regression but it doesn't work because it changes a lot of values especially at the extremes. I have no issues with quantum of change at every step as long as above rules are satisfied for all records.I can't manually change these values because the data is huge and the process has to be repeatable.</p>\n",
    "is_answered": false,
    "view_count": 63,
    "answer_count": 1
  },
  {
    "title": "My jupyter Notebook interface is missing &quot;Widget&quot; section while other people have it",
    "link": "https://stackoverflow.com/questions/79582739/my-jupyter-notebook-interface-is-missing-widget-section-while-other-people-hav",
    "tags": [
      "python",
      "jupyter-notebook",
      "anaconda",
      "data-science",
      "jupyter-lab"
    ],
    "body": "<p>&quot;I'm following the same steps as my instructor, but my screen appears different compared to theirs. I've attached an image showing both screens. What could be causing this discrepancy, and how can I resolve it?&quot;\n<a href=\"https://i.sstatic.net/L3cBVWdr.jpg\" rel=\"nofollow noreferrer\">My screen vs instructor screen</a></p>\n",
    "is_answered": false,
    "view_count": 126,
    "answer_count": 1
  },
  {
    "title": "How to detect and scrape a specific language version of a multilingual publication, if available?",
    "link": "https://stackoverflow.com/questions/79578485/how-to-detect-and-scrape-a-specific-language-version-of-a-multilingual-publicati",
    "tags": [
      "python",
      "web-scraping",
      "beautifulsoup",
      "python-requests",
      "data-science"
    ],
    "body": "<p>I wrote a python script for scraping data from WHO website, I wanted to retrieve Title, author name, date, pdf link and child page link from parent page (i applied some filters on parent page)\nI am getting the data in CSV perfectly fine in every column but for many entries the language is getting changed to arabic or some other language even though the same thing is available in english on the website. Even the link to the pdf is redirecting to the pdf in different language even if english version is there.</p>\n<p>The webpage offers same information in different languages, I feel my script while sending request is fetching data in different language.</p>\n<p>So my question is: How can I modify my script so that it gets all the data in english only?\nI <em><strong>dont want to filter</strong></em> otherwise I'll lost my data.</p>\n<p>I want to make sure that if data is of another language, the script should find the english version of it on the webpage, because obviously it is present there.</p>\n<p>I am attaching code below:</p>\n<pre><code>import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport time\nimport random\nfrom http.client import RemoteDisconnected\n\nBASE_URL = &quot;https://iris.who.int&quot;\nPAGE_COUNT = 51  # Total pages\nRPP = 100  # Results per page\n\n# List of user-agents\nUSER_AGENTS = [\n    &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115 Safari/537.36&quot;,\n    &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 13_0) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15 Safari/605.1.15&quot;,\n    &quot;Mozilla/5.0 (Windows NT 10.0; Win64; rv:90.0) Gecko/20100101 Firefox/90.0&quot;,\n]\n\nsession = requests.Session()\n\ndef get_with_retries(url, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            headers = {\n                &quot;User-Agent&quot;: random.choice(USER_AGENTS)\n            }\n            res = session.get(url, headers=headers, timeout=20)\n            if res.status_code == 429:\n                wait = 30 + random.randint(10, 30)\n                print(f&quot;\u26a0\ufe0f  Rate limit hit. Waiting {wait}s...&quot;)\n                time.sleep(wait)\n                continue\n            res.raise_for_status()\n            return res\n        except RemoteDisconnected:\n            wait = 10 + attempt * 10\n            print(f&quot;[!] RemoteDisconnected \u2014 retrying in {wait}s...&quot;)\n            time.sleep(wait)\n        except requests.exceptions.RequestException as e:\n            print(f&quot;[!] Attempt {attempt + 1}/{max_retries} failed for {url}: {e}&quot;)\n            time.sleep(10 * (attempt + 1))  # exponential backoff\n    return None\n\ndef extract_details(item):\n    # Check if language attribute is English\n    lang = item.get(&quot;lang&quot;, &quot;en&quot;)\n    if lang != &quot;en&quot;:\n        return None  # Skip non-English\n\n    title_tag = item.find(&quot;h4&quot;, class_=&quot;artifact-title&quot;)\n    title = title_tag.get_text(strip=True) if title_tag else &quot;N/A&quot;\n\n    author_tag = item.find(&quot;span&quot;, class_=&quot;author&quot;)\n    if author_tag and author_tag.get(&quot;lang&quot;) and author_tag[&quot;lang&quot;] != &quot;en&quot;:\n        return None  # Skip if author explicitly marked non-English\n    author = author_tag.get_text(strip=True) if author_tag else &quot;N/A&quot;\n\n    date_tag = item.find(&quot;span&quot;, class_=&quot;govdoc-date&quot;)\n    dates = date_tag.get_text(strip=True) if date_tag else &quot;N/A&quot;\n\n    href = item.find(&quot;a&quot;, href=True)\n    child_url = BASE_URL + href[&quot;href&quot;] if href else &quot;N/A&quot;\n\n    return title, author, dates, child_url\n\ndef extract_pdf_link(child_url):\n    response = get_with_retries(child_url)\n    if not response:\n        return None\n\n    soup = BeautifulSoup(response.content, 'html.parser')\n\n    # Prefer links that are clearly English\n    pdf_links = soup.find_all(&quot;a&quot;, href=lambda x: x and &quot;.pdf&quot; in x)\n\n    for link in pdf_links:\n        href = link['href']\n        text = link.get_text(strip=True).lower()\n        if &quot;english&quot; in text or &quot;en&quot; in href:\n            return BASE_URL + href\n\n    # fallback if no clear English link\n    if pdf_links:\n        return BASE_URL + pdf_links[0]['href']\n\n    return None\n\n# Main scraping loop\ndata = []\n\nfor page_num in range(PAGE_COUNT):\n    print(f&quot;\\n\ud83d\udd0d Scraping page {page_num + 1}/{PAGE_COUNT}...&quot;)\n    url = f&quot;https://iris.who.int/discover?filtertype=dateIssued&amp;filter_relational_operator=equals&amp;filter=2025&amp;rpp={RPP}&amp;page={page_num}&quot;\n    res = get_with_retries(url)\n    if not res:\n        print(f&quot;[!] Skipping page {page_num + 1} due to repeated failures.&quot;)\n        continue\n\n    soup = BeautifulSoup(res.text, &quot;html.parser&quot;)\n    items = soup.select(&quot;.ds-artifact-item&quot;)\n\n    for item in items:\n        result = extract_details(item)\n        if not result:\n            continue  # Skip non-English\n\n        title, author, dates, child_url = result\n        pdf_link = extract_pdf_link(child_url) if child_url != &quot;N/A&quot; else &quot;N/A&quot;\n\n        if pdf_link is None:\n            continue  # Skip if no valid PDF found\n\n        row = {\n            &quot;Title&quot;: title,\n            &quot;Author&quot;: author,\n            &quot;Dates&quot;: dates,\n            &quot;PDF_Link&quot;: pdf_link,\n            &quot;Child_Page&quot;: child_url\n        }\n        data.append(row)\n        print(f&quot;\u2705 Added: {title[:60]}...&quot;)\n\n        time.sleep(random.uniform(2, 5))  # Random polite pause between items\n\n    time.sleep(random.uniform(4, 8))  # Random pause between pages\n\n# Save to file\ndf = pd.DataFrame(data)\ndf.to_csv(&quot;who_data_english_only.csv&quot;, index=False, encoding=&quot;utf-8-sig&quot;)\nprint(f&quot;\\n\u2705 Done! {len(df)} entries saved to who_data_english_only.csv&quot;)\n</code></pre>\n",
    "is_answered": true,
    "view_count": 81,
    "answer_count": 2
  },
  {
    "title": "Plotting real-time graphs using Matplotlib does not allow plotting second graph",
    "link": "https://stackoverflow.com/questions/79570139/plotting-real-time-graphs-using-matplotlib-does-not-allow-plotting-second-graph",
    "tags": [
      "python",
      "matplotlib",
      "graph",
      "data-science"
    ],
    "body": "<p>I am reading data from serial port and based on that, plotting graphs using matplotlib.</p>\n<p>The data from serial port is in format:\n<code>ecg 9.89 4.5 123 . . end psd 2.1, 33.2 3.4, 92.10 . . end fft 2.1, 33.2 3.4, 92.10 . . end</code></p>\n<p>There are two different plots. One for ECG and another for FFT/PSD. For ECG, it plots data in real time as they come. But for FFT/PSD, it plots data at the end. The issue I am having is it plots ECG fine but shows fft/psd blank in the first reading. Now when I send second reading from serial port, it shows first fft/psd graph.</p>\n<p>Below is my code. I added some logs in plot_fft_psd. It prints Third!!!!!! in the first iteration. fft_data and psd_data is also fine. But does not plot graph, graph is black/empty.</p>\n<pre><code>def plot_fft_psd(fft_data, psd_data):\n    print(&quot;plotting fft and psd&quot;)\n    fig2, (ax_fft, ax_psd) = plt.subplots(2, 1, figsize=(10, 6), num='Figure 2 - FFT &amp; PSD')\n\n    print(&quot;First!!!!!!&quot;)\n    fft_x, fft_y = zip(*fft_data)\n    ax_fft.plot(fft_x, fft_y, marker='o', color='blue')\n\n    print(&quot;Second!!!!!!&quot;)\n    psd_x, psd_y = zip(*psd_data)\n    ax_psd.plot(psd_x, psd_y, marker='o', color='orange')\n\n    plt.tight_layout()\n    plt.show()\n    plt.pause(0.01)\n    print(&quot;Third!!!!!!&quot;)\n\n\ndef plot_ecg(x_vals, y_vals, ax, line):\n    line.set_xdata(x_vals)\n    line.set_ydata(y_vals)\n\n    ax.set_xlim(0, 180)\n    ax.set_ylim(-150, 200)\n    plt.draw()\n\n\ndef serial_main():\n    ser = serial.Serial(SERIAL_PORT, BAUDRATE, timeout=1)\n    \n    fig1, ecg_ax = plt.subplots(figsize=(12, 6), num='Figure 1 - ECG')\n\n    ecg_x_vals = []\n    ecg_y_vals = []\n    line, = ecg_ax.plot([], [], color='green')\n\n    count = 0\n    fft_data = []\n    psd_data = []\n    received_fft = False\n    received_psd = False\n\n    current_mode = None\n    buffer = []\n\n    while True:\n        line_data = ser.readline().decode('utf-8').strip()\n\n        print(f&quot;&gt; {line_data}&quot;)\n\n        if line_data in (&quot;ecg&quot;, &quot;fft&quot;, &quot;psd&quot;):\n            current_mode = line_data\n            buffer = []\n            continue\n\n        if line_data == &quot;end&quot;:\n            if current_mode == &quot;fft&quot;:\n                fft_data = buffer\n                received_fft = True\n\n            elif current_mode == &quot;psd&quot;:\n                psd_data = buffer\n                received_psd = True\n\n            current_mode = None\n            buffer = []\n\n        try:\n            if current_mode == &quot;ecg&quot;:\n                value = float(line_data)\n                ecg_x_vals.append(count * 0.01)\n                ecg_y_vals.append(value)\n\n                plot_ecg(ecg_x_vals, ecg_y_vals, ecg_ax, line)\n                plt.pause(0.001)\n                count += 1\n\n            elif current_mode in (&quot;fft&quot;, &quot;psd&quot;):\n                x_str, y_str = line_data.split(&quot;,&quot;)\n                x = float(x_str.strip())\n                y = float(y_str.strip())\n                buffer.append((x, y))\n\n        except ValueError:\n            print(f&quot;Invalid data skipped: {line_data}&quot;)\n            continue\n        except Exception as e:\n            print(f&quot;Unexpected error: {e} for line: {line_data}&quot;)\n            continue\n\n        if received_fft and received_psd:\n            print(&quot;received fft and psd&quot;)\n            plot_fft_psd(fft_data, psd_data)\n            received_fft = False\n            received_psd = False\n\n    plt.pause(0.001)\n\n</code></pre>\n",
    "is_answered": false,
    "view_count": 132,
    "answer_count": 0
  },
  {
    "title": "With kotlin-notebook, how do I use DataFrame.readCsv to parse a column of dates formatted mm/dd/yyyy?",
    "link": "https://stackoverflow.com/questions/79527865/with-kotlin-notebook-how-do-i-use-dataframe-readcsv-to-parse-a-column-of-dates",
    "tags": [
      "kotlin",
      "csv",
      "data-science",
      "kotlin-notebook",
      "kotlin-dataframe"
    ],
    "body": "<p>The latest release (<code>0.15.0</code>) of the <a href=\"https://kotlin.github.io/dataframe/gettingstartedjupyternotebook.html\" rel=\"nofollow noreferrer\">Dataframe library for Kotlin Notebook</a> uses the method <code>readCsv</code> (note: lowercase <code>sv</code>), and in the following code, I'm using it to read in a CSV of my Discover card financials, which has dates in the <code>Trans. Date</code> column that look like <code>12/31/2024</code>, but it is throwing the following <code>CellConversionException</code> parsing this format:</p>\n<pre class=\"lang-none prettyprint-override\"><code>org.jetbrains.kotlinx.dataframe.exceptions.CellConversionException: Failed to convert '12/31/2024' from kotlin.String to kotlinx.datetime.LocalDate in column 'Trans. Date' in column [Trans. Date], row 0\nat Cell In[6], line 17\n</code></pre>\n<p>Here is the Kotlin notebook cell:</p>\n<pre class=\"lang-kotlin prettyprint-override\"><code>// import kotlin dataframe library\n%use dataframe(v=0.15.0, enableExperimentalCsv=true)\n\nimport kotlinx.datetime.format.DateTimeFormat\nimport kotlinx.datetime.format.char\nimport kotlinx.datetime.format.Padding.ZERO as ZEROpad\n\n// seems like I need to define the DateTimeFormat,\n// but I don't know how to pass it to the CSV parser.\nval dateFormat : DateTimeFormat&lt;LocalDate&gt; = LocalDate.Format {\n  monthNumber(ZEROpad)\n  char('/')\n  dayOfMonth(ZEROpad)\n  char('/')\n  year(ZEROpad)\n}\n\n// read the csv file into a dataframe\nvar discover = DataFrame.readCsv(&quot;2024 discover.csv&quot;,\n  colTypes = mapOf(\n    // how do I get this conversion to work for given mm/dd/yyyy format?\n    &quot;Trans. Date&quot; to ColType.LocalDate\n  )\n)\n\ndiscover.getRows(0..4)\n</code></pre>\n<p>Note that the <code>dateFormat</code> val is unused! I don't know what to do with it! How do I pass this format (assuming it is correct) to the parser to get a column of machine-readable dates?</p>\n",
    "is_answered": true,
    "view_count": 152,
    "answer_count": 1
  },
  {
    "title": "YOLOv11 Model Converted to TFLite Not Producing Correct Output in TensorFlow",
    "link": "https://stackoverflow.com/questions/79499322/yolov11-model-converted-to-tflite-not-producing-correct-output-in-tensorflow",
    "tags": [
      "python",
      "tensorflow",
      "artificial-intelligence",
      "data-science",
      "yolo"
    ],
    "body": "<p>I'm training an ALPR detection model using the dataset from <a href=\"https://universe.roboflow.com/alpr-12yby/alpr-transformed/\" rel=\"nofollow noreferrer\">Roboflow ALPR</a> with YOLOv11, converted to TFLite using:</p>\n<pre><code>import ultralytics as yolo\n!yolo detect export model=/content/runs/detect/yolov11_anpr/weights/best.pt imgsz=640 batch=1 format=tflite\n</code></pre>\n<p>My Current Python Inference Code (Ultralytics YOLO)\nBoth .pt and .tflite models work correctly in Ultralytics' inference pipeline:</p>\n<pre><code>from PIL import Image\nfrom ultralytics import YOLO\n\nimage = Image.open(&quot;/content/Screenshot From 2025-03-08 16-37-15.png&quot;)\nmodel = YOLO('/content/runs/detect/yolov11_anpr/weights/best_saved_model/best_float32.tflite')\nresults = model(image)\n\nresult = results[0]\nresult.show()\n</code></pre>\n<p>This successfully detects Persian numbers:</p>\n<p><a href=\"https://i.sstatic.net/nfGkCxPN.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/nfGkCxPN.png\" alt=\"licence plate \" /></a></p>\n<p>Here's a visual representation of the successful detection using Ultralytics YOLO:</p>\n<p><strong>Problem</strong>\nHowever, direct inference with TensorFlow (without Ultralytics) doesn't produce correct detections. The output data is incorrect or missing entirely.</p>\n<p><strong>Questions:</strong>\nWhy does inference using Ultralytics YOLO work, but direct TensorFlow inference doesn't?\nWhat preprocessing or post-processing steps am I missing for YOLOv11 TFLite inference with TensorFlow?\nAny insights or solutions to correctly use the TFLite model directly with TensorFlow would be greatly appreciated!</p>\n<p>you can download and test my tflite model with below link :\n<a href=\"https://drive.google.com/file/d/1p4CaFl9g2gPjGUd68xlr_EQlxz-umTre/view?usp=sharing\" rel=\"nofollow noreferrer\">https://drive.google.com/file/d/1p4CaFl9g2gPjGUd68xlr_EQlxz-umTre/view?usp=sharing</a></p>\n",
    "is_answered": false,
    "view_count": 376,
    "answer_count": 1
  },
  {
    "title": "in KDB q, how do you write a custom mavg that drops the top 5 and bottom 5 results before doing moving average",
    "link": "https://stackoverflow.com/questions/79479365/in-kdb-q-how-do-you-write-a-custom-mavg-that-drops-the-top-5-and-bottom-5-resul",
    "tags": [
      "data-science",
      "kdb+",
      "q",
      "moving-average"
    ],
    "body": "<p>I have a 5min table that has a score value like the table below.  How can I add a new column which is the 10-period moving average which drops the highest 2 and lowest 2 scores first, and just average the 6 remaining values?</p>\n<pre><code>q)select from scores_table\n</code></pre>\n<pre>\n|datetime                       | score |\n|------------------------------ | ----- |\n|2022.10.28D22:20:00.000000000  | 223   |\n|2022.10.28D22:25:00.000000000  | 221   |\n|2022.10.28D22:30:00.000000000  | 231   |\n|2022.10.28D22:35:00.000000000  | 182   |\n|2022.10.28D22:40:00.000000000  | 204   |\n|2022.10.28D22:45:00.000000000  | 143   |\n|2022.10.28D22:50:00.000000000  | 228   |\n|2022.10.28D22:55:00.000000000  | 203   |\n|2022.10.28D23:00:00.000000000  | 158   |\n|2022.10.28D23:05:00.000000000  | 98    |\n|2022.10.28D23:10:00.000000000  | 111   |\n|2022.10.28D23:15:00.000000000  | 146   |\n|2022.10.28D23:20:00.000000000  | 94    |\n|2022.10.28D23:25:00.000000000  | 159   |\n|2022.10.28D23:30:00.000000000  | 93    |\n|2022.10.28D23:35:00.000000000  | 91    |\n|2022.10.28D23:40:00.000000000  | 132   |\n|2022.10.28D23:45:00.000000000  | 77    |\n|2022.10.28D23:50:00.000000000  | 179   |\n|2022.10.28D23:55:00.000000000  | 140   |\n</pre>\n<p>In other words, I want to do something like &quot;<strong>update mavg10:10 mavg score from scores_table</strong>&quot; but before doing the average of the past 10 items, drop the highest 2 and lowest 2 scores, then do average on the rest. And then have the result in a new column.</p>\n<p>I also read: <a href=\"https://code.kx.com/q/kb/programming-idioms/#how-do-i-apply-a-function-to-a-sequence-sliding-window\" rel=\"nofollow noreferrer\">https://code.kx.com/q/kb/programming-idioms/#how-do-i-apply-a-function-to-a-sequence-sliding-window</a> and I tried modifying the swin2 code snippet there, but I couldn't get it to work :(</p>\n",
    "is_answered": true,
    "view_count": 97,
    "answer_count": 1
  },
  {
    "title": "Index type 0x73726576 (&quot;vers&quot;) not recognized",
    "link": "https://stackoverflow.com/questions/79475544/index-type-0x73726576-vers-not-recognized",
    "tags": [
      "python",
      "artificial-intelligence",
      "data-science",
      "faiss",
      "retrieval-augmented-generation"
    ],
    "body": "<p>I have a chat bot app that I can run without any problem in my local environment.\nI can both run it locally on pycharm and I can run a docker container locally again. then I deploy it to koyeb using github link and koyeb reads the dockerfile.\nthen an error occurs:</p>\n<pre><code>RuntimeError: Error in faiss::Index* faiss::read_index(faiss::IOReader*, int) \nat /project/faiss/faiss/impl/index_read.cpp:1053: Index type 0x73726576 (&quot;vers&quot;) \nnot recognized\n</code></pre>\n<p>this is the line that I load faiss index</p>\n<pre><code>self.vector_gelenek = FAISS.load_local(\n&quot;vector_store/gelecek&quot;, self.embeddings_large, allow_dangerous_deserialization=True)\n</code></pre>\n<p>the faiss indexes are in github repo. there is no version mismatch in fais-langchain between what I use locally and what is written in the requirements.txt</p>\n<p>why is that? how can I resolve it?</p>\n",
    "is_answered": false,
    "view_count": 34,
    "answer_count": 0
  },
  {
    "title": "MiniBatchKMeans BERTopic not returning topics for half of data",
    "link": "https://stackoverflow.com/questions/79449168/minibatchkmeans-bertopic-not-returning-topics-for-half-of-data",
    "tags": [
      "python",
      "machine-learning",
      "nlp",
      "data-science",
      "topic-modeling"
    ],
    "body": "<p>I am trying to topic a dataset of tweets. I have around 50 million tweets. Unfortunately, such a large dataset will not fit in ram (even 128GB) due to the embeddings. Therefore, I have been working on making an incremental BERTopic as per the <a href=\"https://maartengr.github.io/BERTopic/getting_started/online/online.html\" rel=\"nofollow noreferrer\">docs</a></p>\n<p>As such:</p>\n<pre class=\"lang-none prettyprint-override\"><code>from bertopic.vectorizers import OnlineCountVectorizer\nfrom bertopic.vectorizers import ClassTfidfTransformer\nfrom sklearn.cluster import MiniBatchKMeans\nimport numpy as np\n\n\nclass SafeIncrementalPCA(IncrementalPCA):\n    def partial_fit(self, X, y=None):\n        # Ensure the input is contiguous and in float64\n        X = np.ascontiguousarray(X, dtype=np.float64)\n        return super().partial_fit(X, y)\n    \n    def transform(self, X):\n        result = super().transform(X)\n        # Force the output to be float64 and contiguous\n        return np.ascontiguousarray(result, dtype=np.float64)\n\n\nvectorizer_model = OnlineCountVectorizer(stop_words=&quot;english&quot;)\nctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True, bm25_weighting=True)\numap_model = SafeIncrementalPCA(n_components=100)\ncluster_model = MiniBatchKMeans(n_clusters=1000, random_state=0)\n\nfrom bertopic import BERTopic\n\ntopic_model = BERTopic(umap_model=umap_model,\n                       hdbscan_model=cluster_model,\n\nfor docs_delayed, emb_delayed in tqdm(zip(docs_partitions, embeddings_partitions), total=len(docs_partitions)):\n\n    docs_pdf = docs_delayed.compute()\n    emb_pdf = emb_delayed.compute()\n\n    docs = docs_pdf[&quot;text&quot;].tolist()\n    embeddings = np.vstack(emb_pdf['embeddings'].tolist())\n    \n    # Partial fit your model (make sure your model supports partial_fit, like many scikit-learn estimators do)\n    topic_model.partial_fit(docs, embeddings)\n\n</code></pre>\n<p>and then transforming the dataset into a SQL database:</p>\n<pre class=\"lang-none prettyprint-override\"><code>\nfor docs_delayed, emb_delayed in tqdm(zip(docs_partitions, embeddings_partitions), total=len(docs_partitions)):\n\n    docs_pdf = docs_delayed.compute()\n    emb_pdf = emb_delayed.compute()\n    docs = docs_pdf[&quot;text&quot;].tolist()\n    embeddings = np.vstack(emb_pdf['embeddings'].tolist())\n\n    # 3) Apply BERTopic on this shard\n    topics, probs = topic_model.transform(docs, embeddings)\n\n    # Save topics to DataFrame\n    df_topics = pd.DataFrame({\n        &quot;tweet_id&quot;: docs_pdf[&quot;id&quot;].tolist(),\n        &quot;topic&quot;: topics,\n        &quot;probability&quot;: probs\n    })\n\n    ## Merge &amp; store in DB\n    docs_pdf[&quot;topic&quot;] = df_topics[&quot;topic&quot;]\n    docs_pdf[&quot;probability&quot;] = df_topics[&quot;probability&quot;]\n    docs_pdf.to_sql(&quot;tweets&quot;, engine, if_exists=&quot;append&quot;, index=False)\n</code></pre>\n<p>I've been trying to do this for a quite a while and this is the closest working example I have gotten. The only issue is half of the dataset has null topics in the database at the end. From what I understand of the theory, MiniBatchKMeans should not have any outliers and therefore all tweets should be assigned to at least one topic, right? I've checked out the unclassified tweets in question and there is nothing in their doc that should suggest it would be hard to classify (relative to others that are classified).</p>\n<p>I would be very happy to hear any sort of suggestion on what could be going wrong and how I could fix this!</p>\n<p>Thanks!</p>\n",
    "is_answered": false,
    "view_count": 68,
    "answer_count": 0
  },
  {
    "title": "All Hyperparameters being fetched Strings instead of their actual data types",
    "link": "https://stackoverflow.com/questions/79443953/all-hyperparameters-being-fetched-strings-instead-of-their-actual-data-types",
    "tags": [
      "python",
      "data-science",
      "fastapi"
    ],
    "body": "<p>I wrote the following fast API code to train a classification model:\nHowever, all hyperparameters are being received as strings, even when they should be integers, floats, or booleans.</p>\n<p>Fast API Code:</p>\n<pre><code>class Parameter(BaseModel):\n    key: str\n    selectedValue: Union[str, int, float, bool]\n\nclass Model(BaseModel):\n    modelCategory: str\n    modelName: str\n    parameters: Optional[List[Parameter]] = None\n\nclass HyperparameterTuningRequest(BaseModel):\n    models: List[Model]\n\n@app.post(&quot;/tune_hyperparameters/&quot;)\ndef hyperparameter_tuning(request: HyperparameterTuningRequest):\n    for model in request.models:\n        if model.parameters:\n            for param in model.parameters:\n                print(f&quot;Parameter Key: {param.key}&quot;)\n                print(f&quot;Parameter Value: {param.selectedValue}&quot;)\n                print(f&quot;Parameter Type: {type(param.selectedValue)}&quot;)  # Always prints &lt;class 'str'&gt;\n</code></pre>\n<p>classification code:</p>\n<p>#Boolean conversion</p>\n<pre><code>def convert_to_bool(value):\n    if isinstance(value, bool):\n        return value\n    if isinstance(value, str):\n        return value.lower() == 'true'\n    if isinstance(value, int):\n        return bool(value)\n    return False\n\ndef convert_to_float(value, param_name, default_value):\n    try:\n        float_value = float(value)\n        if param_name == 'l1_ratio':\n            if 0 &lt;= float_value &lt;= 1:\n                return float_value\n            return default_value\n        elif float_value &lt;= 0:\n            return default_value\n        return float_value\n    except (ValueError, TypeError):\n        return default_value\n\nelif modelCategory == &quot;Classification&quot; and &quot;Random Forest&quot; in modelName:\n        if trial is not None:  # Optuna mode\n            # All suggestions must be independent\n            params = {\n                'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n                'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy']),\n                'max_depth': trial.suggest_int('max_depth', 1, 64),\n                'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2']),\n                'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),\n                'oob_score': trial.suggest_categorical('oob_score', [True, False]),\n                'random_state': trial.suggest_int('random_state', 1, 1000),\n                'min_weight_fraction_leaf': trial.suggest_float('min_weight_fraction_leaf', 0.0, 0.5),\n                'min_impurity_decrease': trial.suggest_float('min_impurity_decrease', 0.0, 0.5),\n                'class_weight': trial.suggest_categorical('class_weight', ['balanced', 'balanced_subsample', None]),\n                'n_jobs': -1  # Fixed value for parallelization\n            }\n            \n            return params\n        \n        else:  # Manual mode\n            # Default hyperparameters\n            default_params = {\n                'n_estimators': 100,\n                'criterion': 'gini',\n                'max_depth': None,\n                'min_samples_split': 2,\n                'min_samples_leaf': 1,\n                'min_weight_fraction_leaf': 0.0,\n                'max_features': 'sqrt',\n                'max_leaf_nodes': None,\n                'min_impurity_decrease': 0.0,\n                'bootstrap': True,\n                'oob_score': False,\n                'random_state': 42,\n                'warm_start': False,\n                'class_weight': 'balanced',\n                'n_jobs': -1\n            }\n\n            hyperparams = default_params.copy()\n            \n            if parameters:\n                for key, value in parameters.items():\n                    if value is not None and value != &quot;null&quot;:\n                        param_value = value.get('value') if isinstance(value, dict) else value\n                        \n                        # Special handling for null/None values\n                        if param_value == &quot;null&quot; or param_value == &quot;None&quot;:\n                            hyperparams[key] = None\n                            continue\n\n                        if parameters:\n                            for key, value in parameters.items():\n                                if value is not None and value != &quot;null&quot;:\n                                    param_value = value.get('value') if isinstance(value, dict) else value\n                                    \n                                    # Special handling for null/None values\n                                    if param_value == &quot;null&quot;:\n                                        hyperparams[key] = None\n                                        continue\n\n\n                        if param_value is not None:\n                            if key in ['bootstrap', 'oob_score', 'warm_start']:\n                                hyperparams[key] = convert_to_bool(param_value)\n                            elif key in ['n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf', 'max_leaf_nodes', 'n_jobs']:\n                                hyperparams[key] = convert_to_int(param_value) #if param_value != &quot;null&quot; else None\n                            elif key in ['min_weight_fraction_leaf', 'min_impurity_decrease']:\n                                hyperparams[key] = convert_to_float(param_value, key, default_params[key])\n                            elif key == 'random_state':\n                                hyperparams[key] = None if param_value == &quot;null&quot; else convert_to_int(param_value)\n                            else:\n                                hyperparams[key] = param_value\n\n            # Validate parameters\n            if hyperparams['n_estimators'] is not None and hyperparams['n_estimators'] &lt; 1:\n                raise ValueError(&quot;n_estimators must be greater than 0&quot;)\n            \n            if hyperparams['max_depth'] is not None and hyperparams['max_depth'] &lt; 1:\n                raise ValueError(&quot;max_depth must be greater than 0&quot;)\n            \n            if hyperparams['min_samples_split'] &lt; 2:\n                raise ValueError(&quot;min_samples_split must be greater than or equal to 2&quot;)\n            \n            if hyperparams['min_samples_leaf'] &lt; 1:\n                raise ValueError(&quot;min_samples_leaf must be greater than or equal to 1&quot;)\n            \n            if not 0.0 &lt;= hyperparams['min_weight_fraction_leaf'] &lt;= 0.5:\n                raise ValueError(&quot;min_weight_fraction_leaf must be between 0.0 and 0.5&quot;)\n            \n            if hyperparams['criterion'] not in ['gini', 'entropy']:\n                raise ValueError(&quot;criterion must be 'gini' or 'entropy'&quot;)\n            \n            if hyperparams['max_features'] not in [None, 'sqrt', 'log2'] and not isinstance(hyperparams['max_features'], (int, float)):\n                raise ValueError(&quot;max_features must be None, 'sqrt', 'log2', int or float&quot;)\n\n            # print(&quot;THE CHOSEN Random Forest HPs areeeee:&quot;, hyperparams)\n            return hyperparams\n</code></pre>\n<p><strong>I'm explicitly converting each hyperparameter to it's actual data type, but when I check the data types of the HPs fetched from the postman payload, it shows all the values as strings. Can someone help me figure out the issue?</strong></p>\n<p>Here's my payload:</p>\n<pre><code>&quot;parameters&quot;: \n{\n                            &quot;key&quot;: &quot;n_estimators&quot;,\n                            &quot;selectedValue&quot;: 100,\n                            &quot;_id&quot;: &quot;67a608e3a069a0fdf9096468&quot;\n                        },\n                        {\n                            &quot;key&quot;: &quot;criterion&quot;,\n                            &quot;selectedValue&quot;: &quot;gini&quot;,\n                            &quot;_id&quot;: &quot;67a608e3a069a0fdf9096469&quot;\n                        },\n                        {\n                            &quot;key&quot;: &quot;max_depth&quot;,\n                            &quot;selectedValue&quot;: &quot;None&quot;,\n                            &quot;_id&quot;: &quot;67a608e3a069a0fdf909646a&quot;\n                        },\n                        {\n                            &quot;key&quot;: &quot;min_samples_split&quot;,\n                            &quot;selectedValue&quot;: 2,\n                            &quot;_id&quot;: &quot;67a608e3a069a0fdf909646b&quot;\n                        },\n                        {\n                            &quot;key&quot;: &quot;min_samples_leaf&quot;,\n                            &quot;selectedValue&quot;: 1,\n                            &quot;_id&quot;: &quot;67a608e3a069a0fdf909646c&quot;\n                        },\n                        {\n                            &quot;key&quot;: &quot;min_weight_fraction_leaf&quot;,\n                            &quot;selectedValue&quot;: 0,\n                            &quot;_id&quot;: &quot;67a608e3a069a0fdf909646d&quot;\n                        },\n                        {\n                            &quot;key&quot;: &quot;max_features&quot;,\n                            &quot;selectedValue&quot;: &quot;sqrt&quot;,\n                            &quot;_id&quot;: &quot;67a608e3a069a0fdf909646e&quot;\n                        },\n                        {\n                            &quot;key&quot;: &quot;max_leaf_nodes&quot;,\n                            &quot;selectedValue&quot;: &quot;None&quot;,\n                            &quot;_id&quot;: &quot;67a608e3a069a0fdf909646f&quot;\n                        },\n                        {\n                            &quot;key&quot;: &quot;min_impurity_decrease&quot;,\n                            &quot;selectedValue&quot;: 0,\n                            &quot;_id&quot;: &quot;67a608e3a069a0fdf9096470&quot;\n                        },\n                        {\n                            &quot;key&quot;: &quot;bootstrap&quot;,\n                            &quot;selectedValue&quot;: &quot;True&quot;,\n                            &quot;_id&quot;: &quot;67a608e3a069a0fdf9096471&quot;\n                        },\n                        {\n                            &quot;key&quot;: &quot;oob_score&quot;,\n                            &quot;selectedValue&quot;: &quot;False&quot;,\n                            &quot;_id&quot;: &quot;67a608e3a069a0fdf9096472&quot;\n                        },\n                        {\n                            &quot;key&quot;: &quot;n_jobs&quot;,\n                            &quot;selectedValue&quot;: -1,\n                            &quot;_id&quot;: &quot;67a608e3a069a0fdf9096473&quot;\n                        },\n                        {\n                            &quot;key&quot;: &quot;random_state&quot;,\n                            &quot;selectedValue&quot;: 42,\n                            &quot;_id&quot;: &quot;67a608e3a069a0fdf9096474&quot;\n                        },\n                        {\n                            &quot;key&quot;: &quot;warm_start&quot;,\n                            &quot;selectedValue&quot;: &quot;False&quot;,\n                            &quot;_id&quot;: &quot;67a608e3a069a0fdf9096475&quot;\n                        },\n                        {\n                            &quot;key&quot;: &quot;class_weight&quot;,\n                            &quot;selectedValue&quot;: &quot;balanced&quot;,\n                            &quot;_id&quot;: &quot;67a608e3a069a0fdf9096476&quot;\n                        }\n</code></pre>\n<p>Output for the print staements:</p>\n<pre><code>model name isssssssssssssssssss Random Forest v1\nParameter Keyyyyyy: n_estimators\nParameter Valueeeee: 100\nParameter Value Typeeee: &lt;class 'str'&gt;\nParameter Keyyyyyy: criterion\nParameter Valueeeee: gini\nParameter Value Typeeee: &lt;class 'str'&gt;\nParameter Keyyyyyy: max_depth\nParameter Valueeeee: None\nParameter Value Typeeee: &lt;class 'str'&gt;...\n</code></pre>\n",
    "is_answered": false,
    "view_count": 52,
    "answer_count": 1
  },
  {
    "title": "Non-Symmetric Calculation for Correlation Matrix in Polars Plugin",
    "link": "https://stackoverflow.com/questions/79430794/non-symmetric-calculation-for-correlation-matrix-in-polars-plugin",
    "tags": [
      "rust",
      "data-science",
      "correlation",
      "python-polars",
      "polars-plugins"
    ],
    "body": "<p>I have written a Polars plugin to calculate the symmetric correlation matrix inspired by <a href=\"https://github.com/abstractqqq/polars_ds_extension\" rel=\"nofollow noreferrer\">Polars DS</a> project.</p>\n<p>On the Rust side I have:</p>\n<pre class=\"lang-rust prettyprint-override\"><code>#![allow(unused_imports)]\nuse polars::prelude::*;\nuse pyo3_polars::{derive::polars_expr, export::polars_core::random};\nuse serde::{Deserialize, Serialize};\n\nuse kendalls::tau_b;\n\n#[derive(Deserialize)]\n#[serde(rename_all = &quot;lowercase&quot;)]\nenum CorrMethod {\n    Pearson,\n    Spearman,\n    Kendall,\n}\n\n#[derive(Deserialize)]\n#[serde(rename_all = &quot;lowercase&quot;)]\nenum RankMethodSerde {\n    Average,\n    Min,\n    Max,\n    Dense,\n    Ordinal,\n}\n\nimpl From&lt;RankMethodSerde&gt; for RankMethod {\n    fn from(method: RankMethodSerde) -&gt; Self {\n        match method {\n            RankMethodSerde::Average =&gt; RankMethod::Average,\n            RankMethodSerde::Min =&gt; RankMethod::Min,\n            RankMethodSerde::Max =&gt; RankMethod::Max,\n            RankMethodSerde::Dense =&gt; RankMethod::Dense,\n            RankMethodSerde::Ordinal =&gt; RankMethod::Ordinal,\n        }\n    }\n}\n#[derive(Deserialize)]\nstruct Kwargs {\n    method: CorrMethod,\n    min_periods: u32,\n    rank_method: Option&lt;RankMethodSerde&gt;,\n    descending_rank: Option&lt;bool&gt;,\n}\n\n#[polars_expr(output_type = Float32)]\npub fn corr(series: &amp;[Series], kwargs: Kwargs) -&gt; PolarsResult&lt;Series&gt; {\n    let col_name = series[0].name();\n\n    let (series_x, series_y) = {\n        let series_x_full = &amp;series[0];\n        let series_y_full = &amp;series[1];\n        let mask = series_x_full.is_not_null() &amp; series_y_full.is_not_null();\n        if mask.sum().unwrap_or(0) &lt; kwargs.min_periods {\n            return Ok(Float32Chunked::new(col_name.clone(), [None]).into_series());\n        }\n\n        (&amp;series_x_full.filter(&amp;mask)?, &amp;series_y_full.filter(&amp;mask)?)\n    };\n\n    let corr = match kwargs.method {\n        CorrMethod::Pearson =&gt; pearson_corr(\n            series_x.cast(&amp;DataType::Float32)?.f32()?,\n            series_y.cast(&amp;DataType::Float32)?.f32()?,\n        ),\n        CorrMethod::Spearman =&gt; {\n            let rank_method: RankMethod = kwargs.rank_method.unwrap().into();\n            let descending_rank = kwargs.descending_rank.unwrap();\n\n            let ranked_series_x = &amp;series_x.rank(\n                RankOptions {\n                    method: rank_method,\n                    descending: descending_rank,\n                },\n                None,\n            );\n            let ranked_series_y = &amp;series_y.rank(\n                RankOptions {\n                    method: rank_method,\n                    descending: descending_rank,\n                },\n                None,\n            );\n\n            spearman_corr(\n                ranked_series_x.cast(&amp;DataType::Float32)?.f32()?,\n                ranked_series_y.cast(&amp;DataType::Float32)?.f32()?,\n            )\n        }\n        _ =&gt; unimplemented!(),\n    };\n\n    Ok(Series::from_vec(col_name.clone(), vec![corr]))\n}\n\nfn pearson_corr(array_x: &amp;ChunkedArray&lt;Float32Type&gt;, array_y: &amp;ChunkedArray&lt;Float32Type&gt;) -&gt; f32 {\n    let mean_x = array_x.mean().unwrap();\n    let mean_y = array_y.mean().unwrap();\n\n    let s_x = array_x - mean_x;\n    let s_y = array_y - mean_y;\n\n    let s_xy = (&amp;s_x * &amp;s_y).sum().unwrap();\n\n    let s_x_sq = (&amp;s_x * &amp;s_x).sum().unwrap();\n    let s_y_sq = (&amp;s_y * &amp;s_y).sum().unwrap();\n\n    s_xy / (s_x_sq * s_y_sq).sqrt()\n}\n\nfn spearman_corr(\n    ranked_array_x: &amp;ChunkedArray&lt;Float32Type&gt;,\n    ranked_array_y: &amp;ChunkedArray&lt;Float32Type&gt;,\n) -&gt; f32 {\n    pearson_corr(ranked_array_x, ranked_array_y)\n}\n</code></pre>\n<p>On the Python side I have:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from typing import Literal, Optional\n\nfrom polars import col\nimport polars.selectors as cs\n\nimport polars as pl\n\nfrom ..utils import polars_plugin, parse_to_expr\n\n\ndef _corr(\n    x_vec: pl.Expr,\n    y_vec: pl.Expr,\n    method: Literal[&quot;pearson&quot;, &quot;kendall&quot;, &quot;spearman&quot;] = &quot;pearson&quot;,\n    min_periods: int = 1,\n    rank_method: Literal[&quot;average&quot;, &quot;min&quot;, &quot;max&quot;, &quot;dense&quot;, &quot;ordinal&quot;]\n    | None = &quot;average&quot;,\n    descending_rank: bool | None = False,\n    rounding: int | None = None,\n) -&gt; pl.Expr:\n    corr_expr = polars_plugin(\n        &quot;corr&quot;,\n        args=[parse_to_expr(x_vec), parse_to_expr(y_vec)],\n        kwargs={\n            &quot;method&quot;: method,\n            &quot;min_periods&quot;: min_periods,\n            &quot;rank_method&quot;: rank_method,\n            &quot;descending_rank&quot;: descending_rank,\n        },\n        returns_scalar=True,\n        changes_length=True,\n    )\n    return corr_expr.round(rounding) if rounding is not None else corr_expr\n\n\ndef polars_corr(\n    data: pl.DataFrame | pl.LazyFrame,\n    *,\n    method: Literal[&quot;pearson&quot;, &quot;kendall&quot;, &quot;spearman&quot;] = &quot;pearson&quot;,\n    min_periods: int = 1,\n    rank_method: Literal[&quot;average&quot;, &quot;min&quot;, &quot;max&quot;, &quot;dense&quot;, &quot;ordinal&quot;]\n    | None = &quot;average&quot;,\n    descending_rank: bool | None = False,\n    rounding: int | None = None,\n) -&gt; pl.DataFrame:\n    data = data.lazy()\n    data = data.select(cs.numeric()).cast(pl.Float32)\n\n    columns = data.collect_schema().names()\n\n    corr_frames = [\n        data.select(\n            [\n                pl.lit(x).alias(&quot;feature&quot;),\n                *[\n                    _corr(\n                        x,\n                        y,\n                        method,\n                        min_periods,\n                        rank_method,\n                        descending_rank,\n                        rounding,\n                    ).alias(y)\n                    for y in columns\n                ],\n            ]\n        )\n        for x in columns\n    ]\n\n    # Execute all computations in parallel and combine\n    return pl.concat(pl.collect_all(corr_frames)).select([&quot;feature&quot;, *columns])\n</code></pre>\n<p>The program matches the behavior of <code>pd.DataFrame.corr()</code> and outputs the same table. I would like to gather feedback from the community:</p>\n<ol>\n<li>While the pearson method is blazingly fast, and significantly faster than that of pandas, the spearman method suffers. Is it because of the implementation of the <code>RankMethod</code>? How can I improve?</li>\n<li>The current implementation to assemble the scalar <code>DataFrame</code>s on the Python side double work on the pairwise correlations. How can I avoid these unnecessary calculations?</li>\n</ol>\n",
    "is_answered": false,
    "view_count": 94,
    "answer_count": 0
  },
  {
    "title": "Create a graphic for a multiindexed data frame",
    "link": "https://stackoverflow.com/questions/79373414/create-a-graphic-for-a-multiindexed-data-frame",
    "tags": [
      "python",
      "dataframe",
      "jupyter-notebook",
      "seaborn",
      "data-science"
    ],
    "body": "<p>This is my first question. I don't find the way to create a graphic from a multi indexed data frame so I can show the development goes for the answers of a test. It keeps throwing errors. First part of the code just explains how I created the data frame so you all can have some context. the problem comes when I have to address properly the entries of my dictionary to create a graphic.</p>\n<pre><code>import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n[enter image description here](https://i.sstatic.net/BH0Zvfxz.png)\n# Crear un diccionario para almacenar las estad\u00edsticas\nestadisticas = {\n    'M\u00ednimo': {\n        ('l\u00ednea Base', 'Autocompasi\u00f3n'): df[columnas_autocompasion_1].min().min(),\n        ('l\u00ednea Base', 'Burnout'): df[columnas_burnout_1].min().min(),\n        ('l\u00ednea Base', 'Apoyo Social'): df[columnas_apoyo_social_1].min().min(),\n        ('3 meses', 'Autocompasi\u00f3n'): df[columnas_autocompasion_2].min().min(),\n        ('3 meses', 'Burnout'): df[columnas_burnout_2].min().min(),\n        ('3 meses', 'Apoyo Social'): df[columnas_apoyo_social_2].min().min(),\n        ('6 meses', 'Autocompasi\u00f3n'): df[columnas_autocompasion_3].min().min(),\n        ('6 meses', 'Burnout'): df[columnas_burnout_3].min().min(),\n        ('6 meses', 'Apoyo Social'): df[columnas_apoyo_social_3].min().min()\n    },\n    'Promedio': {\n        ('l\u00ednea Base', 'Autocompasi\u00f3n'): df[columnas_autocompasion_1].mean().mean(),\n        ('l\u00ednea Base', 'Burnout'): df[columnas_burnout_1].mean().mean(),\n        ('l\u00ednea Base', 'Apoyo Social'): df[columnas_apoyo_social_1].mean().mean(),\n        ('3 meses', 'Autocompasi\u00f3n'): df[columnas_autocompasion_2].mean().mean(),\n        ('3 meses', 'Burnout'): df[columnas_burnout_2].mean().mean(),\n        ('3 meses', 'Apoyo Social'): df[columnas_apoyo_social_2].mean().mean(),\n        ('6 meses', 'Autocompasi\u00f3n'): df[columnas_autocompasion_3].mean().mean(),\n        ('6 meses', 'Burnout'): df[columnas_burnout_3].mean().mean(),\n        ('6 meses', 'Apoyo Social'): df[columnas_apoyo_social_3].mean().mean()\n    },\n    'Desviaci\u00f3n T\u00edpica': {\n        ('l\u00ednea Base', 'Autocompasi\u00f3n'): df[columnas_autocompasion_1].std().mean(),\n        ('l\u00ednea Base', 'Burnout'): df[columnas_burnout_1].std().mean(),\n        ('l\u00ednea Base', 'Apoyo Social'): df[columnas_apoyo_social_1].std().mean(),\n        ('3 meses', 'Autocompasi\u00f3n'): df[columnas_autocompasion_2].std().mean(),\n        ('3 meses', 'Burnout'): df[columnas_burnout_2].std().mean(),\n        ('3 meses', 'Apoyo Social'): df[columnas_apoyo_social_2].std().mean(),\n        ('6 meses', 'Autocompasi\u00f3n'): df[columnas_autocompasion_3].std().mean(),\n        ('6 meses', 'Burnout'): df[columnas_burnout_3].std().mean(),\n        ('6 meses', 'Apoyo Social'): df[columnas_apoyo_social_3].std().mean()\n    },\n    'Percentil 25': {\n        ('l\u00ednea Base', 'Autocompasi\u00f3n'): df[columnas_autocompasion_1].quantile(0.25).mean(),\n        ('l\u00ednea Base', 'Burnout'): df[columnas_burnout_1].quantile(0.25).mean(),\n        ('l\u00ednea Base', 'Apoyo Social'): df[columnas_apoyo_social_1].quantile(0.25).mean(),\n        ('3 meses', 'Autocompasi\u00f3n'): df[columnas_autocompasion_2].quantile(0.25).mean(),\n        ('3 meses', 'Burnout'): df[columnas_burnout_2].quantile(0.25).mean(),\n        ('3 meses', 'Apoyo Social'): df[columnas_apoyo_social_2].quantile(0.25).mean(),\n        ('6 meses', 'Autocompasi\u00f3n'): df[columnas_autocompasion_3].quantile(0.25).mean(),\n        ('6 meses', 'Burnout'): df[columnas_burnout_3].quantile(0.25).mean(),\n        ('6 meses', 'Apoyo Social'): df[columnas_apoyo_social_3].quantile(0.25).mean()\n    },\n    'Mediana': {\n        ('l\u00ednea Base', 'Autocompasi\u00f3n'): df[columnas_autocompasion_1].median().mean(),\n        ('l\u00ednea Base', 'Burnout'): df[columnas_burnout_1].median().mean(),\n        ('l\u00ednea Base', 'Apoyo Social'): df[columnas_apoyo_social_1].median().mean(),\n        ('3 meses', 'Autocompasi\u00f3n'): df[columnas_autocompasion_2].median().mean(),\n        ('3 meses', 'Burnout'): df[columnas_burnout_2].median().mean(),\n        ('3 meses', 'Apoyo Social'): df[columnas_apoyo_social_2].median().mean(),\n        ('6 meses', 'Autocompasi\u00f3n'): df[columnas_autocompasion_3].median().mean(),\n        ('6 meses', 'Burnout'): df[columnas_burnout_3].median().mean(),\n        ('6 meses', 'Apoyo Social'): df[columnas_apoyo_social_3].median().mean()\n    },\n    'Percentil 75': {\n        ('l\u00ednea Base', 'Autocompasi\u00f3n'): df[columnas_autocompasion_1].quantile(0.75).mean(),\n        ('l\u00ednea Base', 'Burnout'): df[columnas_burnout_1].quantile(0.75).mean(),\n        ('l\u00ednea Base', 'Apoyo Social'): df[columnas_apoyo_social_1].quantile(0.75).mean(),\n        ('3 meses', 'Autocompasi\u00f3n'): df[columnas_autocompasion_2].quantile(0.75).mean(),\n        ('3 meses', 'Burnout'): df[columnas_burnout_2].quantile(0.75).mean(),\n        ('3 meses', 'Apoyo Social'): df[columnas_apoyo_social_2].quantile(0.75).mean(),\n        ('6 meses', 'Autocompasi\u00f3n'): df[columnas_autocompasion_3].quantile(0.75).mean(),\n        ('6 meses', 'Burnout'): df[columnas_burnout_3].quantile(0.75).mean(),\n        ('6 meses', 'Apoyo Social'): df[columnas_apoyo_social_3].quantile(0.75).mean()\n    },\n    'M\u00e1ximo': {\n        ('l\u00ednea Base', 'Autocompasi\u00f3n'): df[columnas_autocompasion_1].max().mean(),\n        ('l\u00ednea Base', 'Burnout'): df[columnas_burnout_1].max().mean(),\n        ('l\u00ednea Base', 'Apoyo Social'): df[columnas_apoyo_social_1].max().mean(),\n        ('3 meses', 'Autocompasi\u00f3n'): df[columnas_autocompasion_2].max().mean(),\n        ('3 meses', 'Burnout'): df[columnas_burnout_2].max().mean(),\n        ('3 meses', 'Apoyo Social'): df[columnas_apoyo_social_2].max().mean(),\n        ('6 meses', 'Autocompasi\u00f3n'): df[columnas_autocompasion_3].max().mean(),\n        ('6 meses', 'Burnout'): df[columnas_burnout_3].max().mean(),\n        ('6 meses', 'Apoyo Social'): df[columnas_apoyo_social_3].max().mean()\n    }\n}\n\n# Crear el DataFrame con MultiIndex en las columnas\ndf_stats = pd.DataFrame(estadisticas)\n\n# Transformar el DataFrame en formato largo (long format)\ndf_long = df_stats.stack(level=0).reset_index()\ndf_long.columns = ['Estad\u00edstica', 'Momento', 'Escala', 'Valor']\n\n# THIS IS THE PART THAT IT IS NOT WORKING \ng = sns.FacetGrid(df_long, row='Estad\u00edstica', hue='Momento', aspect=2, height=4)\ng.map(sns.scatterplot, 'Escala', 'Valor').add_legend()\ng.set_titles(&quot;{row_name}&quot;)\ng.set_axis_labels(&quot;Escala&quot;, &quot;Valor&quot;)\nplt.show()```\n</code></pre>\n",
    "is_answered": false,
    "view_count": 41,
    "answer_count": 0
  },
  {
    "title": "Re index a list of values in a DataFrame",
    "link": "https://stackoverflow.com/questions/79366097/re-index-a-list-of-values-in-a-dataframe",
    "tags": [
      "python",
      "pandas",
      "data-science",
      "etl"
    ],
    "body": "<p>I'm doing an ETL process for a DataFrame. The original table had NaN values, duplicates and more things that I drop. But now the &quot;old index&quot; show gaps between the values, for example from the index number 8 to index number 10. So I tried <code>set_index</code>, <code>reindex</code>, <code>len(range(df.index))</code> for set up the true value of the index but I can't.</p>\n<pre><code>from datetime import datetime, timedelta\nimport pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\nimport numpy as np\n\nurl = &quot;https://www.myfxbook.com/forex-economic-calendar&quot;\ncodificacion='utf-8'\n\nresponse = requests.get(url)\nresponse.encoding = codificacion\nsoup = BeautifulSoup(response.text, 'html.parser')\n\ntablas = pd.read_html(str(soup), encoding=codificacion)\ntabla = tablas[0]\n# tabla\ntabla.rename(columns={'Unnamed: 3':'S\u00edmbolo'}, inplace=True)\n\ntabla.drop(tabla['Unnamed: 2'].dropna().index, inplace=True)\n\ntabla.drop(columns=['All  None', 'Unnamed: 2'], inplace=True) #para poner otros datos solo se pone columns=['','',''...]\n\ntabla.drop(tabla[tabla['Impact'] == 'Low'].index, inplace=True)\n\nhelp(pd.RangeIndex)\n</code></pre>\n<p>Additionally, I'm new in this of programming, Do your consider that is better read the page of duoumentation, use the command of help for specific cases (i think that is lot more information in this way) or look in foros por solutions of community? I'm writing here because I want force me to not use chatgpt every time that I'm lock, but that is actually good?</p>\n",
    "is_answered": false,
    "view_count": 34,
    "answer_count": 0
  },
  {
    "title": "X (Twitter) webscraping using twikit. How to validate if auth_token expired?",
    "link": "https://stackoverflow.com/questions/79361278/x-twitter-webscraping-using-twikit-how-to-validate-if-auth-token-expired",
    "tags": [
      "python",
      "web-scraping",
      "twitter",
      "data-science"
    ],
    "body": "<p>I am trying to use twikit python lib to webscrape X (formerly known as twitter).</p>\n<p>I am unable to validate if the auth_token expired or not.</p>\n<p>Here is the code snippet for your reference:</p>\n<pre><code>from twikit import Client, TooManyRequests\nimport time\nfrom datetime import datetime\nimport json\nimport csv\nfrom configparser import ConfigParser\nfrom random import randint\nimport asyncio\nimport tracemalloc\nimport os\nimport jwt\nimport tracemalloc\ntracemalloc.start()\n\n\n# Login credentials\nconfig = ConfigParser()\nconfig.read('config.ini')\nusername = config['X']['username']\npassword = config['X']['password']\nemail = config['X']['email']\n\n# Create a client\nclient = Client(language='en-US')\n\n# Define the file path\ncookies_file_path = 'cookies.json'\n\nasync def login_client():\n        await client.login(auth_info_1=username, auth_info_2=email, password=password)\n\nif not os.path.exists(cookies_file_path):\n# Function to login and generate new token\n    asyncio.run(login_client())    \n    client.save_cookies(cookies_file_path) \nelse:\n    # Check if the auth_token is valid or expired\n    token = jwt.decode(client.auth_token, verify=False)['exp']\n    current_time = int(datetime.now().timestamp())\n    if token &lt; current_time:\n        # Generate a new token\n        asyncio.run(login_client())\n        client.save_cookies(cookies_file_path)\n    else:\n         # Load existing cookies\n         client.load_cookies(cookies_file_path)\n    \n\n# Get tweets\nMINIMUM_TWEETS = 10\nQUERY = 'Nvidia stock' #input(&quot;Enter the query: &quot;)\ntweet_data = [] # [tweet_count, tweet.user.name, tweet.text, tweet.created_at, tweet.updated_at, tweet.retweet_count, tweet.favorite_count]\n\nglobal tweet_count\ntweet_count = 0\n\nasync def get_tweets():\n    tweets = await client.search_tweet(query=QUERY, product='Top', count=MINIMUM_TWEETS)\n    global tweet_count\n    for tweet in tweets:\n        tweet_count += 1\n        tweet_data = [tweet_count, tweet.user.name, tweet.text, tweet.created_at, tweet.retweet_count, tweet.favorite_count]\n        print(tweet_data)\n        # with open('tweets.csv', 'a', newline='') as file:\n        #     writer = csv.writer(file)\n        #     writer.writerow(tweet_data)\n    \n\nasyncio.run(get_tweets())\nprint(f&quot;Total tweets: {tweet_count}&quot;)\n</code></pre>\n<p>I am doing great so far but I want a validation with the auth_token saved in my cookies.json. I want to validate if it is expired, and if it is, then relogin, or load the saved cookies from the past.</p>\n<p>I have been trying to use PyJwt Python lib to decode the auth_token and fetch the 'exp' attribute, but it needs a secret_key. This is where I am stuck right now. How do I get the secret_key? I am unable to figure out where to get it. I mean, I am using twikit, so I am not actively storing any secret key and using requests to perform any API calls.</p>\n",
    "is_answered": false,
    "view_count": 476,
    "answer_count": 1
  },
  {
    "title": "Dask merge two big dataframes that do not fit into memory",
    "link": "https://stackoverflow.com/questions/79355880/dask-merge-two-big-dataframes-that-do-not-fit-into-memory",
    "tags": [
      "python",
      "pandas",
      "data-science",
      "bigdata",
      "dask"
    ],
    "body": "<p>I need to merge to big dataframes in dask but my jupyter notebook kernel crashes due to lack of memory. I've got 64GB of RAM but the original datasets are so sparse (medical claims data) and that join leads up to over a 100 million rows. I've read that Dask would be a good tool to handle data that can't fit into memory, but I'm not sure if I use it correctly. I'm reading in all my csv files and also used blocksize to repartition them for Dask. I als included a function to save the intermediate steps as parquet. Is there a way for example to save the intermediate results during the last merge to the disk in order to free up some memory during that procedure. Or are there other tools that would fit better for this problem?</p>\n<pre><code>import dask.dataframe as dd\n\ndef save_and_reload(df, file_name):\n    &quot;&quot;&quot;\n    Saves the Dask DataFrame to a Parquet file and reloads it.\n    &quot;&quot;&quot;\n    df = df.repartition(npartitions=10)  # Adjust partitions for better performance\n    df.to_parquet(file_name, write_index=False)\n    return dd.read_parquet(file_name)\n\ndef main():\n    # Read 11 CSV with smaller partitions\n    df = dd.read_csv(\n        file_path,\n        sep='\\t',\n        dtype=dtypes.get(table_name, None),\n        parse_dates=parse_dates.get(table_name, None),\n        assume_missing=True,\n        blocksize=&quot;16MB&quot;\n    )\n\n    # Merge step-by-step with intermediate saving for 11 .csvs\n    df_merged = dd.merge(dataframes['insurance_information'], dataframes['insurants'], on='pid', how='left')\n    df_merged = save_and_reload(df_merged, 'intermediate_1.parquet')\n    \n    # This goes on for 10 more .csvs\n\n    df_merged = dd.merge(df_merged, dataframes['outpatient_fees'], on='pid', how='left')\n    df_merged = save_and_reload(df_merged, 'intermediate_10.parquet')\n\n    # After this merge the kernel crashes\n    df_merged = dd.merge(df_merged, dataframes['inpatient_fees'], on='pid', how='left')\n    df_merged = save_and_reload(df_merged, 'final_result.parquet')\n\nif __name__ == &quot;__main__&quot;:\n    main()\n</code></pre>\n",
    "is_answered": false,
    "view_count": 58,
    "answer_count": 0
  },
  {
    "title": "Azure Video Indexer API: &quot;USER_NOT_ALLOWED&quot; Error When Checking Blurring Job Status After Completion of Bluring",
    "link": "https://stackoverflow.com/questions/79317603/azure-video-indexer-api-user-not-allowed-error-when-checking-blurring-job-sta",
    "tags": [
      "python",
      "azure",
      "data-science",
      "azure-video-indexer"
    ],
    "body": "<p><a href=\"https://i.sstatic.net/gwcKNNDI.jpg\" rel=\"nofollow noreferrer\">Current permissions</a></p>\n<p>I am working with the Azure Video Indexer API in Python to process videos, specifically to index and blur them. While both the indexing and blurring processes run successfully up to a point, I encounter an issue when attempting to retrieve the status of the blurring job after it completes.</p>\n<p>When querying the Location URL (provided by the indexing job for checking the blurring job status), I receive valid responses until the blurring process reaches 100% completion. After that, every request returns the following error:</p>\n<pre><code>{\n    &quot;ErrorType&quot;: &quot;USER_NOT_ALLOWED&quot;,\n    &quot;Message&quot;: &quot;User has no permission on account&quot;\n}\n\n</code></pre>\n<p>The access token I am using is generated with the following permissions:</p>\n<ul>\n<li>Permission: Contributor</li>\n<li>Scope: Account</li>\n</ul>\n<p>I suspect this issue is related to IAM (Identity and Access Management), but I have already assigned all meaningful permissions I can think of.</p>\n<p><strong>My Goal</strong>\nMy ultimate goal is to download the video after the blurring process is complete. However, I cannot query the API to get the status or the download link due to the above error.</p>\n<p><strong>Job URL Format</strong>\nHere\u2019s the job_url I\u2019m using to check the blurring job status:</p>\n<pre><code>https://api.videoindexer.ai/{self.location}/Accounts/{self.account_id}/Videos/{video_id}/redact/?accessToken={self.access_token}&amp;name={name}\n</code></pre>\n<p><strong>Current Code</strong>\nBelow is the Python code I use to query the job status. It works perfectly until the blurring process is 100% complete:</p>\n<pre><code>def get_video_blurring_processing_status(self, job_url):\n    try:\n        # Append the access token to the job URL\n        url_with_token = f&quot;{job_url}?accessToken={self.access_token}&quot;\n        \n        # Send the request to the job status endpoint\n        response = requests.get(url_with_token)\n        \n        if response.status_code == 200:\n            data = response.json()\n            processing_progress = data.get(&quot;progress&quot;, &quot;Unknown&quot;)\n            state = data.get(&quot;state&quot;, &quot;Unknown&quot;)\n            return {&quot;processing_progress&quot;: processing_progress, &quot;state&quot;: state}\n        \n        else:\n            # Temporary hack\n            # return {&quot;processing_progress&quot;: 100, &quot;state&quot;: &quot;Succeeded&quot;}\n            print(f&quot;Failed to get job status. Status Code: {response.status_code}, Response: {response.content}&quot;)\n            return None\n    \n    except Exception as e:\n        print(f&quot;\u274c Error while checking blurring status: {e}&quot;)\n        return None\n</code></pre>\n<p><strong>What I\u2019ve Tried</strong></p>\n<ul>\n<li>Verified that the access token has Contributor permissions and Account scope.</li>\n<li>Confirmed that the token works for other API calls, such as retrieving video indexing status (up until status = 100%).</li>\n<li>Attempted regenerating the access token multiple times.</li>\n</ul>\n<p><strong>Request for Help</strong></p>\n<ul>\n<li>Has anyone encountered this issue when using the Azure Video Indexer API?</li>\n<li>Is there an additional permission or configuration needed to access the blurred video status or download link?</li>\n<li>Could this be a limitation of the Contributor role, or something else related to how the access token interacts with post-processing operations?</li>\n</ul>\n<p>Any insights or suggestions would be greatly appreciated...</p>\n",
    "is_answered": true,
    "view_count": 97,
    "answer_count": 1
  },
  {
    "title": "h2o Dataframe Causes Function to Hang h2o.remove()",
    "link": "https://stackoverflow.com/questions/79316006/h2o-dataframe-causes-function-to-hang-h2o-remove",
    "tags": [
      "python",
      "dataframe",
      "garbage-collection",
      "data-science",
      "h2o"
    ],
    "body": "<p>I have a data science project utilizing h2o where I setup a loop of heatmap visualizations for explain-ability and to measure overfitting. I want to be able to call the heatmap via a reusable function so I can return the heatmap to display alone or export a series of them to PDF. When I return the figure from the function it hangs. I've debugged by checking the time prior to return and the first statement after return and it takes around 200 seconds.</p>\n<p>I spent a bunch of time trying to debug the timing but no matter what I did...it didn't return for 200 seconds. Inevitably, I figured out that there was some sort of garbage collection happening with the h2o dataframe when the function returned. I was able to add the line h2o.remove(shocked_hf) to the function to confirm this. This statement now took 200 seconds and the function returned fine. Here is a snippet of code that shows how the H2OFrame was created:</p>\n<pre><code># create dataframe with simulated data to test model\nshocked_df = pd.DataFrame(shocked_rows)\n# this h2o frame is only 625 rows by 107 columns\nshocked_hf = h2o.H2OFrame(shocked_df)\n# this next statement takes around 200 seconds\nh2o.remove(shocked_hf)\n</code></pre>\n<p>What is going on here? I'd like to call this function multiple times so there is really no reason to clean up this variable. Even if you do clean it up, there has to be a faster way. I've seen some thoughts of using manual garbage collection, however I think that will just introduce other issues. I think I may need to include the loop inside the function as a stopgap solution, but this just doesn't feel right.</p>\n",
    "is_answered": false,
    "view_count": 32,
    "answer_count": 0
  },
  {
    "title": "ERROR: raise ValueError(f&quot;Shape of passed values is {passed}, indices imply {implied}&quot;) when using GridSearchCV",
    "link": "https://stackoverflow.com/questions/79314154/error-raise-valueerrorfshape-of-passed-values-is-passed-indices-imply-imp",
    "tags": [
      "data-science",
      "pipeline",
      "one-hot-encoding",
      "gridsearchcv"
    ],
    "body": "<p>I tried to use <code>GridSearchCV</code> to train pipeline model but I met this error. When I trained without <code>GridSearchCV</code>, there is no error, so I don't understand what is the problem.</p>\n<pre><code>class FeatureEngineering(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, x):\n        transformed_data = x.toarray()\n        feature_names = preprocessor_1.get_feature_names_out()\n        df_transformed = pd.DataFrame(x.toarray(), columns=preprocessor_1.get_feature_names_out())\n\n        if transformed_data.shape[1] != len(feature_names):\n          raise ValueError(f&quot;Mismatch: Data has {transformed_data.shape[1]} columns but {len(feature_names)} feature names.&quot;)\n    \n        df_transformed['total_room'] = df_transformed['num_dcr__bedrooms'] + df_transformed['num_dcr__bathrooms']\n        df_transformed['total_room_add_floors'] = df_transformed['total_room'] + df_transformed['num_dcr__floors']\n        df_transformed['bedrooms_multi_area'] = df_transformed['num_dcr__bedrooms'] * df_transformed['num_cont__area']\n        df_transformed['bathrooms_multi_area'] = df_transformed['num_dcr__bathrooms'] * df_transformed['num_cont__area']\n        df_transformed['area_floors'] = df_transformed['num_cont__area'] * df_transformed['num_dcr__floors']\n        print(df_transformed.shape)\n        return df_transformed\n\nnum_dcr_cols = ['bathrooms', 'bedrooms', 'floors']\nnum_cont_cols = ['area', 'frontage', 'access_road']\ncat_nom_cols = ['legal_status', 'province']\ncat_ord_cols = ['furniture_state']\ncomp_fe = ['bathrooms', 'bedrooms', 'floors', 'area']\n\ncat_ord = [&quot;Full&quot;, &quot;Basic&quot;, &quot;No furniture&quot;]\n\n# descret cols\nnum_dcr_transformer = Pipeline(steps=[\n    ('imputer', dcr_imputer),\n    ('scaler', StandardScaler())\n])\n\n# continious cols\nnum_cont_transformer = Pipeline(steps=[\n    ('imputer', cont_imputer),\n    ('scaler', StandardScaler())\n])\n\n# nominal cols\ncat_nom_transformer = Pipeline(steps=[\n    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# ordinal cols\ncat_ord_transformer = Pipeline(steps=[\n    ('encoder', OrdinalEncoder(categories=[cat_ord]))\n])\n\n# new cols\nfe = Pipeline(steps=[\n    ('fe', FeatureEngineering())\n])\n\n# ColumnTransformer\npreprocessor_1 = ColumnTransformer(\n    transformers=[\n        ('cat_nom', cat_nom_transformer, cat_nom_cols),\n        ('cat_ord', cat_ord_transformer, cat_ord_cols),\n        ('num_dcr', num_dcr_transformer, num_dcr_cols),\n        ('num_cont', num_cont_transformer, num_cont_cols),\n    ])\n\npreprocessor_2 = Pipeline(steps=[\n    ('preprocessor_1', preprocessor_1),\n    ('fe', fe)\n])\n\npipeline_rf = Pipeline(\n    steps=[\n        ('preprocessor_2', preprocessor_2),\n        ('rf', RandomForestRegressor(random_state=1))\n    ]\n)\n\nparams = {\n    &quot;rf__n_estimators&quot;: [50,100],\n    &quot;rf__max_depth&quot;: [10,20],\n    &quot;rf__min_samples_split&quot;: [5, 10]\n}\n\nmodel_rf_cv = GridSearchCV(pipeline_rf, param_grid=params, cv=3, n_jobs=-1,verbose=4, scoring=&quot;r2&quot;)\nmodel_rf_cv.fit(x_train, y_train)\ny_pred = model_rf_cv.predict(x_test)\nprint(&quot;The best param: &quot;, model_rf_cv.best_params_)\nprint(&quot;Performance on validation set: &quot;, model_rf_cv.best_score_)\nprint(&quot;Perfromance on test set: &quot;, r2_score(y_test, y_pred))\n</code></pre>\n<p>This is my error:</p>\n<blockquote>\n<p>raise ValueError(f&quot;Shape of passed values is {passed}, indices imply {implied}&quot;)\nValueError: Shape of passed values is (12610, 54), indices imply (12610, 55)</p>\n</blockquote>\n<p>I tried train without <code>GridSearchCV</code> and it worked well. I do not know where the error come from, this is my data link:\n<a href=\"https://www.kaggle.com/datasets/nguyentiennhan/vietnam-housing-dataset-2024\" rel=\"nofollow noreferrer\">Link data</a></p>\n",
    "is_answered": false,
    "view_count": 36,
    "answer_count": 0
  },
  {
    "title": "How to manage memory consumption in deep learning models?",
    "link": "https://stackoverflow.com/questions/79284426/how-to-manage-memory-consumption-in-deep-learning-models",
    "tags": [
      "deep-learning",
      "pytorch",
      "data-science",
      "large-language-model",
      "transfer-learning"
    ],
    "body": "<p>When I run this code, the runtime session automatically closes. There is no space in RAM left. Hence the session closes automatically. I am using pytorch in Google Colab notebook. I tried switching from CPU to GPU but still the session closes automatically.</p>\n<pre><code>!pip install datasets\n!pip install transformers\n\nimport torch\nimport wandb\nimport torch.nn as nn\nfrom datasets import load_dataset\nfrom transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n\n!huggingface-cli login\n\nmodel_name = &quot;distilgpt2&quot;\n\nprint(f&quot;Using GPU: {torch.cuda.is_available()}&quot;)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nlen(tokenizer)\n\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})\n\nlen(tokenizer)\n\nclass Custom_GPT2_Model(nn.Module):\n    def __init__(self, tokenizer):\n        super().__init__()\n\n        self.gpt2 = AutoModelForCausalLM.from_pretrained(model_name)\n\n        self.gpt2.resize_token_embeddings(len(tokenizer))\n\n        for param in self.gpt2.parameters():\n            param.requires_grad = False\n\n        self.gpt2.gradient_checkpointing_enable()\n\n        self.custom_layer = nn.Linear(self.gpt2.config.vocab_size, self.gpt2.config.vocab_size)\n\n    def forward(self, input_ids, attention_mask=None, labels=None):\n\n        outputs = self.gpt2(input_ids=input_ids, attention_mask=attention_mask)\n\n        logits = self.custom_layer(outputs.logits)\n\n        loss = None\n        if labels is not None:\n            loss_func = nn.CrossEntropyLoss()\n            # loss = loss_func(logits, labels)\n            loss = loss_func(logits.view(-1, logits.size(-1)), labels.view(-1))\n\n        return {\n            'loss': loss,\n            'logits': logits\n        }\n\nmodel = Custom_GPT2_Model(tokenizer)\n\n# Data Pre-Processing\n\ndataset = load_dataset(&quot;wikitext&quot;, &quot;wikitext-2-raw-v1&quot;)\n\ndataset\n\ndef tokenize_func(examples):\n    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n\ntokenized_dataset = dataset.map(tokenize_func, batched=True, remove_columns=['text'])\ntokenized_dataset\n\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=0.5,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=16,\n    fp16=True,\n    warmup_steps=2500,\n    learning_rate=0.1,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=5000,\n)\n\n\ntrainer = Trainer(\n    model = model,\n    args = training_args,\n    train_dataset = tokenized_dataset['train'].select(range(100)),\n    eval_dataset = tokenized_dataset['test'].select(range(100)),\n    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n)\n\ntrainer.train()\n</code></pre>\n<p>How should I manage memory while fine-tuning deep learning models?</p>\n",
    "is_answered": false,
    "view_count": 49,
    "answer_count": 1
  },
  {
    "title": "How to encode item features with high number of categories for recommendation",
    "link": "https://stackoverflow.com/questions/79270683/how-to-encode-item-features-with-high-number-of-categories-for-recommendation",
    "tags": [
      "machine-learning",
      "encoding",
      "data-science",
      "categorical-data",
      "recommendation-engine"
    ],
    "body": "<p>For the recommendation problem I am working on, there are around 50000 unique brands and 3 level product categories, level_1_cat (50 categories), level_2_cat (100 categories) and level_3_cat (1000 categories). All these item features are represented by integers only. So far I have tried binary-encoding, label-encoding and target-encoding for my lightfm model. With binary-encoding and label-encoding, the results were worse than not using any item features. With target-encoding, the result were similar to not using any item features. I am wondering what else I can try.</p>\n",
    "is_answered": false,
    "view_count": 53,
    "answer_count": 1
  },
  {
    "title": "Aggregate function as an argument",
    "link": "https://stackoverflow.com/questions/79261395/aggregate-function-as-an-argument",
    "tags": [
      "python",
      "pandas",
      "data-science"
    ],
    "body": "<p>I need to write a function:</p>\n<pre class=\"lang-py prettyprint-override\"><code>def group_and_aggregate_data(df:pd.DataFrame, group_by_column:str, agg_func) -&gt; pd.DataFrame\n</code></pre>\n<p>that groups my Excel data by city name and applies the <code>agg_func</code> passed as an argument. I tried this:</p>\n<pre class=\"lang-py prettyprint-override\"><code>def group_and_aggregate_data(dataframe, cond, func):\n    df_bycity = dataframe.groupby(cond).func()\n    return df_bycity\n</code></pre>\n<p>but it doesn't work in Python.</p>\n",
    "is_answered": true,
    "view_count": 56,
    "answer_count": 1
  },
  {
    "title": "problem with &quot;fixedProfile&quot; boundary type for reading from a csv file in openFOAM",
    "link": "https://stackoverflow.com/questions/79241679/problem-with-fixedprofile-boundary-type-for-reading-from-a-csv-file-in-openfoa",
    "tags": [
      "data-science",
      "openfoam"
    ],
    "body": "<p>I have boundary values for inlet boundary my case (these values are predicted with NN a code). the values are tabulated and have been prepared in csv file formant. As I know based on searching the forums the &quot; <strong>fixedProfile</strong>&quot; boundary type might a good solution for me. but I do not know how to use it properly. I have a csv file for U and p but after running OF warns me: &quot;<strong>Table read from &quot;u.csv&quot; is empty</strong>&quot;.</p>\n<p>this is what I've written in U dictionary for the desired value:</p>\n<pre><code>       type            fixedProfile;\n       profile         tableFile;\n       profileCoeffs\n       {\n            file                &quot;u.csv&quot;;\n            format              csv;\n            nHeaderLine         0;\n            refColumn           0;\n            componentColumns    (1 0 0);\n            separator           &quot;&quot;;\n            mergeSeparators     no;\n            outOfBounds         clamp;\n            interpolationScheme linear;\n       }\n        direction        (0 -1 0);\n        origin           0;\n</code></pre>\n<p>the main question is what is that what should be the structure of the csv file? should it include coordinates (x , y)?</p>\n",
    "is_answered": false,
    "view_count": 74,
    "answer_count": 0
  },
  {
    "title": "Error in odbcConnectAccess2007(&quot;BD-Embalses.mdb&quot;). R could not find the function &quot;odbcConnectAccess2007&quot; after installing and loading RODBC",
    "link": "https://stackoverflow.com/questions/79215418/error-in-odbcconnectaccess2007bd-embalses-mdb-r-could-not-find-the-function",
    "tags": [
      "r",
      "database",
      "ms-access",
      "odbc",
      "data-science"
    ],
    "body": "<p>I'm a student of Data Science, and I'm working in a project to visualize a database of Spanish reservoirs; but I'm getting this error:</p>\n<pre><code># package installations if required:\nif(!require(&quot;tidyverse&quot;)) install.packages(&quot;tidyverse&quot;)\nif(!require(&quot;janitor&quot;)) install.packages(&quot;janitor&quot;)\nif(!require(&quot;patchwork&quot;)) install.packages(&quot;patchwork&quot;)\nif(!require(&quot;RODBC&quot;)) install.packages(&quot;RODBC&quot;)\n\n# load the packages\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(janitor)\nlibrary(patchwork)\nlibrary(RODBC)\n\n# URL of the database\nurl &lt;- &quot;https://www.miteco.gob.es/content/dam/miteco/es/agua/temas/evaluacion-de-los-recursos-hidricos/boletin-hidrologico/Historico-de-embalses/BD-Embalses.zip&quot;\n\n# download\ntempf &lt;- tempfile() # temp file\ndownload.file(url, tempf) \nunzip(tempf)\n\n# open the connection with mdb\nconn &lt;- odbcConnectAccess2007(&quot;BD-Embalses.mdb&quot;)\n</code></pre>\n<p>I installed everything on my system (Arch linux), but I can't make the connection to the database for some reason (it says that the function could not be found). Everything is in Spanish, I'm just translating it into English.</p>\n",
    "is_answered": true,
    "view_count": 71,
    "answer_count": 1
  },
  {
    "title": "Problem with SHAP plot for multiclass problem",
    "link": "https://stackoverflow.com/questions/79204051/problem-with-shap-plot-for-multiclass-problem",
    "tags": [
      "python",
      "data-science"
    ],
    "body": "<p>I have following this code:</p>\n<pre><code>from xgboost import XGBClassifier\nimport shap\nfrom sklearn.preprocessing import LabelEncoder\n\n# Encode the labels for multiclass classification\nlabel_encoder = LabelEncoder()\ny_enc = label_encoder.fit_transform(y)\n\n# Train an XGBoost model\nmodel = XGBClassifier(objective=&quot;multi:softprob&quot;, num_class=3)\nmodel.fit(X, y_enc)\n\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X)\nclasses = label_encoder.inverse_transform(range(\n                            len(label_encoder.classes_)))\nshap.summary_plot(shap_values, X, class_names=classes, plot_type='barh')\n</code></pre>\n<p>but the plot shown like this:</p>\n<p><img src=\"https://i.sstatic.net/2XhmryM6.png\" alt=\"\" />\n<a href=\"https://i.sstatic.net/VCMW5F9t.png\" rel=\"nofollow noreferrer\">2</a></p>\n<p>Any idea what went wrong?</p>\n",
    "is_answered": false,
    "view_count": 38,
    "answer_count": 1
  },
  {
    "title": "Does polars load the entire parquet into the memory if we want to retrive certain column?",
    "link": "https://stackoverflow.com/questions/79188730/does-polars-load-the-entire-parquet-into-the-memory-if-we-want-to-retrive-certai",
    "tags": [
      "python",
      "dataframe",
      "data-science",
      "parquet",
      "python-polars"
    ],
    "body": "<p>I am new to the data science. I am working on the Polars to read the parquet files. Total size of all these parquet files is 240 GB. I have an EC2 machine with 64 GB and 8 vCP.</p>\n<p>I was under the assumption that as Parquet is a columnar file format so whenever I retrieve columns from the Parquet files then it doesn't need to load the entire file into the memory and only loads the required columns. (As a noob I am not sure how it works)</p>\n<p>But today when I tried to load 3 columns with the total size of 600 MB (Total column size) then Memory usage went through the roof. It consumed the entire 64 GB of RAM.</p>\n<p>I am not able to find any documentation about the lifecycle of loading parquet files into the polars and how it reads the column.</p>\n<p>Can someone explain me how this works or point me to good documentation</p>\n<p>Here is the code</p>\n<pre><code>import polars as pl\nimport pyarrow.parquet as pq\n\n# Directory containing the Parquet files\ndirectory = '/home/ubuntu/parquet_files/'\n\n# Load data using Polars\ndf = pl.scan_parquet(directory)\ngrouped_df = df.select([\n    pl.col(&quot;L_SHIPDATE&quot;).alias(&quot;L_SHIPDATE&quot;),\n    pl.col(&quot;L_LINESTATUS&quot;).alias(&quot;L_LINESTATUS&quot;),\n    pl.col(&quot;L_RETURNFLAG&quot;).alias(&quot;L_RETURNFLAG&quot;)\n]).collect(streaming=True)\n</code></pre>\n",
    "is_answered": false,
    "view_count": 317,
    "answer_count": 0
  },
  {
    "title": "How to use Python to replicate Random Forest Regression prediction using decision paths?",
    "link": "https://stackoverflow.com/questions/79183884/how-to-use-python-to-replicate-random-forest-regression-prediction-using-decisio",
    "tags": [
      "machine-learning",
      "scikit-learn",
      "data-science",
      "random-forest"
    ],
    "body": "<p>I'm trying to test whether I've understood the way RandomForestRegressor produces forecast after a model's fitted. I used the California housing example to train a simple model and predict the first value in my test set. I then used apply() to obtain the nodes of all training data and the test sample of interest in each regression tree. The prediction result is 1.41307</p>\n<pre><code>from sklearn import datasets\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\nX, y = datasets.fetch_california_housing(as_frame=True, return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nrf = RandomForestRegressor(random_state=0)\nrf.fit(X_train, y_train)\n\ntraining_nodes = rf.apply(X_train)\ntest_nodes = rf.apply(X_test.iloc[[0]])\n\nrf.predict( X_test.iloc[[0]] )\narray([1.41307])\n\n</code></pre>\n<p>To replicate the forecast, I need to: 1. Find the nodes of my test data of interest across all regression trees 2. Find all training data that shared the same node as my test data of interest across all regression trees. 3. For each tree, calculate the average of y_train and store them. 4. Calculate the average of stored average y_train. The result I get is 1.4053, it's a large difference compared to rf.predict. Can someone please show me how I got it wrong and what are the correct steps?</p>\n<pre><code>matching_samples_per_tree = []\n\ntarget_values_per_tree = []\n\nfor tree_idx in range(training_nodes.shape[1]):\n    # Find indices where training samples fall into the same leaf as the test sample in this tree\n    matching_indices = np.where(training_nodes[:, tree_idx] == test_nodes[0][tree_idx])[0]\n    \n    matching_samples_per_tree.append(matching_indices)\n    \n    # Retrieve and store the target values of these matching training samples\n    target_values_per_tree.append(y_train.iloc[matching_indices].values)\n\nmeans_of_each_array = []\n\nfor vals in target_values_per_tree:\n    mean_value = np.mean(vals)\n    means_of_each_array.append(mean_value)\n\nnp.mean(means_of_each_array) \n#1.405348333333333\n</code></pre>\n",
    "is_answered": false,
    "view_count": 71,
    "answer_count": 0
  },
  {
    "title": "Invalid value of type &#39;builtins.str&#39; received for the &#39;color&#39; property of scatter.line Received value: &#39;rgba(166, 206, 227, np.float64(1.0))&#39;",
    "link": "https://stackoverflow.com/questions/79169081/invalid-value-of-type-builtins-str-received-for-the-color-property-of-scatte",
    "tags": [
      "python",
      "data-science",
      "data-analysis",
      "valueerror"
    ],
    "body": "<p><strong>I am using this set of imports</strong></p>\n<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom chart_studio import plotly\n# import plotly.plotly as pl\nimport plotly.graph_objs as go\nimport plotly.offline as of\nimport cufflinks as cf\nimport datetime as dt\n%matplotlib inline\n</code></pre>\n<pre><code>of.init_notebook_mode(connected = True)\ncf.go_offline()\n</code></pre>\n<p><strong>And when i try to run this code</strong></p>\n<pre><code>df.iplot(kind='scatter',xTitle='Projects',\n         yTitle='Donations',title='Projects vs Donations',\n         symbol='x',colorscale='paired',mode='markers')\n</code></pre>\n<p>**It throws error **</p>\n<pre><code>ValueError                                Traceback (most recent call last)\nCell In[94], line 1\n----&gt; 1 df.iplot(kind='scatter',xTitle='Projects',\n      2          yTitle='Donations',title='Projects vs Donations',\n      3          symbol='x',colorscale='paired',mode='markers')\n\nFile ~\\anaconda3\\Lib\\site-packages\\cufflinks\\plotlytools.py:839, in _iplot(self, kind, data, layout, filename, sharing, title, xTitle, yTitle, zTitle, theme, colors, colorscale, fill, width, dash, mode, interpolation, symbol, size, barmode, sortbars, bargap, bargroupgap, bins, histnorm, histfunc, orientation, boxpoints, annotations, keys, bestfit, bestfit_colors, mean, mean_colors, categories, x, y, z, text, gridcolor, zerolinecolor, margin, labels, values, secondary_y, secondary_y_title, subplots, shape, error_x, error_y, error_type, locations, lon, lat, asFrame, asDates, asFigure, asImage, dimensions, asPlot, asUrl, online, **kwargs)\n    837     if not isinstance(text,list):\n    838         text=self[text].values\n--&gt; 839 data=df.to_iplot(colors=colors,colorscale=colorscale,kind=kind,interpolation=interpolation,fill=fill,width=width,dash=dash,sortbars=sortbars,keys=keys,\n    840         bestfit=bestfit,bestfit_colors=bestfit_colors,mean=mean,mean_colors=mean_colors,asDates=asDates,mode=mode,symbol=symbol,size=size,\n    841         text=text,**kwargs)     \n    842 trace_kw=check_kwargs(kwargs,TRACE_KWARGS)\n    843 for trace in data:\n\nFile ~\\anaconda3\\Lib\\site-packages\\cufflinks\\plotlytools.py:161, in _to_iplot(self, colors, colorscale, kind, mode, interpolation, symbol, size, fill, width, dash, sortbars, keys, bestfit, bestfit_colors, opacity, mean, mean_colors, asDates, asTimestamp, text, **kwargs)\n    159     lines_plotly=[Bar(lines[key]).to_plotly_json() for key in keys]\n    160 else:\n--&gt; 161     lines_plotly=[Scatter(lines[key]).to_plotly_json() for key in keys]\n    162 for trace in lines_plotly:\n    163     if isinstance(trace['name'],pd.Timestamp):\n\nFile ~\\anaconda3\\Lib\\site-packages\\plotly\\graph_objs\\_scatter.py:3530, in Scatter.__init__(self, arg, alignmentgroup, cliponaxis, connectgaps, customdata, customdatasrc, dx, dy, error_x, error_y, fill, fillcolor, fillgradient, fillpattern, groupnorm, hoverinfo, hoverinfosrc, hoverlabel, hoveron, hovertemplate, hovertemplatesrc, hovertext, hovertextsrc, ids, idssrc, legend, legendgroup, legendgrouptitle, legendrank, legendwidth, line, marker, meta, metasrc, mode, name, offsetgroup, opacity, orientation, selected, selectedpoints, showlegend, stackgaps, stackgroup, stream, text, textfont, textposition, textpositionsrc, textsrc, texttemplate, texttemplatesrc, uid, uirevision, unselected, visible, x, x0, xaxis, xcalendar, xhoverformat, xperiod, xperiod0, xperiodalignment, xsrc, y, y0, yaxis, ycalendar, yhoverformat, yperiod, yperiod0, yperiodalignment, ysrc, zorder, **kwargs)\n   3528 _v = line if line is not None else _v\n   3529 if _v is not None:\n-&gt; 3530     self[&quot;line&quot;] = _v\n   3531 _v = arg.pop(&quot;marker&quot;, None)\n   3532 _v = marker if marker is not None else _v\n\nFile ~\\anaconda3\\Lib\\site-packages\\plotly\\basedatatypes.py:4860, in BasePlotlyType.__setitem__(self, prop, value)\n   4858 # ### Handle compound property ###\n   4859 if isinstance(validator, CompoundValidator):\n-&gt; 4860     self._set_compound_prop(prop, value)\n   4862 # ### Handle compound array property ###\n   4863 elif isinstance(validator, (CompoundArrayValidator, BaseDataValidator)):\n\nFile ~\\anaconda3\\Lib\\site-packages\\plotly\\basedatatypes.py:5271, in BasePlotlyType._set_compound_prop(self, prop, val)\n   5268 # Import value\n   5269 # ------------\n   5270 validator = self._get_validator(prop)\n-&gt; 5271 val = validator.validate_coerce(val, skip_invalid=self._skip_invalid)\n   5273 # Save deep copies of current and new states\n   5274 # ------------------------------------------\n   5275 curr_val = self._compound_props.get(prop, None)\n\nFile ~\\anaconda3\\Lib\\site-packages\\_plotly_utils\\basevalidators.py:2512, in CompoundValidator.validate_coerce(self, v, skip_invalid, _validate)\n   2509     v = self.data_class()\n   2511 elif isinstance(v, dict):\n-&gt; 2512     v = self.data_class(v, skip_invalid=skip_invalid, _validate=_validate)\n   2514 elif isinstance(v, self.data_class):\n   2515     # Copy object\n   2516     v = self.data_class(v)\n\nFile ~\\anaconda3\\Lib\\site-packages\\plotly\\graph_objs\\scatter\\_line.py:374, in Line.__init__(self, arg, backoff, backoffsrc, color, dash, shape, simplify, smoothing, width, **kwargs)\n    372 _v = color if color is not None else _v\n    373 if _v is not None:\n--&gt; 374     self[&quot;color&quot;] = _v\n    375 _v = arg.pop(&quot;dash&quot;, None)\n    376 _v = dash if dash is not None else _v\n\nFile ~\\anaconda3\\Lib\\site-packages\\plotly\\basedatatypes.py:4868, in BasePlotlyType.__setitem__(self, prop, value)\n   4864         self._set_array_prop(prop, value)\n   4866     # ### Handle simple property ###\n   4867     else:\n-&gt; 4868         self._set_prop(prop, value)\n   4869 else:\n   4870     # Make sure properties dict is initialized\n   4871     self._init_props()\n\nFile ~\\anaconda3\\Lib\\site-packages\\plotly\\basedatatypes.py:5212, in BasePlotlyType._set_prop(self, prop, val)\n   5210         return\n   5211     else:\n-&gt; 5212         raise err\n   5214 # val is None\n   5215 # -----------\n   5216 if val is None:\n   5217     # Check if we should send null update\n\nFile ~\\anaconda3\\Lib\\site-packages\\plotly\\basedatatypes.py:5207, in BasePlotlyType._set_prop(self, prop, val)\n   5204 validator = self._get_validator(prop)\n   5206 try:\n-&gt; 5207     val = validator.validate_coerce(val)\n   5208 except ValueError as err:\n   5209     if self._skip_invalid:\n\nFile ~\\anaconda3\\Lib\\site-packages\\_plotly_utils\\basevalidators.py:1411, in ColorValidator.validate_coerce(self, v, should_raise)\n   1409     validated_v = self.vc_scalar(v)\n   1410     if validated_v is None and should_raise:\n-&gt; 1411         self.raise_invalid_val(v)\n   1413     v = validated_v\n   1415 return v\n\nFile ~\\anaconda3\\Lib\\site-packages\\_plotly_utils\\basevalidators.py:296, in BaseValidator.raise_invalid_val(self, v, inds)\n    293             for i in inds:\n    294                 name += &quot;[&quot; + str(i) + &quot;]&quot;\n--&gt; 296         raise ValueError(\n    297             &quot;&quot;&quot;\n    298     Invalid value of type {typ} received for the '{name}' property of {pname}\n    299         Received value: {v}\n    300 \n    301 {valid_clr_desc}&quot;&quot;&quot;.format(\n    302                 name=name,\n    303                 pname=self.parent_name,\n    304                 typ=type_str(v),\n    305                 v=repr(v),\n    306                 valid_clr_desc=self.description(),\n    307             )\n    308         )\n\nValueError: \n    Invalid value of type 'builtins.str' received for the 'color' property of scatter.line\n        Received value: 'rgba(166, 206, 227, np.float64(1.0))'\n\n    The 'color' property is a color and may be specified as:\n      - A hex string (e.g. '#ff0000')\n      - An rgb/rgba string (e.g. 'rgb(255,0,0)')\n      - An hsl/hsla string (e.g. 'hsl(0,100%,50%)')\n      - An hsv/hsva string (e.g. 'hsv(0,100%,100%)')\n      - A named CSS color:\n            aliceblue, antiquewhite, aqua, aquamarine, azure,\n            beige, bisque, black, blanchedalmond, blue,\n            blueviolet, brown, burlywood, cadetblue,\n            chartreuse, chocolate, coral, cornflowerblue,\n            cornsilk, crimson, cyan, darkblue, darkcyan,\n            darkgoldenrod, darkgray, darkgrey, darkgreen,\n            darkkhaki, darkmagenta, darkolivegreen, darkorange,\n            darkorchid, darkred, darksalmon, darkseagreen,\n            darkslateblue, darkslategray, darkslategrey,\n            darkturquoise, darkviolet, deeppink, deepskyblue,\n            dimgray, dimgrey, dodgerblue, firebrick,\n            floralwhite, forestgreen, fuchsia, gainsboro,\n            ghostwhite, gold, goldenrod, gray, grey, green,\n            greenyellow, honeydew, hotpink, indianred, indigo,\n            ivory, khaki, lavender, lavenderblush, lawngreen,\n            lemonchiffon, lightblue, lightcoral, lightcyan,\n            lightgoldenrodyellow, lightgray, lightgrey,\n            lightgreen, lightpink, lightsalmon, lightseagreen,\n            lightskyblue, lightslategray, lightslategrey,\n            lightsteelblue, lightyellow, lime, limegreen,\n            linen, magenta, maroon, mediumaquamarine,\n            mediumblue, mediumorchid, mediumpurple,\n            mediumseagreen, mediumslateblue, mediumspringgreen,\n            mediumturquoise, mediumvioletred, midnightblue,\n            mintcream, mistyrose, moccasin, navajowhite, navy,\n            oldlace, olive, olivedrab, orange, orangered,\n            orchid, palegoldenrod, palegreen, paleturquoise,\n            palevioletred, papayawhip, peachpuff, peru, pink,\n            plum, powderblue, purple, red, rosybrown,\n            royalblue, rebeccapurple, saddlebrown, salmon,\n            sandybrown, seagreen, seashell, sienna, silver,\n            skyblue, slateblue, slategray, slategrey, snow,\n            springgreen, steelblue, tan, teal, thistle, tomato,\n            turquoise, violet, wheat, white, whitesmoke,\n            yellow, yellowgreen\n</code></pre>\n<p><a href=\"https://i.sstatic.net/pBEwGTXf.png\" rel=\"nofollow noreferrer\">This is what I get (Output Image)</a></p>\n<p>I tried to change the versions of the plotly module to older one.\nAlso with this set of code  the required graph can be obtained but i want to use the intractive plot (iplot) for better and detailed graphs.</p>\n<pre><code>df.plot.scatter(xlabel='Projects', ylabel='Donations', title='Projects vs Donations' , x='Projects', y='Donations')\nplt.show()\n</code></pre>\n",
    "is_answered": false,
    "view_count": 703,
    "answer_count": 1
  },
  {
    "title": "Creating Team Data Science Process in ADO",
    "link": "https://stackoverflow.com/questions/79167418/creating-team-data-science-process-in-ado",
    "tags": [
      "azure-devops",
      "process",
      "data-science"
    ],
    "body": "<p>Our organization is currently utilizing Azure DevOps (ADO), and we're interested in integrating the &quot;Team Data Science Process&quot; (TDSP) into our data science and machine learning project workflows within ADO.</p>\n<p>Upon accessing ADO Board --&gt; Process, I notice that there are four available processes:</p>\n<ul>\n<li>Basic</li>\n<li>Agile</li>\n<li>Scrum</li>\n<li>CMMI</li>\n</ul>\n<p>What I am seeking guidance on is how we can effectively incorporate TDSP using one of these processes. Specifically, I would like each new project to encompass the following stages, each of which can be labeled as either 'To Do', 'Doing', or 'Done':</p>\n<ul>\n<li>Business Understanding</li>\n<li>Data Acquisition and Understanding</li>\n<li>Modeling</li>\n<li>Deployment</li>\n</ul>\n<p>Any assistance or advice on how we can achieve this would be greatly appreciated.</p>\n",
    "is_answered": true,
    "view_count": 111,
    "answer_count": 1
  },
  {
    "title": "Handling Multiple Entity Candidates in Short Texts for Entity Linking with SciSpacy",
    "link": "https://stackoverflow.com/questions/79165649/handling-multiple-entity-candidates-in-short-texts-for-entity-linking-with-scisp",
    "tags": [
      "nlp",
      "data-science",
      "spacy",
      "named-entity-recognition",
      "entity-linking"
    ],
    "body": "<p>I am working on linking short texts to entities in a biomedical knowledge graph (UMLS CUIs) using SciSpacy for a research project. The goal is to analyze the relationship between the linked entity and a separate predefined entity.</p>\n<p>My challenge is managing multiple possible entities identified in the texts, which introduces noise into the results. Although I use heuristics such as regex, a manual stop list, and filtering by semantic categories (TUIs) to clean the data, the issue persists due to the text complexity. I typically select the top ~3 entities per text based on the NER score, with a relatively high threshold.</p>\n<p>For instance, the text &quot;Standard PRS for Alzheimer's&quot; incorrectly links entities for &quot;Standard&quot; and &quot;PRS,&quot; in addition to &quot;Alzheimer's.&quot; Another example, &quot;Other diseases of respiratory system, NEC,&quot; captures &quot;respiratory&quot; and &quot;diseases&quot; but misses &quot;NEC&quot; (Necrotizing enterocolitis), which should be prioritized.</p>\n<p>I've tried filtering results by semantic similarity using a biomedical model, but this approach is still imprecise and heavily dependent on the number of results. The linker often seems to prioritize entities appearing earlier in the text. I also use an abbreviation expander to handle non-standard acronym forms.</p>\n<p>I think a smarter linker (not supported by scispacy) might help, or better matching at the sentence/whole text level, but I don't know much about that. (I do some filtering of results using sentence transformers, but that's just cossine sim - I couldn't find a clear cutoff that generalized well).</p>\n<p>I do not have the resources/time to learn to fine-tune a new linker model+data (this is just a sub-component in my overall phd).</p>\n<p>I'm looking for advice on more effective strategies for entity linking at the sentence or whole-text level without the resources to fine-tune a new model. Compatability with SciSpacy is important, since linkage to the UMLS ontology (for the KG CUI entites) is a must.</p>\n",
    "is_answered": false,
    "view_count": 48,
    "answer_count": 1
  },
  {
    "title": "Is there a better and effective way to convert a large raster file into a vector point?",
    "link": "https://stackoverflow.com/questions/79148187/is-there-a-better-and-effective-way-to-convert-a-large-raster-file-into-a-vector",
    "tags": [
      "r",
      "data-science",
      "spatial",
      "shapefile",
      "r-raster"
    ],
    "body": "<p>The idea is to convert some raster files (39 fire occurrences) into vector points (.shp), allowing the use of the <em>extract()</em> function in the terra package to relate the occurrence points to another raster file, e.g., land use, temperature, solar radiation. Is there a better way to do this? The only strategy I thought of was to convert it into a point file.</p>\n<p>The size of a single raster file is about 85 MB, and after using the <em>as.points()</em> function, the vector file is about 5.8 GB (318 GB for all 39 files). The <em>rbind</em> does't work, memory crash.</p>\n<p>Here's what I have so far:</p>\n<pre><code>library(terra)\n\narquivos &lt;- list.files('Matriciais', pattern = '*.tif$', full.names = T)\nmatriciais &lt;- sapply(arquivos, rast)\n\ndir.create('Vetoriais', showWarnings = F)\n\nfor (i in seq_along(matriciais)) {\n  pontos &lt;- as.points(matriciais[[i]])\n  \n  nome &lt;- basename(arquivos[[i]])\n  ano &lt;- sub('.*_(\\\\d{4})\\\\.tif$', '\\\\1', nome)\n  \n  pontos$ID &lt;- seq_len(nrow(pontos))\n  \n  pontos$Ano &lt;- as.numeric(ano)\n  \n  pontos$Mes &lt;- values(matriciais[[i]])\n  \n  writeVector(pontos, file.path('Vetoriais', paste0('Pontos_Focos_', ano, '.shp')), overwrite = T)\n\n  print(paste0('Processo do ano ', ano, ' conclu\u00eddo'))\n}\n\nvetores &lt;- do.call(rbind, lapply(list.files('Vetoriais', pattern = '*.shp$', full.names = T), vect))\n</code></pre>\n",
    "is_answered": false,
    "view_count": 95,
    "answer_count": 0
  },
  {
    "title": "Loading Fuzzywuzzy into conda environment on VS Code won&#39;t work",
    "link": "https://stackoverflow.com/questions/79146128/loading-fuzzywuzzy-into-conda-environment-on-vs-code-wont-work",
    "tags": [
      "python",
      "visual-studio-code",
      "data-science",
      "fuzzywuzzy"
    ],
    "body": "<p>I currently cannot install fuzzywuzzy but fuzzy matching is quite vital to a data project I'm working on as the dataset contains thousands of points and needs to be grouped computationally. I'm fairly new to VS Code so perhaps that is why but used the terminal like normal as follows.</p>\n<p>So I have tried:</p>\n<p><code>Conda install fuzzywuzzy</code></p>\n<p>but it outputs</p>\n<pre class=\"lang-none prettyprint-override\"><code>Channels:\n - defaults\nPlatform: osx-64\nCollecting package metadata (repodata.json): done\nSolving environment: - warning  libmamba Added empty dependency for problem type SOLVER_RULE_UPDATE\nfailed\n\nLibMambaUnsatisfiableError: Encountered problems while solving:\n  - package fuzzywuzzy-0.18.0-py310hecd8cb5_0 requires python &gt;=3.10,&lt;3.11.0a0, but none of the providers can be installed\n\nCould not solve for environment specs\nThe following packages are incompatible\n\u251c\u2500 fuzzywuzzy is installable with the potential options\n\u2502  \u251c\u2500 fuzzywuzzy 0.18.0 would require\n\u2502  \u2502  \u2514\u2500 python &gt;=3.10,&lt;3.11.0a0 , which can be installed;\n\u2502  \u251c\u2500 fuzzywuzzy 0.18.0 would require\n\u2502  \u2502  \u2514\u2500 python &gt;=3.11,&lt;3.12.0a0 , which can be installed;\n\u2502  \u251c\u2500 fuzzywuzzy 0.18.0 would require\n\u2502  \u2502  \u2514\u2500 python &gt;=3.7,&lt;3.8.0a0 , which can be installed;\n\u2502  \u251c\u2500 fuzzywuzzy 0.18.0 would require\n\u2502  \u2502  \u2514\u2500 python &gt;=3.8,&lt;3.9.0a0 , which can be installed;\n\u2502  \u2514\u2500 fuzzywuzzy 0.18.0 would require\n\u2502     \u2514\u2500 python &gt;=3.9,&lt;3.10.0a0 , which can be installed;\n\u2514\u2500 pin-1 is not installable because it requires\n   \u2514\u2500 python 3.12.* , which conflicts with any installable versions previously reported.\n</code></pre>\n<p>I'm somewhat at a loss. I'm using Python 3.12.4 as of right now. Any ideas about how I can get this working would be greatly appreciated.</p>\n",
    "is_answered": false,
    "view_count": 273,
    "answer_count": 2
  },
  {
    "title": "How to use Github Rest API properly",
    "link": "https://stackoverflow.com/questions/79141121/how-to-use-github-rest-api-properly",
    "tags": [
      "github",
      "web-scraping",
      "data-science"
    ],
    "body": "<p>I am doing my college assignment project. But am not able to find these data using github api:\nlogin: Their Github user ID\nname: Their full name\ncompany: The company they work at. Clean up company names. At least make sure:\nThey're trimmed of whitespace\nLeading @ symbol is stripped (Note: ONLY the first one is stripped)\nThey are converted to UPPERCASE\nlocation: The city they are in\nemail: Their email address\nhireable: Whether they are open to being hired\nbio: A short bio about them\npublic_repos: The number of public repositories they have\nfollowers: The number of followers they have\nfollowing: The number of people they are following\ncreated_at: When they joined Github\nGiven below in my code which I used and get the json file</p>\n<blockquote>\n<p>json</p>\n</blockquote>\n<pre><code>import requests\nimport \n\nquery = &quot;location:Bengaluru+followers:&gt;100&quot;\nurl = f&quot;https://api.github.com/search/users?q={query}&amp;sort=followers&amp;order=desc&quot;\n\nresponse = requests.get(url)\ndata = response.json()  # Parse the JSON response\n\nformatted_json = json.dumps(data, indent=4)  # Convert the JSON object to a formatted string\n\n# Save the formatted JSON string to a file\nF = open(&quot;user_data.json&quot;, &quot;w&quot;)\nF.write(formatted_json)\nF.close()\n</code></pre>\n<p>I want to find the values of all the columns asked using GITHUB REST API</p>\n",
    "is_answered": false,
    "view_count": 46,
    "answer_count": 1
  },
  {
    "title": "Need to fix time zone issue for my Python, Pyspark processing job",
    "link": "https://stackoverflow.com/questions/79077092/need-to-fix-time-zone-issue-for-my-python-pyspark-processing-job",
    "tags": [
      "python",
      "pyspark",
      "data-science",
      "parquet",
      "data-engineering"
    ],
    "body": "<p>I have database column in Postgres aurora \u201clast_updated_timestamp\u201d which is without timezone and has value \u201c2024-08-01 00:00:00\u201d\nI am fetching that value using pyspark df when I do df.show()\nIt is showing same value :\n2024-08-01 00:00:00.</p>\n<p>For my use case, I have a function write_df_to_delta_table() which writes this df to AWS S3 data lake in the form of delta table which is in parquet format.</p>\n<p>When I download this parquet file and view it using Avro and Parquet viewer plugin in Pycharm, it is showing as\n2024-07-31T18:30:00Z[UTC]</p>\n<p>For my use case, I want it to show as\n2024-08-01 00:00:00[PST].</p>\n<p>I don\u2019t want to change conf of my spark session as it may affect other processes I have.</p>\n<p>I tried implementing below code:\n<code>df = df.withColumn( &quot;updated_timestamp_pst&quot;,  from_utc_timestamp(col(&quot;updated_timestamp&quot;), &quot;PST&quot;) )</code></p>\n<p>But now it is showing as:</p>\n<p>2024-08-01T00:00:00Z[UTC]\nwhich is not acceptable.</p>\n<p>I want it only in PST but with same timestamp value.</p>\n",
    "is_answered": true,
    "view_count": 147,
    "answer_count": 2
  },
  {
    "title": "`inner_kws` having no effect on Seaborn violin plot",
    "link": "https://stackoverflow.com/questions/79067886/inner-kws-having-no-effect-on-seaborn-violin-plot",
    "tags": [
      "python",
      "matplotlib",
      "seaborn",
      "data-science",
      "visualization"
    ],
    "body": "<p>I generated a bunch of violin plots, here is an example of one and the code that generates it:</p>\n<pre class=\"lang-py prettyprint-override\"><code>plt.figure(figsize=(8, 4))\nax = sns.violinplot(\n    x=data, # `data` is a few thousand float values between 0 and 1\n    orient='h',\n    color=get_color(ff), # `get_color` returns a color based on the dataset, #FFBE0B in this case\n    cut=0\n)\n</code></pre>\n<p><a href=\"https://i.sstatic.net/FyZk7e5V.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/FyZk7e5V.png\" alt=\"a purple violin plot\" /></a></p>\n<p>I want to make the black box in the middle quite a bit bigger. According to the documentation from Seaborn at <a href=\"https://seaborn.pydata.org/generated/seaborn.violinplot.html\" rel=\"nofollow noreferrer\">https://seaborn.pydata.org/generated/seaborn.violinplot.html</a>, I should be able to do this with the <code>inner_kws</code> parameter. I added this argument to the above code:</p>\n<pre class=\"lang-py prettyprint-override\"><code>plt.figure(figsize=(8, 4))\nax = sns.violinplot(\n    x=data, # `data` is a few thousand float values between 0 and 1\n    orient='h',\n    color=get_color(ff), # `get_color` returns a color based on the dataset, #FFBE0B in this case\n    inner_kws=dict(box_width=150, whis_width=20),\n    cut=0\n)\n</code></pre>\n<p>Above, the box and whisker width are 150 and 20 respectively. I've also tried 15 and 2, and 1500 and 200. No matter what values I enter here, the figure does not change at all. What am I doing wrong?</p>\n",
    "is_answered": true,
    "view_count": 180,
    "answer_count": 1
  },
  {
    "title": "AutoTS: Forecast with top3 models instead of best model",
    "link": "https://stackoverflow.com/questions/79067097/autots-forecast-with-top3-models-instead-of-best-model",
    "tags": [
      "python",
      "time-series",
      "data-science",
      "automl"
    ],
    "body": "<p>I am using the AutoTS package for multivariate time series forecasting. The idea is to fit the model on a train set and forecast for validation set because I want to get metrics for each individual column. Pick top 3 models from validation and forecast for next 4 quarters of the entire data.</p>\n<p>I am performing a train-validation split to compare the mean absolute error (MAE) results of individual columns with other univariate time series models. This approach ensures a common comparison metric. If there are better methods for achieving this, please suggest them.</p>\n<pre><code>    # Prepare the data for AutoTS\n    model = AutoTS(\n        forecast_length=2,\n        frequency='infer',\n        model_list=['NVAR','MAR','MultivariateRegression','NeuralForecast','DynamicFactorMQ'],\n        transformer_list=&quot;fast&quot;,  # &quot;superfast&quot;, &quot;default&quot;, &quot;fast_parallel&quot;\n        transformer_max_depth=2,\n        max_generations=1,\n        num_validations=1,\n        validation_method='backwards',\n        no_negatives=True,\n        remove_leading_zeroes=True\n    )\n    \n    model = model.fit(\n        train_df,\n        date_col='year-quarter-date',\n        value_col='value',\n        id_col='vial_size'\n    )\n    \n    # Get the top 3 models based on the score\n    top_3_models = model.results().sort_values(by='Score').head(3)\n    \n    # Calculate MAE for each of the top 3 models\n    for _, row in top_3_models.iterrows():\n        model_name = row['Model']\n        model_param_dict = row['ModelParameters']\n        model_transform_dict = row['TransformationParameters']\n        \n        # Calculate next two quarters' year-quarter-date\n        last_date = pd.to_datetime(train_df['year-quarter-date'].max())\n        next_quarters = pd.date_range(last_date, periods=3, freq='Q')[1:]\n        next_quarters_str = next_quarters.to_period('Q').astype(str)\n        \n        # Predict the next two quarters using the predict method\n        forecast_result = model_forecast(\n            model_name=model_name,\n            model_param_dict=model_param_dict,\n            model_transform_dict=model_transform_dict,\n            df_train=train_df,\n            forecast_length=2,\n            frequency='Q',\n            no_negatives=True,\n            remove_leading_zeroes=True,\n        )\n    models = top_models[molecule]\n    for _, model in models:\n        model.forecast_length = 4\n        model = model.fit(\n            molecule_df,\n            date_col='year-quarter-date',\n            value_col='value',\n            id_col='vial_size'\n        )\n        prediction = model.predict()\n        forecast = prediction.forecast\n</code></pre>\n<p>This is taking more time to work through and I want to understand if there is a better way to do what I want to achieve.\nProcess Flow:</p>\n<ol>\n<li>Split the data into train and validation(last 2 quarters)</li>\n<li>Fit autoTS model(forecast length=2) to get mae scores for even the\nindividual columns</li>\n<li>Pick top3 models from this</li>\n<li>Forecast using these top 3 models for the next 4 quarters</li>\n</ol>\n",
    "is_answered": false,
    "view_count": 162,
    "answer_count": 1
  },
  {
    "title": "Check if any value in a Polars DataFrame is True",
    "link": "https://stackoverflow.com/questions/79061819/check-if-any-value-in-a-polars-dataframe-is-true",
    "tags": [
      "python",
      "dataframe",
      "data-science",
      "python-polars"
    ],
    "body": "<p>This is quite a simple ask but I can't seem to find any clear simplistic solution to this, feels like I'm missing something.</p>\n<p>Let's say I have a DataFrame of type</p>\n<pre><code>df = pl.from_repr(&quot;&quot;&quot;\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a     \u2506 b     \u2506 c     \u2502\n\u2502 ---   \u2506 ---   \u2506 ---   \u2502\n\u2502 bool  \u2506 bool  \u2506 bool  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 false \u2506 true  \u2506 false \u2502\n\u2502 false \u2506 false \u2506 false \u2502\n\u2502 false \u2506 false \u2506 false \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n&quot;&quot;&quot;)\n</code></pre>\n<p>How do I do a simple check if any of the values in the DataFrame is True?\nSome solutions I have found is</p>\n<pre><code>selection = df.select(pl.all().any(ignore_nulls=True))\n</code></pre>\n<p>or</p>\n<pre><code>selection = df.filter(pl.any_horizontal())\n</code></pre>\n<p>and then check in that row</p>\n<pre><code>any(selection.row(0))\n</code></pre>\n<p>Is just seems like so many steps for a single check</p>\n",
    "is_answered": true,
    "view_count": 1529,
    "answer_count": 3
  },
  {
    "title": "Create Kedro PartitionedDataset of PartitionedDatasets",
    "link": "https://stackoverflow.com/questions/79044783/create-kedro-partitioneddataset-of-partitioneddatasets",
    "tags": [
      "python",
      "machine-learning",
      "dataset",
      "data-science",
      "kedro"
    ],
    "body": "<p>I'm working in a kedro project where I want to automatically label thousands of audio files, apply transformations to them and then store them in a folder of folders, each subfolder corresponding to one label. I want that folder of folders to be a catalog entry  on my yml file</p>\n<p>I followed <a href=\"https://docs.kedro.org/en/stable/data/how_to_create_a_custom_dataset.html\" rel=\"nofollow noreferrer\">this Kedro tutorial</a> and created my own custom dataset for saving/loading .wav files in kedro catalog. I also am able to create <code>PartitionedDataset </code> catalog entries in <code>catalog.yml</code> such as</p>\n<pre><code>audio_folder:\n  type: partitions.PartitionedDataset\n  dataset: my_kedro_project.datasets.audio_dataset.SoundDataset\n  path: data/output/audios/\n  filename_suffix: &quot;.WAV&quot;\n</code></pre>\n<p>for saving/loading folders of .WAV files in Kedro catalog.</p>\n<p>The next level of abstraction I would require is to be able to create a catalog entry corresponding to a folder containig folders such as the <code>audio_folder</code> above. I don't want to do this by dynamicaly creating catalog entries but by extending <code>PartitionedDataset</code> class. This is because I want the folder of folders to be part of my <code>catalog.yml</code>. My questions are</p>\n<ol>\n<li>Is this possible? Has any of you ever try something like this?</li>\n<li>In case it is possible, my custom class should only include the <code>_load</code>, <code>_save</code>, and <code>_describe</code> methods as when I'm customizing an <code>AbstractDataset</code>?</li>\n</ol>\n<p><strong>EDIT</strong></p>\n<p>I finally decided to create annother custom class extending <code>AbstractDataset</code>. Here are some details about the <code>_load</code> and <code>_save</code> methods:</p>\n<pre><code>def _load(self):\n        subfolder_names=[ subfolder_name \n                         for subfolder_name in os.listdir(self._mainfolderpath) \n                         if os.path.isdir(os.path.join(self._mainfolderpath, subfolder_name)) \n                        ]\n        \n        \n        wav_paths_dict={}\n        for subfolder_name in subfolder_names:\n            subfolder_path=os.path.join(self._mainfolderpath, subfolder_name)\n            wav_files=[]\n            for root, dirs, files in os.walk(subfolder_path):\n                for file in files:\n                    if file.lower().endswith('.wav'):\n                        wav_file_path=os.path.join(root, file)\n                        wav_file_name=os.path.split(wav_file_path)[-1].replace('.wav','').replace('.WAV','')\n                        wav_files.append((wav_file_name,wav_file_path))\n                wav_paths_dict[subfolder_name]=dict(wav_files)\n\n        \n        partitioned_dataset_dict={}\n        for subfolder_name, sub_dict in wav_paths_dict.items():\n            partitioned_dataset=[(wav_file_name,SoundDataset(wav_file_path).load()) for wav_file_name,wav_file_path in sub_dict.items()]\n            partitioned_dataset_dict[subfolder_name]=dict(partitioned_dataset)\n        \n        return partitioned_dataset_dict\n</code></pre>\n<p>And</p>\n<pre><code>def _save(self, subfolders_dictionary):\n        if os.path.isdir(self._mainfolderpath):\n            for root, dirs, files in os.walk(self._mainfolderpath,topdown=False):\n                for name in files:\n                    os.remove(os.path.join(root, name))\n                for name in dirs:\n                    os.rmdir(os.path.join(root, name))\n            os.rmdir(self._mainfolderpath)\n        os.mkdir(self._mainfolderpath)\n        for subfolder_name in subfolders_dictionary.keys():\n            subfolder_path=os.path.join(self._mainfolderpath, subfolder_name) \n            os.mkdir(os.path.normpath(subfolder_path))\n\n            #print(subfolder_name, subfolder_path)\n            partitioned_dataset = PartitionedDataset(\n            path=subfolder_path,\n            dataset=SoundDataset,\n            filename_suffix=&quot;.WAV&quot;,\n            )\n            \n            partitioned_dataset.save(subfolders_dictionary[subfolder_name])\n</code></pre>\n",
    "is_answered": false,
    "view_count": 156,
    "answer_count": 0
  },
  {
    "title": "association matrix, nominal-nominal scale, nominal-ratio scale",
    "link": "https://stackoverflow.com/questions/79031023/association-matrix-nominal-nominal-scale-nominal-ratio-scale",
    "tags": [
      "python",
      "data-science",
      "correlation",
      "exploratory-data-analysis"
    ],
    "body": "<p>For ratio scale data it is relatively simple to create and visualize a <strong>correlation matrix</strong> e.g. as shown below. How can I <strong>do the same</strong> for a data frame that contains also <strong>nominal scale</strong> data? I would like <strong>to see associations (e.g. Cramer's Phi or others)</strong> for nominal-nominal, and nominal-ratio pairs and correlation for ratio-ratio pair variables. Something like the matrix the <em>sweetviz</em> Python package generates for an automatic exploratory data analysis.</p>\n<pre><code>import seaborn as sns\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(15,15))         # Sample figsize in inches (width, heights)\nhm = sns.heatmap(df[meta_data + summary_ravenscroft].corr(method='spearman'), vmin=-1, vmax=1, annot=True, ax=ax);\n\n# Give a title to the heatmap. Pad defines the distance of the title from the top of the heatmap.\nhm.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);\n</code></pre>\n",
    "is_answered": false,
    "view_count": 30,
    "answer_count": 0
  },
  {
    "title": "Techniques for adaptive prediction with feedback in an evolving feature space",
    "link": "https://stackoverflow.com/questions/79022083/techniques-for-adaptive-prediction-with-feedback-in-an-evolving-feature-space",
    "tags": [
      "machine-learning",
      "regression",
      "data-science",
      "reinforcement-learning"
    ],
    "body": "<p>I am working on a prediction problem where the target variable \ud835\udc66 is drawn from a normal distribution, and the relationship between the continuous feature space \ud835\udc4b and \ud835\udc66 remains stable over time. However, the target values (e.g., mean and standard deviation) evolve over time in response to changes in the system. I do not have prior knowledge of the true target values in advance, so I am leveraging techniques such as online regression and reinforcement learning (RL) to iteratively adjust my predictions based on feedback.</p>\n<p>The feedback mechanism only indicates whether my prediction was an overestimate or an underestimate, and this feedback is provided after a delay. The magnitude of error is unknown, and only directional feedback (over/under) is given. I am currently using an incremental approach where I update the predictions by adjusting them up or down based on the feedback.</p>\n<p>I have two main questions regarding improving this approach:</p>\n<p>(1) Improving Adaptive Adjustment Techniques:</p>\n<ul>\n<li>I am currently using online regression and RL, where the feedback informs me whether to increment or decrement the prior prediction. Are there more advanced techniques I should explore for making more intelligent adjustments beyond simple increments/decrements? Specifically, are there approaches that allow for more nuanced adjustments when feedback is limited to over/under indications, and that could potentially lead to faster convergence or better predictions over time?</li>\n</ul>\n<p>(2) Efficient Management of Adaptive Update Values:</p>\n<ul>\n<li>To enhance my current approach, I have considered maintaining dynamic upper and lower bounds for the predictions, adjusting these bounds based on feedback (i.e., narrowing the range between the over/under estimates). Another strategy I\u2019ve explored is using an exponentially weighted moving average (EWMA) of prior adjustments, where repeated underestimation leads to progressively larger corrections. However, managing these adjustments across a large feature space is computationally expensive.</li>\n<li>My initial approach of mapping feature \ud835\udc4b to these update values (such as bounds or EWMA adjustments) using a dictionary became impractical as the feature space grew. I also attempted mapping these values to a regression model, but it did not perform well, likely due to the non-stationary nature of the updates.</li>\n<li>Given that these adjustment values are not static targets but dynamically evolving based on feedback, what is the best way to manage or model them efficiently in a high-dimensional feature space? Are there more suitable strategies for adaptive updates in such a setting, possibly involving function approximation techniques or memory-efficient data structures?</li>\n</ul>\n",
    "is_answered": false,
    "view_count": 36,
    "answer_count": 0
  },
  {
    "title": "Selecting multiple columns (`MultiIndex` based) within a `DataFrameGroupBy`",
    "link": "https://stackoverflow.com/questions/79003349/selecting-multiple-columns-multiindex-based-within-a-dataframegroupby",
    "tags": [
      "python",
      "pandas",
      "group-by",
      "data-science"
    ],
    "body": "<p>I have a complex dataframe with multiple columns. All of them being <code>MultiIndex</code> based. At some point I wanted to be quite specific when it comes to estimating some metrics so I started experimenting with the <code>.groupby</code> method. I can manage to do the basics: 1) computing the aggregation method on the whole dataframe or 2) computing it for one specific column. However, I am interested in computing the aggreagtion method by indicating some of the names within the first column levels. This is quite easy to do when there is just a single level within the columns. In order to be understood, I created the following MRO that reproduces my idea and the errors I am getting:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import numpy as np\nimport pandas as pd\n\n\ncolumns = pd.MultiIndex.from_tuples(\n    [\n        (&quot;Dimensions&quot;, &quot;x&quot;),\n        (&quot;Dimensions&quot;, &quot;y&quot;),\n        (&quot;Dimensions&quot;, &quot;z&quot;),\n        (&quot;Coefficient&quot;, &quot;&quot;),\n        (&quot;Comments&quot;, &quot;&quot;),\n    ],\n    names=[&quot;Category&quot;, &quot;Details&quot;],\n)\n\ndf = pd.DataFrame(index=range(11), columns=columns)\ndf[(&quot;Dimensions&quot;, &quot;x&quot;)] = np.random.randint(1, 100, size=11)\ndf[(&quot;Dimensions&quot;, &quot;y&quot;)] = np.random.randint(1, 100, size=11)\ndf[(&quot;Dimensions&quot;, &quot;z&quot;)] = np.random.randint(1, 100, size=11)\ndf[(&quot;Coefficient&quot;, &quot;&quot;)] = np.random.randint(1, 50, size=11)  # Coefficient como entero aleatorio\ndf[(&quot;Comments&quot;, &quot;&quot;)] = np.random.choice([&quot;Good&quot;, &quot;Average&quot;, &quot;Bad&quot;], size=11)\ndf[&quot;Comments&quot;] = df[&quot;Comments&quot;].astype(&quot;category&quot;)\n\n# Basic metrics\nprint(df.groupby(&quot;Comments&quot;).mean())  # It works\nprint(df.groupby(&quot;Comments&quot;)[&quot;Dimensions&quot;].mean())  # It works\n\n# Selecting multiple columns within a MultiIndex based one. Different ideas I tried:\ndf.groupby(&quot;Comments&quot;)[&quot;Dimensions&quot;, &quot;Coefficient&quot;].mean()  # It does not work\ndf.groupby(&quot;Comments&quot;)[[&quot;Dimensions&quot;, &quot;Coefficient&quot;]].mean()  # It does not work\ndf.groupby(&quot;Comments&quot;).agg({&quot;Dimensions&quot;: &quot;mean&quot;, &quot;Coefficient&quot;: &quot;mean&quot;})  # It does not work\n\n</code></pre>\n",
    "is_answered": true,
    "view_count": 66,
    "answer_count": 2
  },
  {
    "title": "Input file specified two times",
    "link": "https://stackoverflow.com/questions/79003095/input-file-specified-two-times",
    "tags": [
      "python",
      "shell",
      "jupyter-notebook",
      "data-science",
      "data-preprocessing"
    ],
    "body": "<p>I am using shell in Jupyter with Python programming Language. When I use to prepare a dataset, I fail to complete it on sorting by column and case sensitive.</p>\n<p>The line is like this:</p>\n<pre><code>!head -n 5 $filename | sort -f -t' ' -k2\n</code></pre>\n<p>I have tried so many things to solve it, but my bad. I can not get out from this problem yet. I don't know much about the error and stack on the same place.</p>\n<p><a href=\"https://i.sstatic.net/kTH6NVb8.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/kTH6NVb8.png\" alt=\"enter image description here\" /></a></p>\n<p>For more specify the problem, I am attaching a screen shot of my code.</p>\n<p>I tried</p>\n<pre><code>!head -n 5 $filename | sort -f -t\\ -k2\n</code></pre>\n<p>But nothing helps me to overcome from this error: <strong>Input file specified two times</strong>.</p>\n",
    "is_answered": false,
    "view_count": 67,
    "answer_count": 0
  },
  {
    "title": "How can I shift a column in a dataframe x times into an array, apply a function, and create a new column?",
    "link": "https://stackoverflow.com/questions/78985850/how-can-i-shift-a-column-in-a-dataframe-x-times-into-an-array-apply-a-function",
    "tags": [
      "python",
      "pandas",
      "dataframe",
      "performance",
      "data-science"
    ],
    "body": "<p>I am having a hard time understanding how I can better improve this code:</p>\n<pre><code>from datetime import datetime\n\nimport math\nimport pandas as pd\nimport numpy as np\n\ndef trending(row, label, amt, prepend = False, postpend = False, reverse = True):\n    arr = []\n\n    if prepend:\n        arr.append(row[label])\n\n    for i in range(amt):\n        arr.append(row[f'{label}_{i}'])\n\n    if postpend:\n        arr.append(row[label])\n\n    if reverse:\n        arr.reverse()\n\n    if any(x == None or math.isnan(x) for x in arr):\n        return 0\n\n    return slope(arr)\n\ndef slope(arr):\n    if len(arr) &lt;= 1:\n        return 0\n    coeffs = np.polyfit(range(len(arr)), arr, 1)\n    slope = coeffs[-2]\n    return float(slope)\n\ndef trends(df):\n    start = datetime.now()\n\n    look_back = 5\n\n    for i in range(look_back):\n        j = i + 1\n        df2 = df[['close', 'pvi', 'nvi', 'smi', 'roc', 'macd', 'histogram', 'percent_b', 'height']]\n        df2 = df2.add_suffix(f'_{i}')\n        df2 = df2.shift(j)\n        df = pd.concat([df, df2], axis=1)\n\n    def close_trend(row):\n        return trending(row, 'close', look_back, True, False)\n\n    def pvi_trend(row):\n        return trending(row, 'pvi', look_back, True, False)\n\n    def nvi_trend(row):\n        return trending(row, 'nvi', look_back, True, False)\n\n    def smi_trend(row):\n        return trending(row, 'smi', look_back, True, False)\n\n    def macd_trend(row):\n        return trending(row, 'macd', look_back, True, False)\n\n    def roc_trend(row):\n        return trending(row, 'roc', look_back, True, False)\n\n    def histogram_trend(row):\n        return trending(row, 'histogram', look_back, True, False)\n\n    def percent_b_trend(row):\n        return trending(row, 'percent_b', look_back, True, False)\n\n    def height_trend(row):\n        return trending(row, 'height', look_back, True, False)\n    \n    df['close_trend'] = df.apply(close_trend, axis=1)\n    df['pvi_trend'] = df.apply(pvi_trend, axis=1)\n    df['nvi_trend'] = df.apply(nvi_trend, axis=1)\n    df['smi_trend'] = df.apply(smi_trend, axis=1)\n    df['macd_trend'] = df.apply(macd_trend, axis=1)\n    df['roc_trend'] = df.apply(roc_trend, axis=1)\n    df['histogram_trend'] = df.apply(histogram_trend, axis=1)\n    df['percent_b_trend'] = df.apply(percent_b_trend, axis=1)\n    df['height_trend'] = df.apply(height_trend, axis=1)\n\n    print(f'Trends function took {datetime.now() - start}')\n\n    return df\n\ndata = []\n\nfor i in range(10000):\n    data.append([545.9, 0.3333398862,   0.01673619117,  0.2111060119,   55.95725508,    1.100447539,    0.8652411735,   0.8219623901,   1.808441041,    46.79554862])\n\ncolumns = ['close', 'macd', 'histogram', 'roc', 'rsi', 'pvi', 'nvi', 'percent_b', 'height', 'smi']\n\ndf = pd.DataFrame(columns=columns, data=data)\n\ndf = trends(df)\n\nprint(df)\n</code></pre>\n<p>It works, but it is awfully slow. I have a dataframe without about 10k rows and it takes about 30s. Basically, what I am trying to accomplish is this:</p>\n<ol>\n<li>Shift the &quot;trend columns&quot; x number of times so each row has the x number of previous values</li>\n<li>Take those values + the current rows value and find the slope</li>\n<li>Set the slope to a &quot;trending&quot; column</li>\n</ol>\n",
    "is_answered": true,
    "view_count": 100,
    "answer_count": 2
  },
  {
    "title": "Neo4j error while creating graph database",
    "link": "https://stackoverflow.com/questions/78941026/neo4j-error-while-creating-graph-database",
    "tags": [
      "python",
      "neo4j",
      "data-science",
      "knowledge-graph",
      "rag"
    ],
    "body": "<pre><code>typellm_type = os.getenv(&quot;LLM_TYPE&quot;, &quot;ollama&quot;)\nif llm_type == &quot;ollama&quot;:\n    llm = ChatOllama(model=&quot;llama3.1&quot;, temperature=0)\nelse:\n    llm = ChatOpenAI(temperature=0, model=&quot;gpt-4o-mini&quot;)\nllm_transformer = LLMGraphTransformer(llm=llm)\n\ngraph_documents = llm_transformer.convert_to_graph_documents(documents)\n\ngraph_documents[0]\nGraphDocument(nodes=[Node(id='maternal chronic renal failure', type='Disease'), Node(id='sextigesta', type='Medical Condition'), Node(id='pregnancy', type='Event'), Node(id='patient', type='Person'), Node(id='regional hospital', type='Hospital'), Node(id='week 27+2', type='Time Period')], relationships=[Relationship(source=Node(id='patient', type='Person'), target=Node(id='sextigesta', type='Medical Condition'), type='IS_A'), Relationship(source=Node(id='patient', type='Person'), target=Node(id='maternal chronic renal failure', type='Disease'), type='HAS_CONDITION'), Relationship(source=Node(id='patient', type='Person'), target=Node(id='regional hospital', type='Hospital'), type='IS_ADMISSIONED_TO'), Relationship(source=Node(id='pregnancy', type='Event'), target=Node(id='maternal chronic renal failure', type='Disease'), type='HAS_RISK_FACTOR'), Relationship(source=Node(id='pregnancy', type='Event'), target=Node(id='week 27+2', type='Time Period'), type='IS_AT_STAGE')], source=Document(metadata={'source': 'text_files_en/S2254-28842017000200184-1.txt'}, page_content='A 45-year-old patient, sextigesta, with high-risk pregnancy due to maternal chronic renal failure, was admitted to a regional hospital, from a regional hospital, due to threatened preterm labor at week 27+2, with tocolytic treatment.'))\n\n\ngraph.add_graph_documents(\n    graph_documents,\n    baseEntityLabel=True,\n    include_source=True\n\n\n\n---------------------------------------------------------------------------\nClientError                               Traceback (most recent call last)\nCell In[17], line 1\n----&gt; 1 graph.add_graph_documents(\n      2     graph_documents,\n      3     baseEntityLabel=True,\n      4     include_source=True\n      5 )\n\nFile ~/Downloads/GraphRAG-with-Llama-3.1-main/myenv/lib/python3.9/site-packages/langchain_community/graphs/neo4j_graph.py:583, in Neo4jGraph.add_graph_documents(self, graph_documents, include_source, baseEntityLabel)\n    581     node.type = _remove_backticks(node.type)\n    582 # Import nodes\n--&gt; 583 self.query(\n    584     node_import_query,\n    585     {\n    586         &quot;data&quot;: [el.__dict__ for el in document.nodes],\n    587         &quot;document&quot;: document.source.__dict__,\n    588     },\n    589 )\n    590 # Import relationships\n    591 self.query(\n    592     rel_import_query,\n    593     {\n   (...)\n    607     },\n    608 )\n\nFile ~/Downloads/GraphRAG-with-Llama-3.1-main/myenv/lib/python3.9/site-packages/langchain_community/graphs/neo4j_graph.py:420, in Neo4jGraph.query(self, query, params)\n    418 try:\n    419     data = session.run(Query(text=query, timeout=self.timeout), params)\n--&gt; 420     json_data = [r.data() for r in data]\n    421     if self.sanitize:\n    422         json_data = [value_sanitize(el) for el in json_data]\n\nFile ~/Downloads/GraphRAG-with-Llama-3.1-main/myenv/lib/python3.9/site-packages/langchain_community/graphs/neo4j_graph.py:420, in &lt;listcomp&gt;(.0)\n    418 try:\n    419     data = session.run(Query(text=query, timeout=self.timeout), params)\n--&gt; 420     json_data = [r.data() for r in data]\n    421     if self.sanitize:\n    422         json_data = [value_sanitize(el) for el in json_data]\n\nFile ~/Downloads/GraphRAG-with-Llama-3.1-main/myenv/lib/python3.9/site-packages/neo4j/_sync/work/result.py:378, in Result.__iter__(self)\n    376     yield self._record_buffer.popleft()\n    377 elif self._streaming:\n--&gt; 378     self._connection.fetch_message()\n    379 elif self._discarding:\n    380     self._discard()\n\nFile ~/Downloads/GraphRAG-with-Llama-3.1-main/myenv/lib/python3.9/site-packages/neo4j/_sync/io/_common.py:178, in ConnectionErrorHandler.__getattr__.&lt;locals&gt;.outer.&lt;locals&gt;.inner(*args, **kwargs)\n    176 def inner(*args, **kwargs):\n    177     try:\n--&gt; 178         func(*args, **kwargs)\n    179     except (Neo4jError, ServiceUnavailable, SessionExpired) as exc:\n    180         assert not asyncio.iscoroutinefunction(self.__on_error)\n\nFile ~/Downloads/GraphRAG-with-Llama-3.1-main/myenv/lib/python3.9/site-packages/neo4j/_sync/io/_bolt.py:860, in Bolt.fetch_message(self)\n    856 # Receive exactly one message\n    857 tag, fields = self.inbox.pop(\n    858     hydration_hooks=self.responses[0].hydration_hooks\n    859 )\n--&gt; 860 res = self._process_message(tag, fields)\n    861 self.idle_since = monotonic()\n    862 return res\n\n# File ~/Downloads/GraphRAG-with-Llama-3.1-main/myenv/lib/python3.9/site-packages/neo4j/_sync/io/_bolt5.py:370, in Bolt5x0._process_message(self, tag, fields)\n    368 self._server_state_manager.state = self.bolt_states.FAILED\n    369 try:\n--&gt; 370     response.on_failure(summary_metadata or {})\n    371 except (ServiceUnavailable, DatabaseUnavailable):\n    372     if self.pool:\n\nFile ~/Downloads/GraphRAG-with-Llama-3.1-main/myenv/lib/python3.9/site-packages/neo4j/_sync/io/_common.py:245, in Response.on_failure(self, metadata)\n    243 handler = self.handlers.get(&quot;on_summary&quot;)\n    244 Util.callback(handler)\n--&gt; 245 raise Neo4jError.hydrate(**metadata)\n\nClientError: {code: Neo.ClientError.Procedure.ProcedureCallFailed} {message: Failed to invoke procedure `apoc.create.addLabels`: Caused by: org.neo4j.internal.kernel.api.exceptions.schema.IllegalTokenNameException: '' is not a valid token name. Token names cannot be empty or contain any null-bytes.}\n \n</code></pre>\n<p>I am trying to upload the text of a file document in neo4j to create a graphic knowledge base for RAG.\nFo this particular document, I am encountering the error\nClientError: {code: Neo.ClientError.Procedure.ProcedureCallFailed} {message: Failed to invoke procedure <code>apoc.create.addLabels</code>: Caused by: org.neo4j.internal.kernel.api.exceptions.schema.IllegalTokenNameException: '' is not a valid token name. Token names cannot be empty or contain any null-bytes.}</p>\n<p>please help.</p>\n",
    "is_answered": false,
    "view_count": 142,
    "answer_count": 0
  },
  {
    "title": "Count and groupby a specfic value",
    "link": "https://stackoverflow.com/questions/78931722/count-and-groupby-a-specfic-value",
    "tags": [
      "python",
      "pandas",
      "dataframe",
      "group-by",
      "data-science"
    ],
    "body": "<p>I have a dataframe where i want to count a specific value that occurs in a row.\nThis code below gives the right answer and now i want to add a new coluumn to my dataframe</p>\n<pre><code>occur = df.groupby(['Code_5elaag','Essentieel_Optioneel']).size()\noccur\n\n**Code_5elaag  Essentieel_Optioneel**\n1101         essentieel               8\n             optioneel                8\n1102         essentieel               8\n             optioneel               51\n1103         essentieel               8\n                                     ..\n96231        optioneel                6\n96232        essentieel               1\n             optioneel                2\n96290        essentieel               9\n             optioneel               17\n</code></pre>\n<p>When i assign a new colum to the frame this is the output:</p>\n<pre><code>uniq['ess'] = df.groupby(['Code_5elaag'])['Essentieel_Optioneel'].transform(np.size)\n\n    Code_5elaag Omschrijving_5elaag Soort_Skill Aantal_skills   ess\n0   1101    Officieren landmacht    taken   16  16              15\n16  1102    Officieren luchtmacht   taken   59  59              59\n75  1103    Officieren marechaussee taken   16  16              16\n\n</code></pre>\n<p>But that is not what i want i want to divide the amount of <strong>Aantal_skills</strong> to how much is <strong>essentieel and optioneel</strong> fo for the first row it should be <strong>8 essentieel</strong> and <strong>8 optional</strong></p>\n",
    "is_answered": true,
    "view_count": 60,
    "answer_count": 1
  },
  {
    "title": "Confusion Matrix in SQL",
    "link": "https://stackoverflow.com/questions/78919810/confusion-matrix-in-sql",
    "tags": [
      "sql",
      "postgresql",
      "data-science",
      "confusion-matrix"
    ],
    "body": "<p>I have some rules which gets fired if a criteria for a process gets fulfilled. I am calculating the performance of those rules in a confusion matrix form.</p>\n<p>If the rule was fired, it means the process was predicted positive. So if the process was actually positive that'll be a True Positive scenario <br>\nIf the rule was fired, it means the process was predicted positive. So if the process was not actually positive that'll be a False Positive scenario <br>\nIf the rule_name is NULL, it means none of the rule was fired, So if the process was actually negative that's a False Negative scenario.</p>\n<p>Note: if a process is null in the rule_name, that's a false negative for every rule because none of the rule got fired while the process actually failed.</p>\n<div class=\"s-table-container\"><table class=\"s-table\">\n<thead>\n<tr>\n<th>id</th>\n<th>rule_name</th>\n<th>true_label</th>\n<th>predicted_label</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>rule1</td>\n<td>positive</td>\n<td>positive</td>\n</tr>\n<tr>\n<td>2</td>\n<td>rule1</td>\n<td>positive</td>\n<td>positive</td>\n</tr>\n<tr>\n<td>3</td>\n<td>rule1</td>\n<td>negative</td>\n<td>positive</td>\n</tr>\n<tr>\n<td>4</td>\n<td>null</td>\n<td>positive</td>\n<td>positive</td>\n</tr>\n<tr>\n<td>5</td>\n<td>null</td>\n<td>positive</td>\n<td>positive</td>\n</tr>\n<tr>\n<td>6</td>\n<td>null</td>\n<td>negative</td>\n<td>positive</td>\n</tr>\n<tr>\n<td>7</td>\n<td>rule2</td>\n<td>positive</td>\n<td>positive</td>\n</tr>\n<tr>\n<td>8</td>\n<td>rule2</td>\n<td>negative</td>\n<td>positive</td>\n</tr>\n<tr>\n<td>9</td>\n<td>rule2</td>\n<td>negative</td>\n<td>positive</td>\n</tr>\n</tbody>\n</table></div>\n<p>My result set should look like:</p>\n<div class=\"s-table-container\"><table class=\"s-table\">\n<thead>\n<tr>\n<th></th>\n<th>True Positive</th>\n<th>False Positive</th>\n<th>True Negative</th>\n<th>False Negative</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>rule1</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>2</td>\n</tr>\n<tr>\n<td>rule2</td>\n<td>1</td>\n<td>2</td>\n<td>1</td>\n<td>2</td>\n</tr>\n</tbody>\n</table></div>\n<p>I am doing this in SQL. I can do this by filtering each rule_name i.e.</p>\n<pre><code>\nselect 'rule1' as rule\n    , count(case when rule_name = 'rule1' and true_label = 'positive' and predicted_label = 'positive' then 1 else null  end)as tp_count\n    , count(case when rule_name = 'rule1' and true_label != 'positive' and predicted_label = 'positive' then 1 else null  end)as fp_count\n    , count(case when (rule_name != 'rule1' or rule_name is null) and and true_label != 'positive' then 1 else null end) as tn_count\nfrom table\n;\n</code></pre>\n<p>I have multiple such rules so, I don't want to do this calculation each rule with <code>union all</code> I am looking for a better way to do this calculation. My main challenge is to count the True Negatives where I need to count the <code>rule_name</code> NULLs as well.</p>\n",
    "is_answered": true,
    "view_count": 240,
    "answer_count": 2
  },
  {
    "title": "the data augmentation doesn&#39;t work properly",
    "link": "https://stackoverflow.com/questions/78914899/the-data-augmentation-doesnt-work-properly",
    "tags": [
      "python",
      "deep-learning",
      "data-science",
      "data-augmentation",
      "image-augmentation"
    ],
    "body": "<p>according to me, the given code should create 5700 images, which is 10 times the number of original images, instead, when I check the shape of it, it only gives 1140, which is twice the number of images. What am I missing in this code?</p>\n<pre><code># Custom augmentation functions\ndef random_translate(image, max_translate):\n    shift = np.random.uniform(-max_translate, max_translate, 3)\n    translated_image = scipy.ndimage.shift(image, shift, mode='nearest')\n    return translated_image\n\ndef random_rotate(image, max_angle):\n    angles = np.random.uniform(-max_angle, max_angle, 3)\n    rotated_image = scipy.ndimage.rotate(image, angles[0], axes=(1, 2), reshape=False)\n    rotated_image = scipy.ndimage.rotate(rotated_image, angles[1], axes=(0, 2), reshape=False)\n    rotated_image = scipy.ndimage.rotate(rotated_image, angles[2], axes=(0, 1), reshape=False)\n    return rotated_image\n\ndef random_flip(image):\n    if random.random() &gt; 0.5:\n        image = np.flip(image, axis=0)\n    if random.random() &gt; 0.5:\n        image = np.flip(image, axis=1)\n    if random.random() &gt; 0.5:\n        image = np.flip(image, axis=2)\n    return image\n\ndef random_noise(image, noise_level=0.01):\n    noise = np.random.normal(0, noise_level, image.shape)\n    noisy_image = image + noise\n    return noisy_image\n\ndef random_brightness(image, max_delta=0.2):\n    delta = np.random.uniform(-max_delta, max_delta)\n    bright_image = np.clip(image + delta, 0, 1)  # Clip to maintain valid pixel range\n    return bright_image\n\ndef random_contrast(image, lower=0.8, upper=1.2):\n    factor = np.random.uniform(lower, upper)\n    mean = np.mean(image, axis=(0, 1, 2), keepdims=True)\n    contrast_image = np.clip((image - mean) * factor + mean, 0, 1)\n    return contrast_image\n\ndef random_scale(image, min_scale=0.9, max_scale=1.1):\n    scale = np.random.uniform(min_scale, max_scale)\n    height, width = image.shape[:2]\n    scaled_image = scipy.ndimage.zoom(image, (scale, scale, 1), order=1)\n    if scale &lt; 1.0:\n        pad_height = (height - scaled_image.shape[0]) // 2\n        pad_width = (width - scaled_image.shape[1]) // 2\n        scaled_image = np.pad(scaled_image, ((pad_height, pad_height), (pad_width, pad_width), (0, 0)), mode='constant')\n    else:\n        start_height = (scaled_image.shape[0] - height) // 2\n        start_width = (scaled_image.shape[1] - width) // 2\n        scaled_image = scaled_image[start_height:start_height + height, start_width:start_width + width]\n    return scaled_image\n\ndef random_shear(image, max_shear=0.2):\n    shear = np.random.uniform(-max_shear, max_shear)\n    afine_tf = tf.keras.preprocessing.image.random_shear(shear)\n    shear_image = tf.keras.preprocessing.image.apply_affine_transform(image, shear=afine_tf)\n    return shear_image\n\ndef elastic_transform(image, alpha=1000, sigma=30):\n    random_state = np.random.RandomState(None)\n    shape = image.shape\n    dx = scipy.ndimage.gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=&quot;constant&quot;, cval=0) * alpha\n    dy = scipy.ndimage.gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=&quot;constant&quot;, cval=0) * alpha\n    dz = scipy.ndimage.gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=&quot;constant&quot;, cval=0) * alpha\n    x, y, z = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]), np.arange(shape[2]), indexing='ij')\n    indices = np.reshape(x + dx, (-1, 1)), np.reshape(y + dy, (-1, 1)), np.reshape(z + dz, (-1, 1))\n    distored_image = scipy.ndimage.map_coordinates(image, indices, order=1, mode='reflect')\n    return distored_image.reshape(image.shape)\n\n\ndef custom_data_generator(images, labels, batch_size,\n                          max_translate=10, max_angle=15, noise_level=0.01,\n                          max_delta=0.2, lower_contrast=0.8, upper_contrast=1.2,\n                          min_scale=0.9, max_scale=1.1, max_shear=0.2, alpha=1000, sigma=30):\n    num_images = images.shape[0]\n    while True:\n        batch_indices = np.random.choice(num_images, batch_size)\n        batch_images = []\n        batch_labels = []\n        for idx in batch_indices:\n            image = images[idx]\n            label = labels[idx]\n\n            # Apply all augmentation functions\n            image = random_translate(image, max_translate)\n            image = random_rotate(image, max_angle)\n            image = random_flip(image)\n            image = random_noise(image, noise_level)\n            image = random_brightness(image, max_delta)\n            image = random_contrast(image, lower_contrast, upper_contrast)\n            image = random_scale(image, min_scale, max_scale)\n            image = tf.keras.preprocessing.image.random_shear(image, max_shear)\n            image = elastic_transform(image, alpha, sigma)\n\n            batch_images.append(image)\n            batch_labels.append(label)\n\n        batch_images = np.array(batch_images)\n        batch_labels = np.array(batch_labels)\n        yield batch_images, batch_labels\n\n\n# Directory to save the images\nsave_dir = 'mri_augmented_images'\nos.makedirs(save_dir, exist_ok=True)\n\nfor i in range(mri_resized.shape[0]):\n    for j in range(mri_resized.shape[3]):  # Loop over channels\n        img_array = mri_resized[i, :, :, j]\n        img = Image.fromarray((img_array * 255).astype('uint8'))  # Scale to [0, 255] and convert to uint8\n        img.save(os.path.join(save_dir, f'image_{i}_channel_{j}.png'))\n\n# Using the custom data generator to augment images and retain labels\ntarget_num_images = 5700\nbatch_size = 3  # Define your batch size\nnum_batches_needed = (target_num_images + batch_size - 1) // batch_size\ndata_gen = custom_data_generator(mri_resized, labels, batch_size=batch_size)\n\n# Generate and save augmented images with labels\nfor batch_images, batch_labels in data_gen:  # Directly iterate over the generator\n    for j, img_array in enumerate(batch_images):\n        img = Image.fromarray((img_array[:, :, 0] * 255).astype('uint8'))  # Save only the first channel\n        img_index = i * batch_size + j\n        img.save(os.path.join(save_dir, f'aug_image_{img_index}_label_{batch_labels[j]}.png'))\n\nprint(f&quot;Generated and saved {target_num_images} aug\n\nmented images.&quot;)\n</code></pre>\n",
    "is_answered": false,
    "view_count": 47,
    "answer_count": 0
  },
  {
    "title": "Adapting this formula to Python Polars, stuck on Euluers Number and Exponent",
    "link": "https://stackoverflow.com/questions/78897674/adapting-this-formula-to-python-polars-stuck-on-euluers-number-and-exponent",
    "tags": [
      "python",
      "data-science",
      "python-polars"
    ],
    "body": "<p>I have the following formula</p>\n<p><a href=\"https://i.sstatic.net/51VQfuuH.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/51VQfuuH.png\" alt=\"enter image description here\" /></a></p>\n<p>Citation: <a href=\"https://www.chess-journal.com/evaluatingSharpness1.html\" rel=\"nofollow noreferrer\">https://www.chess-journal.com/evaluatingSharpness1.html</a></p>\n<p>In terms of my data frame:</p>\n<pre><code>w = &quot;win&quot;\nd = &quot;draw&quot;\nl = &quot;loss&quot;\n</code></pre>\n<p>and e is our buddy Euler's number.</p>\n<p>What would be a tidy way of representing that final component? And should I use math.e for the e?</p>\n<p>This is what I have so far</p>\n<pre class=\"lang-py prettyprint-override\"><code>df = df.with_columns(\n    ((pl.min_horizontal(pl.col(&quot;win&quot;), pl.col(&quot;loss&quot;)) / 50) * (333 / pl.col(&quot;draw&quot;))).alias(&quot;sharpness&quot;)\n)\n</code></pre>\n",
    "is_answered": true,
    "view_count": 93,
    "answer_count": 1
  },
  {
    "title": "Wierd chart layout with pandas/matplotlib line chart",
    "link": "https://stackoverflow.com/questions/78893764/wierd-chart-layout-with-pandas-matplotlib-line-chart",
    "tags": [
      "python",
      "pandas",
      "matplotlib",
      "data-science"
    ],
    "body": "<p>I am new to Data Science and Python/NumPy/Pandas world in general.  I have a dataset which is an order/order line item/product dataset.  I am trying to plot a basic line chart with pandas:</p>\n<pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\npd.set_option('display.max_columns', 500)\n\ndata_link = &quot;https://&lt;github_link&gt;/orders.csv&quot;\ndata = pd.read_csv(data_link, encoding='latin-1')\ndaily_sales = (data\n             .groupby('Order Date')    # dimensions\n             .agg({'Sales': 'sum'})    # metric\n        )\n\ndaily_sales.index = pd.DatetimeIndex(daily_sales.index)\nfig, my_ax = plt.subplots(figsize=(14, 4))\ndaily_sales.plot(ax=my_ax)\nplt.tight_layout()\nplt.show()\n</code></pre>\n<p><a href=\"https://i.sstatic.net/X4ftRzcg.jpg\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/X4ftRzcg.jpg\" alt=\"The Wierd Line Plot\" /></a></p>\n<p>It should ideally look like this:</p>\n<p><a href=\"https://i.sstatic.net/9nwMyJKN.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/9nwMyJKN.png\" alt=\"How it should look \" /></a></p>\n<p>I did try .sort_values(by='Sales') to the groupby as some suggested that it is because that the X axis values are not sorted.  Though X axis is Order Date, I tried .sort_values(by='Order Date') to begin with.  Still similar graph.  Totally confused.  Please help.</p>\n",
    "is_answered": true,
    "view_count": 55,
    "answer_count": 1
  },
  {
    "title": "Assertion Error: 4 columns passed, passed data had 3 columns. Python/ipynb",
    "link": "https://stackoverflow.com/questions/78882452/assertion-error-4-columns-passed-passed-data-had-3-columns-python-ipynb",
    "tags": [
      "python",
      "error-handling",
      "html-table",
      "data-science"
    ],
    "body": "<p>I am trying to convert a pro-football-reference stats table into a csv file that shows defensive stats vs rushing. In my program I was able to get 10 columns of data for my runningback stats, however when I implement the same code to defense stats i get hit with the error &quot;Assertion Error: 4 columns passed, passed data had 3 columns&quot;\nHere is my code:</p>\n<pre><code>headers = [th.getText() for th in soup.findAll('tr')[1].findAll('th')]\nheaders = headers[1:]\nprint(headers[:5])\n</code></pre>\n<pre><code>rows = soup.findAll('tr', class_ = lambda table_rows: table_rows != &quot;thead&quot;)\nplayer_stats = [[td.getText() for td in rows [i].findAll('td')]\n                for i in range(len(rows))]\nplayer_stats = player_stats[2:]\n</code></pre>\n<pre><code>stats = pd.DataFrame(player_stats, columns = headers)\nstats.head()\n</code></pre>\n<p>From the research I have done I think its due to the table having multiple &quot;rush&quot; headers over the stat headers I want, it also seems to have problems grabbing the teams names. Here is the table I am trying to grab from minus the remaining 30 teams. Or visit this link for the table: <a href=\"http://pfref.com/pi/share/kZhgC\" rel=\"nofollow noreferrer\">http://pfref.com/pi/share/kZhgC</a></p>\n<pre><code>Rush    Rush    Rush\nTm                   Att     Yds     TD\nArizona Cardinals   25.8    119.2   0.82\nSeattle Seahawks    24.0    106.8   1.12\n</code></pre>\n",
    "is_answered": false,
    "view_count": 29,
    "answer_count": 0
  },
  {
    "title": "Plotting a 4D graph where X,Y,Z are independent and the fourth dimension is a heatmap that interpolates between points",
    "link": "https://stackoverflow.com/questions/78882225/plotting-a-4d-graph-where-x-y-z-are-independent-and-the-fourth-dimension-is-a-he",
    "tags": [
      "python",
      "matplotlib",
      "data-science",
      "interpolation",
      "heatmap"
    ],
    "body": "<p>I have been trying to plot a 4d heat map, where my X,Y,Z components are coordinates and thus independent of one another. I then want to map a fourth component onto each coordinate, and represent its relative intensities as a heatmap. I then want to interpolate between all points to produce a contour/gradient.</p>\n<p>This would be using real data, where I have measured the temperature of a liquid in a cup. For now I am using just plan numbers I made up, but I need to be using discrete numbers.</p>\n<p>Below is my code:</p>\n<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\n### INTENSITY VALUES FOR HEATMAP (TEMPERATURES) ###\n#Center\nfdr1 = 1,2,3,2,1\n#Inner cylinder\nfdr11 = 2,2,2,2,2,2,3,3,3,3,3,3,4,4,4,4,4,4,3,3,3,3,3,3,2,2,2,2,2,2\n#Middle cylinder\nfdr21 = 3,3,3,3,3,3,4,4,4,4,4,4,5,5,5,5,5,5,4,4,4,4,4,4,3,3,3,3,3,3\n#Outer cylinder\nfdr31 = 4,4,4,4,4,4,5,5,5,5,5,5,6,6,6,6,6,6,5,5,5,5,5,5,4,4,4,4,4,4\n\n###CO-ORDINATES###\na=0\nb=0\nrO = 7.5\nrM = 5\nrI = 2.5\nheights =  1,2,3,4,5\n\n#6 radial points\nO = np.linspace(0,2*np.pi,7)\n\nXs = []\nfor j in O:\n  xx = a+rO*np.cos(j)\n  Xs.append(xx)\nXs1 = []\nfor j in O:\n  xx = a+rM*np.cos(j)\n  Xs1.append(xx)\nXs2 = []\nfor j in O:\n  xx = a+rI*np.cos(j)\n  Xs2.append(xx)\n\nYs = []\nfor k in O:\n  yy = b+rO*np.sin(k)\n  Ys.append(yy)\nYs1 = []\nfor k in O:\n  yy = b+rM*np.sin(k)\n  Ys1.append(yy)\nYs2 = []\nfor k in O:\n  yy = b+rI*np.sin(k)\n  Ys2.append(yy)\n\nCoords = []\nfor i in heights:\n  ptsA = (0,0,i)\n  ptsB = (Xs[0],Ys[0],i)\n  ptsC = (Xs[1],Ys[1],i)\n  ptsD = (Xs[2],Ys[2],i) \n  ptsE = (Xs[3],Ys[3],i)\n  ptsF = (Xs[4],Ys[4],i)\n  ptsG = (Xs[5],Ys[5],i)\n  CoordsO.append(ptsA)\n  CoordsO.append(ptsB)  \n  CoordsO.append(ptsC)\n  CoordsO.append(ptsD)\n  CoordsO.append(ptsE)\n  CoordsO.append(ptsF)\n  CoordsO.append(ptsG)\n\nCoords1 = []\nfor i in heights:\n  ptsB = (Xs1[0],Ys1[0],i)\n  ptsC = (Xs1[1],Ys1[1],i)\n  ptsD = (Xs1[2],Ys1[2],i) \n  ptsE = (Xs1[3],Ys1[3],i)\n  ptsF = (Xs1[4],Ys1[4],i)\n  ptsG = (Xs1[5],Ys1[5],i)\n  Coords1.append(ptsB)\n  Coords1.append(ptsC)\n  Coords1.append(ptsD)\n  Coords1.append(ptsE)\n  Coords1.append(ptsF)\n  Coords1.append(ptsG)\n\nCoords2 = []\nfor i in heights:\n  ptsB = (Xs2[0],Ys2[0],i)\n  ptsC = (Xs2[1],Ys2[1],i)\n  ptsD = (Xs2[2],Ys2[2],i) \n  ptsE = (Xs2[3],Ys2[3],i)\n  ptsF = (Xs2[4],Ys2[4],i)\n  ptsG = (Xs2[5],Ys2[5],i)\n  Coords2.append(ptsB)\n  Coords2.append(ptsC)\n  Coords2.append(ptsD)\n  Coords2.append(ptsE)\n  Coords2.append(ptsF)\n  Coords2.append(ptsG)\n\nCoordsOT = np.transpose(CoordsO)\nCoordsMT = np.transpose(Coords1)\nCoordsIT = np.transpose(Coords2)\n\nXSO,YSO,ZSO = CoordsOT\nXSM,YSM,ZSM = CoordsMT\nXSI,YSI,ZSI = CoordsIT\n\n###Heat Map###\ngrad = 'coolwarm'\n\nmB = plt.cm.ScalarMappable(cmap=grid)\n\ncA = fdr11\nfAcolors = mB.to_rgba(cA)\n\ncB = fdr21\nfAcolors = mB.to_rgba(cA)\n\ncC = fdr31\nfAcolors = mB.to_rgba(cA)\n\ndef data_for_cylinder(center_x,center_y,radius,height_z):\n  z = np.linspace(0,height_z,50)\n  theta = np.linspace(0,2*np.pi,50)\n  theta_grid, z_grid = np.meshgrid(theta,z)\n  x_grid = radius*np.cos(theta_grid) + center_x\n  y_grid = radius*np.sin(theta_grid) + center_y\n  return x_grid,y_grid,z_grid\n\nfig = plt.figure(figsize=(15,15), dpi=300\nax = fig.add_subplot(111,projection='3d'\n\nfig.colorbar(mpl.cm.ScalarMappable(norm = mpl.colors.Normalize(0,1),cmap=grad),\n  ax=ax,orientation='vertical')\nXo,Yo,Zo = data_for_cylinder(0,0,7.5,5)\nXm,Ym,Zm = data_for_cylinder(0,0,5,5)\nXi,Yi,Zi = data_for_cylinder(0,0,2.5,5)\nax.plot_surface(Xo,Yo,Zo,alpha=0.05)\nax.plot_surface(Xm,Ym,Zm,alpha=0.05)\nax.plot_surface(Xi,Yi,Zi,alpha=0.05)\nax.scatter(XSO,YSO,ZSO,facecolors=fAcolors,marker='o',s=200,edgecolors='black')\nax.scatter(XSM,YSM,ZSM,facecolors=fBcolors,marker='o',s=200,edgecolors='black')\nax.scatter(XSI,YSI,ZSI,facecolors=fCcolors,marker='o',s=200,edgecolors='black')\n\nplt.show()\n</code></pre>\n<p>This produces:</p>\n<p><a href=\"https://i.sstatic.net/2V5Rn9M6.png\" rel=\"nofollow noreferrer\">enter image description here</a></p>\n<p>As you can see, it plots 3 sets of data as cylinders, however the heatmapping doesn't really work, and I am lost on how to interpolate these points. If anyone has an insights, or a completely different way to visualize this data please let me know.</p>\n<p>I have successfully plotted the 3d data into cylinders on my plot. I have also plotted the intensities to each of these points, however I have noticed that the heatmap is not one that joints all three data sets, which means it is wrong. I couldn't figure out how to make one heatmap/colors apply to all three data sets so that it normalized to the min and max of all three.</p>\n<p>I cannot figure out at all how to interpolate between these points. I am at a loss.</p>\n",
    "is_answered": false,
    "view_count": 58,
    "answer_count": 0
  },
  {
    "title": "Nifty50 data science project in python Error occuring KeyError: &#39;Date&#39;",
    "link": "https://stackoverflow.com/questions/78879312/nifty50-data-science-project-in-python-error-occuring-keyerror-date",
    "tags": [
      "python",
      "database",
      "deep-learning",
      "data-science",
      "project"
    ],
    "body": "<p>I am working on Nifty50 dataset as my Data science project but this error occurs when I'm trying to implement datetime. Please help me?</p>\n<p>This is my code to change Date which is an object into datetime:</p>\n<pre><code>nfty50_data['Date'] = pd.to_datetime(nifty50_data['Date'])\n</code></pre>\n<p>but that error occurs:</p>\n<blockquote>\n<p>KeyError: 'Date'</p>\n</blockquote>\n",
    "is_answered": true,
    "view_count": 63,
    "answer_count": 1
  },
  {
    "title": "Error get_features_name_out in getting back the feature name",
    "link": "https://stackoverflow.com/questions/78874263/error-get-features-name-out-in-getting-back-the-feature-name",
    "tags": [
      "python",
      "pandas",
      "scikit-learn",
      "data-science"
    ],
    "body": "<p>I want to know the feature importance to my data, so I use permutation_importance. When I get the result, it seems the feature already decoded, and I want to know the name of my feauture using <code>get_features_name_out</code>. It turns an error <code>'StandardScaler' object has no attribute 'get_feature_names_out' </code>. If I tried to interprest manually, I am afraid there is a mistake in order. It should be (3,0,1,2) in order. Smoker, age, bmi, sex .\nHere is the code</p>\n<pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.inspection import permutation_importance\n\n# Prepare data\nX = df[['age', 'bmi', 'sex', 'smoker']]\ny = df['charges']\n\n# Define the preprocessor\ncategorical_transformer = OneHotEncoder(drop='first', sparse=False)\nnumerical_transformer = StandardScaler()\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, ['age', 'bmi']),\n        ('cat', categorical_transformer, ['sex', 'smoker'])\n    ]\n)\n\n# Preprocess the data\nX_preprocessed = preprocessor.fit_transform(X)\n\n# Extract feature names\nnum_features = numerical_transformer.get_feature_names_out(['age', 'bmi'])\ncat_features = categorical_transformer.get_feature_names_out(['sex', 'smoker'])\nfeature_names = np.concatenate([num_features, cat_features])\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)\n\n# Train KNeighborsRegressor\nknn_regressor = KNeighborsRegressor()\nreg_model = knn_regressor.fit(X_train, y_train)\n\n# Evaluate feature importance using permutation importance\nresults = permutation_importance(knn_regressor, X_test, y_test, n_repeats=10, random_state=42, scoring='neg_mean_squared_error')\n\n# Display feature importances with names\nfor i, importance in enumerate(results.importances_mean):\n    print(f&quot;Feature '{feature_names[i]}': Importance: {importance}&quot;)\n\n\nsorted_indices = np.argsort(results.importances_mean)\nfor i in sorted_indices[::-1]:\n    print(f&quot;Feature '{feature_names[i]}', Importance: {results.importances_mean[i]}&quot;)\n</code></pre>\n<p>I want to know the names of feature back. And maybe the explanation why the order of feature importance is not correct, because I have plot manually between charges vs each feature, the correct order should be smoker, age, bmi, sex.</p>\n",
    "is_answered": false,
    "view_count": 43,
    "answer_count": 1
  },
  {
    "title": "How to deal with multiple dictionary entries that have identical keys but different values (json returned from API)",
    "link": "https://stackoverflow.com/questions/78872379/how-to-deal-with-multiple-dictionary-entries-that-have-identical-keys-but-differ",
    "tags": [
      "python",
      "json",
      "data-science"
    ],
    "body": "<p>So the problem is the following I made a station on the Banco do Brasil website using the hidden Api that returned me values from 414 cities However It happened crazy that for each who made the application had the brilliant idea of not formatting any key says the all the keys were with the same name \u201cNomeBeneficiario\u201d (15 thousand lines), I find it incredible that this fact happened, certainly an exceptional error</p>\n<p>Does anyone know how to reformat, or how to extract the values I want?</p>\n<p>and this is just a small portion of the json I used as an example, the original has 15,000 lines, I use one looping do collect data</p>\n<pre><code>{\n    &quot;January&quot;: {\n        &quot;quantidadeBeneficio&quot;: 43,\n        &quot;indicadorContinuarPesquisa&quot;: &quot;N&quot;,\n        &quot;quantidadeOcorrencia&quot;: [\n            {\n                &quot;nomeBeneficio&quot;: &quot;ITAMARAJU                                         -BA&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;ICS   - ICMS ESTADUAL&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;DATA                PARCELA                                   VALOR DISTRIBUIDO&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;03.01.2023        COTA-PARTE                                        146.746,17C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO SAUDE                                      22.011,92D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO FUNDEB                                     29.349,23D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  TOTAL NA DATA                                      95.385,02C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;10.01.2023        COTA-PARTE                                         30.770,32C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO SAUDE                                       4.615,54D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO FUNDEB                                      6.154,06D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  TOTAL NA DATA                                      20.000,72C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;17.01.2023        COTA-PARTE                                        646.359,98C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO SAUDE                                      96.953,99D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO FUNDEB                                    129.271,99D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  TOTAL NA DATA                                     420.134,00C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;24.01.2023        COTA-PARTE                                        186.707,09C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO SAUDE                                      28.006,06D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO FUNDEB                                     37.341,41D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  TOTAL NA DATA                                     121.359,62C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;31.01.2023        COTA-PARTE                                        345.724,35C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO SAUDE                                      51.858,65D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO FUNDEB                                     69.144,87D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  TOTAL NA DATA                                     224.720,83C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;TOTAL POR PARCELA / NATUREZA&quot;\n            }, \n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  COTA-PARTE                                      1.356.307,91C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO SAUDE                                     203.446,16D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO FUNDEB                                    271.261,56D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEBITO FUNDO                                      474.707,72D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  CREDITO FUNDO                                   1.356.307,91C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;TOTAL DISTRIBUIDO NO PERIODO&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEBITO BENEF.                                     474.707,72D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  CREDITO BENEF.                                  1.356.307,91C&quot;\n            }\n        ]\n    },\n    &quot;February&quot;: {\n        &quot;quantidadeBeneficio&quot;: 38,\n        &quot;indicadorContinuarPesquisa&quot;: &quot;N&quot;,\n        &quot;quantidadeOcorrencia&quot;: [\n            {\n                &quot;nomeBeneficio&quot;: &quot;ITAMARAJU                                         -BA&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;ICS   - ICMS ESTADUAL&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;DATA                PARCELA                                   VALOR DISTRIBUIDO&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;07.02.2023        COTA-PARTE                                        292.297,18C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO SAUDE                                      43.844,57D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO FUNDEB                                     58.459,43D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  TOTAL NA DATA                                     189.993,18C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;14.02.2023        COTA-PARTE                                        579.379,81C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO SAUDE                                      86.906,97D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO FUNDEB                                    115.875,96D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  TOTAL NA DATA                                     376.596,88C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;23.02.2023        COTA-PARTE                                        231.268,18C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO SAUDE                                      34.690,22D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO FUNDEB                                     46.253,63D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  TOTAL NA DATA                                     150.324,33C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;28.02.2023        COTA-PARTE                                        255.599,39C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO SAUDE                                      38.339,90D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO FUNDEB                                     51.119,87D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  TOTAL NA DATA                                     166.139,62C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;TOTAL POR PARCELA / NATUREZA&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  COTA-PARTE                                      1.358.544,56C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO SAUDE                                     203.781,66D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO FUNDEB                                    271.708,89D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEBITO FUNDO                                      475.490,55D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  CREDITO FUNDO                                   1.358.544,56C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;TOTAL DISTRIBUIDO NO PERIODO&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEBITO BENEF.                                     475.490,55D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  CREDITO BENEF.                                  1.358.544,56C&quot;\n            }\n        ]\n    },\n    &quot;March&quot;: {\n        &quot;quantidadeBeneficio&quot;: 38,\n        &quot;indicadorContinuarPesquisa&quot;: &quot;N&quot;,\n        &quot;quantidadeOcorrencia&quot;: [\n            {\n                &quot;nomeBeneficio&quot;: &quot;ITAMARAJU                                         -BA&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;ICS   - ICMS ESTADUAL&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;DATA                PARCELA                                   VALOR DISTRIBUIDO&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;07.03.2023        COTA-PARTE                                        539.023,36C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO SAUDE                                      80.853,50D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO FUNDEB                                    107.804,67D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  TOTAL NA DATA                                     350.365,19C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;14.03.2023        COTA-PARTE                                        541.413,33C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO SAUDE                                      81.211,99D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO FUNDEB                                    108.282,66D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  TOTAL NA DATA                                     351.918,68C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;21.03.2023        COTA-PARTE                                        190.618,47C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO SAUDE                                      28.592,77D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO FUNDEB                                     38.123,69D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  TOTAL NA DATA                                     123.902,01C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;28.03.2023        COTA-PARTE                                        226.588,95C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO SAUDE                                      33.988,34D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO FUNDEB                                     45.317,79D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  TOTAL NA DATA                                     147.282,82C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;TOTAL POR PARCELA / NATUREZA&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  COTA-PARTE                                      1.497.644,11C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO SAUDE                                     224.646,60D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO FUNDEB                                    299.528,81D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEBITO FUNDO                                      524.175,41D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  CREDITO FUNDO                                   1.497.644,11C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;TOTAL DISTRIBUIDO NO PERIODO&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEBITO BENEF.                                     524.175,41D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  CREDITO BENEF.                                  1.497.644,11C&quot;\n            }\n        ]\n    },\n    &quot;April&quot;: {\n        &quot;quantidadeBeneficio&quot;: 38,\n        &quot;indicadorContinuarPesquisa&quot;: &quot;N&quot;,\n        &quot;quantidadeOcorrencia&quot;: [\n            {\n                &quot;nomeBeneficio&quot;: &quot;ITAMARAJU                                         -BA&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;ICS   - ICMS ESTADUAL&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;DATA                PARCELA                                   VALOR DISTRIBUIDO&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;04.04.2023        COTA-PARTE                                        627.139,78C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO SAUDE                                      94.070,96D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO FUNDEB                                    125.427,95D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  TOTAL NA DATA                                     407.640,87C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;11.04.2023        COTA-PARTE                                         93.830,51C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO SAUDE                                      14.074,57D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO FUNDEB                                     18.766,10D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  TOTAL NA DATA                                      60.989,84C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;18.04.2023        COTA-PARTE                                        674.866,62C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO SAUDE                                     101.229,99D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO FUNDEB                                    134.973,32D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  TOTAL NA DATA                                     438.663,31C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;25.04.2023        COTA-PARTE                                        192.545,93C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO SAUDE                                      28.881,88D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO FUNDEB                                     38.509,18D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  TOTAL NA DATA                                     125.154,87C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;TOTAL POR PARCELA / NATUREZA&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  COTA-PARTE                                      1.588.382,84C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO SAUDE                                     238.257,40D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO FUNDEB                                    317.676,55D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEBITO FUNDO                                      555.933,95D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  CREDITO FUNDO                                   1.588.382,84C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;TOTAL DISTRIBUIDO NO PERIODO&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEBITO BENEF.                                     555.933,95D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  CREDITO BENEF.                                  1.588.382,84C&quot;\n            }\n        ]\n    },\n    &quot;May&quot;: {\n        &quot;quantidadeBeneficio&quot;: 43,\n        &quot;indicadorContinuarPesquisa&quot;: &quot;N&quot;,\n        &quot;quantidadeOcorrencia&quot;: [\n            {\n                &quot;nomeBeneficio&quot;: &quot;ITAMARAJU                                         -BA&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;ICS   - ICMS ESTADUAL&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;DATA                PARCELA                                   VALOR DISTRIBUIDO&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;03.05.2023        COTA-PARTE                                        873.381,70C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO SAUDE                                     131.007,25D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO FUNDEB                                    174.676,34D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  TOTAL NA DATA                                     567.698,11C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;09.05.2023        COTA-PARTE                                         63.508,36C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO SAUDE                                       9.526,25D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO FUNDEB                                     12.701,67D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  TOTAL NA DATA                                      41.280,44C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;16.05.2023        COTA-PARTE                                        642.245,37C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO SAUDE                                      96.336,80D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO FUNDEB                                    128.449,07D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  TOTAL NA DATA                                     417.459,50C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;23.05.2023        COTA-PARTE                                        208.658,97C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO SAUDE                                      31.298,84D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO FUNDEB                                     41.731,79D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  TOTAL NA DATA                                     135.628,34C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;30.05.2023        COTA-PARTE                                        361.045,93C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO SAUDE                                      54.156,88D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  DEDUCAO FUNDEB                                     72.209,18D&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;                  TOTAL NA DATA                                     234.679,87C&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;&quot;\n            },\n            {\n                &quot;nomeBeneficio&quot;: &quot;TOTAL POR PARCELA / NATUREZA&quot;\n            },\n            {\n\n\n            {\n                                     \n    }\n}\n</code></pre>\n<p>i have used this code to print the values after identify &quot;total Parcela&quot; and returned the last elements]</p>\n<pre><code>import json\nimport polars as pl\n\nwith open(r&quot;C:\\Users\\Henrique RIbeiro\\Documents\\projetos em andamentos\\daf extra\u00e7\u00e3o\\daf_extractions\\json4\\renomeados\\ABAIRA_dados_anuais.json&quot;, &quot;r&quot;, encoding=&quot;utf-8&quot;) as file:\n    data = json.load(file)\n\nmonths = [&quot;January&quot;, &quot;February&quot;, &quot;March&quot;, &quot;April&quot;, &quot;May&quot;, &quot;June&quot;, &quot;July&quot;, &quot;August&quot;, &quot;September&quot;, &quot;October&quot;, &quot;November&quot;, &quot;December&quot;]\n\nfor month in months:\n    if month in data:\n        after_total_parcela = False\n        print(f&quot;{month}&quot;)\n        \n        # Iterate over each month\n        for item in data[month]['quantidadeOcorrencia']:\n            benefit_name = item.get('nomeBeneficio', '').strip()\n            if benefit_name == &quot;TOTAL POR PARCELA / NATUREZA&quot;:\n                after_total_parcela = True\n            elif after_total_parcela and benefit_name:\n                print(benefit_name)\n        \n        print(&quot;\\n&quot;)  \n</code></pre>\n",
    "is_answered": false,
    "view_count": 57,
    "answer_count": 0
  },
  {
    "title": "Why is it that calling standard sum on a numpy array produces a different result than numpy.sum?",
    "link": "https://stackoverflow.com/questions/78869326/why-is-it-that-calling-standard-sum-on-a-numpy-array-produces-a-different-result",
    "tags": [
      "python",
      "numpy",
      "data-science"
    ],
    "body": "<p>Observe in the following code, creating an numpy array and calling the builtin python <code>sum</code> function produces different results than <code>numpy.sum</code></p>\n<p>How is numpy's sum function implemented? And why is the result different?</p>\n<pre><code>test = [.1]*10\ntest = [np.float64(x) for x in test]\ntest[5]= np.float64(-.9)\n\nd = [np.asarray(test) for x in range(0,60000)]\nsum(sum(d))\n</code></pre>\n<p>outputs</p>\n<pre><code>np.float64(-1.7473212210461497e-08)\n</code></pre>\n<p>but</p>\n<pre><code>np.sum(d)\n</code></pre>\n<p>outputs</p>\n<pre><code>np.float64(9.987344284922983e-12)\n</code></pre>\n",
    "is_answered": true,
    "view_count": 162,
    "answer_count": 2
  },
  {
    "title": "R: monte carlo binomial gambling simulation",
    "link": "https://stackoverflow.com/questions/78868809/r-monte-carlo-binomial-gambling-simulation",
    "tags": [
      "r",
      "statistics",
      "data-science",
      "montecarlo"
    ],
    "body": "<p>I am trying to figure out how to simulate a biased gambling problem using monte carlo simulations.</p>\n<p><strong>The problem is:</strong>\nSimulate two players tossing a coin; A and B.\nPlayer A has a 0.55 chance to win. Player B has a 0.45 chance.\nEach player starts with $3.\nIf one player wins they take $1 from the other.\nThe game ends when either one player has all the money OR 25 iterations have been played.\nI then want to plot the relative frequencies of players winning, then run this many time in order to get the estimates for player A and player B winning all the money.</p>\n<p>What I am stuck on is getting the montecarlo simulations going and the calculating the probability of one player accumulating all the other player's money.</p>\n<p>So far I can generate the data frame for one game and plot it.</p>\n<pre><code>Game &lt;- c('Bethany', 'Algernon')   #outcomes in the game\n\n#initialise an empty df\nGames_data &lt;- data.frame(Game = numeric(),\n                        winner = character(),\n                        Bethany_bank = numeric(),\n                        Algernon_bank = numeric(),\n                        Bethany_Freq = numeric(),\n                        Algernon_Freq =  numeric()\n)\n\n#intialise variables\ncount &lt;- 26\ni &lt;- 1\ntemp_Bethany_bank &lt;- 3\ntemp_Algernon_bank &lt;- 3\n\n#populate the data frame until 25 games or someone wins\nwhile(i &lt; count) {\n  temp_game &lt;- i\n  temp_winner &lt;- sample(Game, prob =c(0.55, 0.45), size = 1)\n  \n  if(temp_winner == 'Bethany') {\n    temp_Bethany_bank &lt;- temp_Bethany_bank + 1\n    temp_Algernon_bank &lt;- temp_Algernon_bank - 1\n  } else {\n      temp_Bethany_bank &lt;- temp_Bethany_bank - 1\n      temp_Algernon_bank &lt;- temp_Algernon_bank + 1}\n  \n  temp_Bethany_freq = 0.0\n  temp_Algernon_freq = 0.0\n  \n  temp &lt;- data.frame(Game = temp_game,\n                     winner = temp_winner,\n                     Bethany_bank = temp_Bethany_bank,\n                     Algernon_bank = temp_Algernon_bank,\n                     Bethany_Freq = temp_Bethany_freq,\n                     Algernon_Freq = temp_Algernon_freq\n                     )\n  \n  Games_data &lt;- rbind(Games_data, temp)\n  \n  Games_data$Bethany_Freq &lt;- cumsum(Games_data$winner == 'Bethany') / 1:nrow(Games_data)\n  Games_data$Algernon_Freq &lt;- cumsum(Games_data$winner == 'Algernon') / 1:nrow(Games_data)\n  \n  if(Games_data$Bethany_bank[i] &lt;= 0 || Games_data$Algernon_bank[i] &lt;= 0) {break} else {i &lt;- i + 1}\n}\n\n#show the dataframe and the plot:\nGames_data\n\nggplot(data = Games_data) +\n  geom_point(aes(x = Game, y =  Bethany_Freq), color = 'coral', alpha = 0.8) + #Bethany's wins\n  geom_point(aes(x = Game, y =  Algernon_Freq), color = 'steelblue', alpha = 0.8) + #Bethany's wins\n  geom_line(aes(x = Game, y =  Bethany_Freq), color = 'coral') +\n  geom_line(aes(x = Game, y =  Algernon_Freq), color = 'steelblue') +\n  theme_classic() + \n  labs(title = &quot;Relative frequency plots for Bethany vs Algernon over 25 games&quot;) \n\n</code></pre>\n<p>How can I run this many times, like 100 or 1000, store the outputs in an object, plot all the trials and then calculate the probability of one player getting all the money?</p>\n<p>Cheers in advance!</p>\n",
    "is_answered": true,
    "view_count": 116,
    "answer_count": 1
  },
  {
    "title": "Finding maximum of multiple arrays in respect to their index, that can vary",
    "link": "https://stackoverflow.com/questions/78862342/finding-maximum-of-multiple-arrays-in-respect-to-their-index-that-can-vary",
    "tags": [
      "pandas",
      "numpy",
      "data-science",
      "scientific-computing"
    ],
    "body": "<h3>Problem description:</h3>\n<p>I have got values that do correspond to specific <em>conrer_ids</em>, <em>elements</em>. From multiple sets of these values I need to find maximum ones and generate arrays containing <em>conrer_ids</em>, <em>elements</em> and their respective values.</p>\n<hr />\n<h3>example:</h3>\n<p>old values:</p>\n<div class=\"s-table-container\"><table class=\"s-table\">\n<thead>\n<tr>\n<th style=\"text-align: left;\"></th>\n<th style=\"text-align: right;\">value</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\">(0, 'element 0')</td>\n<td style=\"text-align: right;\">0.831994</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">(1, 'element 0')</td>\n<td style=\"text-align: right;\">0.575897</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">(2, 'element 0')</td>\n<td style=\"text-align: right;\">0.0241688</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">(3, 'element 0')</td>\n<td style=\"text-align: right;\">0.930143</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">(4, 'element 1')</td>\n<td style=\"text-align: right;\">0.43566</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">(5, 'element 1')</td>\n<td style=\"text-align: right;\">0.00264849</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">(6, 'element 1')</td>\n<td style=\"text-align: right;\">0.133718</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">(7, 'element 1')</td>\n<td style=\"text-align: right;\">0.171456</td>\n</tr>\n</tbody>\n</table></div>\n<p>new values:</p>\n<div class=\"s-table-container\"><table class=\"s-table\">\n<thead>\n<tr>\n<th style=\"text-align: left;\"></th>\n<th style=\"text-align: right;\">value</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\">(4, 'element 1')</td>\n<td style=\"text-align: right;\">0.584522</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">(5, 'element 1')</td>\n<td style=\"text-align: right;\">0.784499</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">(6, 'element 1')</td>\n<td style=\"text-align: right;\">0.206276</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">(7, 'element 1')</td>\n<td style=\"text-align: right;\">0.96535</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">(8, 'element 2')</td>\n<td style=\"text-align: right;\">0.203246</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">(9, 'element 2')</td>\n<td style=\"text-align: right;\">0.429909</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">(10, 'element 2')</td>\n<td style=\"text-align: right;\">0.979901</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">(11, 'element 2')</td>\n<td style=\"text-align: right;\">0.950208</td>\n</tr>\n</tbody>\n</table></div>\n<p>expected result:</p>\n<div class=\"s-table-container\"><table class=\"s-table\">\n<thead>\n<tr>\n<th style=\"text-align: left;\"></th>\n<th style=\"text-align: right;\">0</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\">(0, 'element 0')</td>\n<td style=\"text-align: right;\">0.831994</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">(1, 'element 0')</td>\n<td style=\"text-align: right;\">0.575897</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">(2, 'element 0')</td>\n<td style=\"text-align: right;\">0.0241688</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">(3, 'element 0')</td>\n<td style=\"text-align: right;\">0.930143</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">(4, 'element 1')</td>\n<td style=\"text-align: right;\">0.584522</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">(5, 'element 1')</td>\n<td style=\"text-align: right;\">0.784499</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">(6, 'element 1')</td>\n<td style=\"text-align: right;\">0.206276</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">(7, 'element 1')</td>\n<td style=\"text-align: right;\">0.96535</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">(8, 'element 2')</td>\n<td style=\"text-align: right;\">0.203246</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">(9, 'element 2')</td>\n<td style=\"text-align: right;\">0.429909</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">(10, 'element 2')</td>\n<td style=\"text-align: right;\">0.979901</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">(11, 'element 2')</td>\n<td style=\"text-align: right;\">0.950208</td>\n</tr>\n</tbody>\n</table></div>\n<p>I am tried using pd.concat(), df.max(), but it seems to be too slow. So I was wondering, if there is faster alternative. I need to make this work on arrays of ~ <strong>1e6</strong> length. I also tried some groupby solution, but it was not too much faster.</p>\n<h3>Code That I used</h3>\n<pre><code>import pandas as pd\nimport numpy as np\n\nELEMENT_COUNT_TOTAL = 3\nELEMENT_OVERLAP = 1\nELEMENT_OFFSET = 1\n\nif ELEMENT_COUNT_TOTAL &lt; ELEMENT_OVERLAP + ELEMENT_OFFSET:\n    raise ValueError(&quot;ELEMENT_COUNT_TOTAL should be greater than ELEMENT_OVERLAP + ELEMENT_OFFSET&quot;)\n\nCORNER_COUNT_TOTAL = ELEMENT_COUNT_TOTAL * 4\n\n_corner_ids = [i for i in range(CORNER_COUNT_TOTAL)]\n_elements = [f&quot;element {i // 4}&quot; for i in range(CORNER_COUNT_TOTAL)]\n\nold_elements = _elements[:(ELEMENT_OVERLAP + ELEMENT_OFFSET) * 4]\nold_corner_ids = np.array(_corner_ids[:(ELEMENT_OVERLAP + ELEMENT_OFFSET) * 4])\n\nnew_elements = _elements[ELEMENT_OFFSET * 4 :ELEMENT_COUNT_TOTAL * 4]\nnew_corner_ids = np.array(_corner_ids[ELEMENT_OFFSET * 4 :ELEMENT_COUNT_TOTAL * 4])\n\ndef generate_values(count) -&gt; np.ndarray:\n    return np.random.rand(count)\n\nold_values = generate_values(len(old_corner_ids))\nnew_values = generate_values(len(new_corner_ids))\n\nold_df = pd.DataFrame({&quot;value&quot;: old_values}, index=[old_corner_ids, old_elements])\nnew_df = pd.DataFrame({&quot;value&quot;: new_values}, index=[new_corner_ids, new_elements])\n\nprint(old_df.to_markdown())\nprint()\nprint(new_df.to_markdown())\nprint()\n\ncombined_df = pd.concat([old_df, new_df], axis=1, keys=[&quot;old&quot;, &quot;new&quot;])\nresult_df = combined_df.max(axis=1)\nprint(result_df.to_markdown())\n</code></pre>\n<h3>EDIT:</h3>\n<p>Here are results, what I have tried for 1e3 elements:</p>\n<pre><code>%%timeit -n 1000\nresult_df = pd.concat([old_df, new_df], axis=1).max(axis=1)\n</code></pre>\n<p>2.4 ms \u00b1 34.2 \u03bcs per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each)</p>\n<pre><code>%%timeit -n 1000\nresult_df = pd.concat([old_df, new_df]).groupby(level=[0, 1]).max()\n</code></pre>\n<p>2.82 ms \u00b1 518 \u03bcs per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)</p>\n<pre><code>%%timeit -n 1000\ncombined_df = (pd.concat([old_df, new_df], axis=0).reset_index().rename(columns={&quot;level_0&quot;: &quot;corner_id&quot;, &quot;level_1&quot;: &quot;element&quot;}))\nresult_df = combined_df.groupby([&quot;element&quot;, &quot;corner_id&quot;])[&quot;value&quot;].max().reset_index()\n</code></pre>\n<p>2.02 ms \u00b1 34 \u03bcs per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each)</p>\n<p>Thank you for any suggestions, have a wonderful day!</p>\n",
    "is_answered": false,
    "view_count": 76,
    "answer_count": 2
  },
  {
    "title": "Getting ValueError: All arrays must be of the same length",
    "link": "https://stackoverflow.com/questions/78858321/getting-valueerror-all-arrays-must-be-of-the-same-length",
    "tags": [
      "python",
      "python-3.x",
      "machine-learning",
      "regression",
      "data-science"
    ],
    "body": "<p>I have been trying to convert a dictionary into a dataframe but everytime i keep getting ValueError: All arrays must be of the same length. i Have checkde the length of each array and confirmed them to be the same but i am still getting the same error</p>\n<pre><code>def metrics_from_pipes(pipes_dict):\n     for name, pipeline in pipes_dict.items():\n        \n        pipeline.fit(X_train, y_train)\n        y_pred_val = pipeline.predict(X_val)\n        y_pred_train = pipeline.predict(X_train)\n\n\ntrain_metrics = {\n            'model':list(pipes_dict.keys()),\n            'MAE':train_mae,\n            'MAPE':train_mape,\n            'RMSE':train_rmse,\n            'RSquared':train_rsquared\n        }\n        \n        train_metrics_data = pd.DataFrame(train_metrics)\n        val_metrics = {\n            'model':list(pipes_dict.keys()),\n            'MAE':val_mae,\n            'MAPE':val_mape,\n            'RMSE':val_rmse,\n            'RSquared':val_rsquared            \n        }\n        \n        val_metrics_data = pd.DataFrame(val_metrics,)\n\n        #Merging metrics from train and test set\n        train_val_metrics = train_metrics_data.merge(val_metrics_data,\n                                               on = 'Model',\n                                               how = 'left',\n                                               suffixes = ('_train', '_val'))\n        \n        # sorting columns \n        train_val_metrics = train_val_metrics.reindex(columns = ['Model',\n                                                               'MAE_train',\n                                                                'MAPE_train',\n                                                                'RMSE_train',\n                                                                'RSquared_train',\n                                                                'MAE_val',\n                                                                'MAPE_val',\n                                                                'RMSE_val',\n                                                                'RSquared_val'])\n        \n    \n    \n    return train_val_metrics.set_index('Model').transpose()\n\n# get the metrics table\nmetrics_table = metrics_from_pipes(pipelines)\n</code></pre>\n<p>running this code gives this error</p>\n<pre><code>ValueError                                Traceback (most recent call last)\nCell In[45], line 82\n     80     return train_val_metrics.set_index('Model').transpose()\n     81 # get the metrics table\n---&gt; 82 metrics_table = metrics_from_pipes(pipelines)\n     83 #print('Table 1: Base Models Metrics')\n     84 #metrics_table.style.background_gradient(cmap = Blues)\n     85 metrics_table\n\nCell In[45], line 50, in metrics_from_pipes(pipes_dict)\n     41 # aggregate the performance metric lists into seperate dataframes\n     42 train_metrics = {\n     43     'model':list(pipes_dict.keys()),\n     44     'MAE':train_mae,\n   (...)\n     47     'RSquared':train_rsquared\n     48 }\n---&gt; 50 train_metrics_data = pd.DataFrame(train_metrics)\n     51 val_metrics = {\n     52     'model':list(pipes_dict.keys()),\n     53     'MAE':val_mae,\n   (...)\n     56     'RSquared':val_rsquared            \n     57 }\n     59 val_metrics_data = pd.DataFrame(val_metrics,)\n\nValueError: All arrays must be of the same length\n</code></pre>\n<p>when i checked for the result of the dictionary for both train_metrics and val metrics, i got this</p>\n<pre><code>({'model': ['Linear Regression',\n   'Random Forest Regressor',\n   'Gradient Boost Regression',\n   'Extra Tree Regressor'],\n  'MAE': [829.1023412412194,\n   288.33455697065233,\n   712.9637267872279,\n   0.0010629575741748962],\n  'MAPE': [1.0302372135902111,\n   0.20937541440883897,\n   0.538244903316323,\n   6.306697580961048e-07],\n  'RMSE': [1120.5542708017374,\n   416.48933196590013,\n   1012.399201767692,\n   0.05804079289490426],\n  'RSquared': [0.5598288286601083,\n   0.9391916010838417,\n   0.6406981997919169,\n   0.9999999988190745]},\n {'model': ['Linear Regression',\n   'Random Forest Regressor',\n   'Gradient Boost Regression',\n   'Extra Tree Regressor'],\n  'MAE': [855.9254413559535,\n   802.5902302175274,\n   772.3140648475379,\n   839.9018341377154],\n  'MAPE': [1.0395487579496652,\n   0.5607987708065988,\n   0.5438627253681279,\n   0.5852285872937784],\n  'RMSE': [1148.6549900167981,\n   1158.8411708570625,\n   1109.6145558003204,\n   1223.23337689915],\n  'RSquared': [0.5876710102285392,\n   0.5803255834810521,\n   0.6152231339508221,\n   0.5323905190373128]})\n</code></pre>\n",
    "is_answered": true,
    "view_count": 79,
    "answer_count": 1
  },
  {
    "title": "How can I replace null values in polars with a prefix with ascending numbers?",
    "link": "https://stackoverflow.com/questions/78854478/how-can-i-replace-null-values-in-polars-with-a-prefix-with-ascending-numbers",
    "tags": [
      "python",
      "dataframe",
      "data-science",
      "data-cleaning",
      "python-polars"
    ],
    "body": "<p>I am trying to replace null values in my dataframe column by a prefix and ascending numbers(to make each unique).ie</p>\n<pre class=\"lang-py prettyprint-override\"><code>df = pl.from_repr(&quot;&quot;&quot;\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name         \u2506 asset_number \u2502\n\u2502 ---          \u2506 ---          \u2502\n\u2502 str          \u2506 str          \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Office Chair \u2506 null         \u2502\n\u2502 Office Chair \u2506 null         \u2502\n\u2502 Office Chair \u2506 null         \u2502\n\u2502 Office Chair \u2506 CMP - 001    \u2502\n\u2502 Office Chair \u2506 CMP - 005    \u2502\n\u2502 Office Chair \u2506 null         \u2502\n\u2502 Table        \u2506 null         \u2502\n\u2502 Table        \u2506 CMP - 007    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n&quot;&quot;&quot;)\n</code></pre>\n<p>the null values should be replaced to something like PREFIX - 001,PREFIX - 002,...</p>\n",
    "is_answered": true,
    "view_count": 121,
    "answer_count": 3
  },
  {
    "title": "How to use SDMX to get M2 money data from ECB?",
    "link": "https://stackoverflow.com/questions/78774766/how-to-use-sdmx-to-get-m2-money-data-from-ecb",
    "tags": [
      "dataframe",
      "data-science",
      "sdmx"
    ],
    "body": "<p>I need to fetch data from following 2 links for my analysis :</p>\n<ol>\n<li><a href=\"https://data.ecb.europa.eu/data/datasets/BSI/BSI.M.U2.Y.V.M20.X.1.U2.2300.Z01.E\" rel=\"nofollow noreferrer\">https://data.ecb.europa.eu/data/datasets/BSI/BSI.M.U2.Y.V.M20.X.1.U2.2300.Z01.E</a></li>\n<li><a href=\"https://data.ecb.europa.eu/data/datasets/RTD/RTD.M.JP.Y.M_M2.J\" rel=\"nofollow noreferrer\">https://data.ecb.europa.eu/data/datasets/RTD/RTD.M.JP.Y.M_M2.J</a></li>\n</ol>\n<p>I have never worked with SDMX library before and I am having trouble understanding the documentation. According to my understanding from <a href=\"https://data.ecb.europa.eu/help/api/data\" rel=\"nofollow noreferrer\">this link</a>, I tried to put the following code together but it is not working, I get 404 error.</p>\n<pre><code>resource = 'data'           # The resource for data queries is always 'data'\nflowRef ='BSI'              # Dataflow describing the data that needs to be returned\nkey = 'M.U2.Y.V.M20.X.1.U2.2300.Z01.E'  # series key for M2 euro area\n\n# Define the parameters\nparameters = {\n    'startPeriod': '2010-01-01',  # Start date of the time series\n    'endPeriod': '2024-07-20',  # End of the time series\n    'detail' : 'dataonly' \n}\n\nrequest_url = entrypoint + resource + '/'+ flowRef + '/' + key\nresponse = requests.get(request_url, params=parameters)\nprint(response)\n</code></pre>\n<p>I also tried the following code after seeing - <a href=\"https://stackoverflow.com/questions/67803469/how-to-download-data-from-ecb-using-pandasdmx\">how to download data from ECB using pandaSDMX?</a></p>\n<pre><code>datamessage = sdmx.Request('ECB').data(\n    resource_id='BSI',\n    key='M.U2.Y.V.M20.X.1.U2.2300.Z01.E', \n    params=dict(startPeriod='2010-04-01', endPeriod='2024-07-20'),\n)\n\ndf = sdmx.to_pandas(datamessage)\n</code></pre>\n",
    "is_answered": false,
    "view_count": 103,
    "answer_count": 1
  },
  {
    "title": "In jupyter notebok title of graph not showing.Why no title is there in the graph?",
    "link": "https://stackoverflow.com/questions/78774659/in-jupyter-notebok-title-of-graph-not-showing-why-no-title-is-there-in-the-graph",
    "tags": [
      "python",
      "jupyter-notebook",
      "seaborn",
      "data-science",
      "countplot"
    ],
    "body": "<p><a href=\"https://i.sstatic.net/LhDgj1fd.png\" rel=\"nofollow noreferrer\">this is set of codes i ran</a>\n<a href=\"https://i.sstatic.net/jtCXvZFd.png\" rel=\"nofollow noreferrer\">here is the graph,see everything is fine</a>\n<a href=\"https://i.sstatic.net/9QSnOm4K.png\" rel=\"nofollow noreferrer\">this is the 2nd set of codes i ran, in the imaage u can see there is no title</a></p>\n<p>i was trying to plot a count graph,i wrote the code for title that is plt.title('top 15 cities count')\nNo error is thrown but still no title is produced.\nthe problem is i just ran similar codes before this one and that worked fine</p>\n",
    "is_answered": true,
    "view_count": 52,
    "answer_count": 1
  },
  {
    "title": "Stuck in handling incorrect input data on web app for model training",
    "link": "https://stackoverflow.com/questions/78763624/stuck-in-handling-incorrect-input-data-on-web-app-for-model-training",
    "tags": [
      "python",
      "machine-learning",
      "flask",
      "exception",
      "data-science"
    ],
    "body": "<p>I am trying to add an exception feature in an ML project I am working on, I create a web app which accepts student performance data as a CSV file and then performs different machine learning algorithms and selects and saves the model with the best R2 score, it deletes any previous model if already existing and replaces it with model trained on new data, and then displays the R2 score to the user. The app is working fine with correct data, I tried to build a process to show an error message to the user if the input data is incorrect. I have this use case where in the following portion of the CSV file I deleted one of the column entries in one of the record:</p>\n<pre><code>&quot;gender&quot;,&quot;race_ethnicity&quot;,&quot;parental_level_of_education&quot;,&quot;lunch&quot;,&quot;test_preparation_course&quot;,&quot;math_score&quot;,&quot;reading_score&quot;,&quot;writing_score&quot;\n&quot;female&quot;,&quot;group B&quot;,&quot;bachelor's degree&quot;,&quot;standard&quot;,&quot;none&quot;,&quot;72&quot;,&quot;72&quot;,&quot;74&quot;\n&quot;female&quot;,&quot;group C&quot;,&quot;some college&quot;,&quot;standard&quot;,&quot;completed&quot;,&quot;69&quot;,&quot;90&quot;,&quot;88&quot;\n&quot;female&quot;,&quot;group B&quot;,&quot;master's degree&quot;,&quot;standard&quot;,&quot;none&quot;,&quot;90&quot;,&quot;95&quot;,&quot;93&quot;\n&quot;male&quot;,&quot;group A&quot;,&quot;associate's degree&quot;,&quot;free/reduced&quot;,&quot;none&quot;,&quot;47&quot;,&quot;57&quot;,&quot;44&quot;\n</code></pre>\n<p>Here I changed the second entry, from <code>&quot;female&quot;,&quot;group C&quot;,&quot;some college&quot;,&quot;standard&quot;,&quot;completed&quot;,&quot;69&quot;,&quot;90&quot;,&quot;88&quot;</code> to <code>&quot;female&quot;,&quot;some college&quot;,&quot;standard&quot;,&quot;completed&quot;,&quot;69&quot;,&quot;90&quot;,&quot;88&quot;</code>, to check how it handles the error. Actually, as I share below the log file, it shows that the program was able to create a model, maybe because I used imputer to fix missing values, and thus was able to build a model and show the R2 score in the logs. The issue is, that it is not showing the R2 score, nor any error on the webpage, instead the site stops working and shows error code 400, but in the logs it shows status code 200, and doesn\u2019t show any error in terminal. I am sharing the screenshot of Network tab of developer options, if it may help in figuring out the issue.</p>\n<p><a href=\"https://i.sstatic.net/yrkiv9r0.png\" rel=\"nofollow noreferrer\">Screenshot of crashed web page after submitting incorrect input file</a></p>\n<p>Logs file output:</p>\n<pre><code>[2024-07-18 09:26:07,184] _internal.py:97 _log() werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m\n * Running on all addresses (0.0.0.0)\n * Running on http://127.0.0.1:8080\n * Running on http://172.16.5.4:8080\n[2024-07-18 09:26:07,184] _internal.py:97 _log() werkzeug - INFO - [33mPress CTRL+C to quit[0m\n[2024-07-18 09:26:27,148] _internal.py:97 _log() werkzeug - INFO - 127.0.0.1 - - [18/Jul/2024 09:26:27] &quot;GET / HTTP/1.1&quot; 200 -\n[2024-07-18 09:26:35,995] _internal.py:97 _log() werkzeug - INFO - 127.0.0.1 - - [18/Jul/2024 09:26:35] &quot;GET / HTTP/1.1&quot; 200 -\n[2024-07-18 09:26:42,111] _internal.py:97 _log() werkzeug - INFO - 127.0.0.1 - - [18/Jul/2024 09:26:42] &quot;GET /input HTTP/1.1&quot; 200 -\n[2024-07-18 09:27:04,034] train_pipeline.py:35 delete_and_recreate_model() root - INFO - Saved new raw data as CSV file\n[2024-07-18 09:27:04,034] data_ingestion.py:26 initiate_data_ingestion() root - INFO - Entered data ingestion method or component\n[2024-07-18 09:27:04,041] data_ingestion.py:29 initiate_data_ingestion() root - INFO - Read dataset as df\n[2024-07-18 09:27:04,047] data_ingestion.py:35 initiate_data_ingestion() root - INFO - Train test split initiating\n[2024-07-18 09:27:04,068] data_ingestion.py:42 initiate_data_ingestion() root - INFO - ingestion of data completed\n[2024-07-18 09:27:04,071] data_transformation.py:60 initiate_data_transformation() root - INFO - Read train and test data completed\n[2024-07-18 09:27:04,074] data_transformation.py:76 initiate_data_transformation() root - INFO - numerical features are Index(['reading_score', 'writing_score'], dtype='object') and categorical features are Index(['gender', 'race_ethenicity', 'parental_level_of_education', 'lunch',\n       'test_preparation_course'],\n      dtype='object')\n[2024-07-18 09:27:04,074] data_transformation.py:31 get_data_transformer() root - INFO - numerical columns scaling completed\n[2024-07-18 09:27:04,074] data_transformation.py:40 get_data_transformer() root - INFO - categorical columns logging completed\n[2024-07-18 09:27:04,074] data_transformation.py:81 initiate_data_transformation() root - INFO - applying preprocessing object on train and test df\n[2024-07-18 09:27:04,124] model_trainer.py:32 initiate_model_trainer() root - INFO - Split training and test input data\n[2024-07-18 09:27:23,565] _internal.py:97 _log() werkzeug - INFO - 127.0.0.1 - - [18/Jul/2024 09:27:23] &quot;GET /input HTTP/1.1&quot; 200 -\n[2024-07-18 09:28:06,418] model_trainer.py:99 initiate_model_trainer() root - INFO - Best model found\n[2024-07-18 09:28:06,420] train_pipeline.py:49 delete_and_recreate_model() root - INFO - New R2 score is: 0.8803008999935347\n[2024-07-18 09:28:06,420] application.py:59 input_data() root - INFO - Processing completed. New R2 score: 0.8803008999935347\n[2024-07-18 09:28:06,421] _internal.py:97 _log() werkzeug - INFO - 127.0.0.1 - - [18/Jul/2024 09:28:06] &quot;POST /input HTTP/1.1&quot; 200 -\n\n</code></pre>\n<p>My application.py code:</p>\n<pre><code>from flask import Flask,request,render_template\nimport numpy as np\nimport pandas as pd\nfrom src.exception import CustomException\nimport sys\nfrom src.logger import logging\nfrom sklearn.preprocessing import StandardScaler\nfrom src.pipeline.predict_pipeline import CustomData,PredictPipeline\nfrom src.pipeline.train_pipeline import RetrainWithNewData\n\napplication=Flask(__name__)\napp=application\n\n@app.route('/input', methods=['GET', 'POST'])\ndef input_data():\n    if request.method == 'GET':\n        return render_template('home2.html')\n    else:\n        try:\n            file = request.files['file']\n            \n            # Check if file is present\n            if file.filename == '':\n                return render_template('home2.html', error=&quot;No file selected&quot;)\n            \n            # Check file extension (assuming you want CSV files)\n            if not file.filename.lower().endswith('.csv'):\n                return render_template('home2.html', error=&quot;Invalid file type. Please upload a CSV file.&quot;)\n            \n            retrainPipeline = RetrainWithNewData(file)\n            new_r2_score = retrainPipeline.delete_and_recreate_model()\n\n            logging.info(f&quot;Processing completed. New R2 score: {new_r2_score}&quot;)\n\n            return render_template('home2.html', new_r2_score=new_r2_score)\n\n        except Exception as e:\n            # For unexpected exceptions, you might want to log them and show a generic message\n            logging.error(f&quot;Unexpected error: {str(e)}&quot;)\n            return render_template('home2.html', error=str(e))\nif __name__==&quot;__main__&quot;:\n    app.run(host=&quot;0.0.0.0&quot;,port=8080)\n</code></pre>\n<p>My home2.html code:</p>\n<pre><code>&lt;html&gt;\n&lt;body&gt;\n    &lt;form action=&quot;{{ url_for('input_data')}}&quot; method=&quot;post&quot; enctype=&quot;multipart/form-data&quot;&gt;\n        &lt;h2&gt;Upload Data&lt;/h2&gt;\n        &lt;input type=&quot;file&quot; id=&quot;file&quot; name=&quot;file&quot; required&gt;\n        &lt;br&gt;&lt;br&gt;\n        &lt;input type=&quot;submit&quot; name=&quot;upload_submit&quot; value=&quot;Upload data in CSV format&quot;&gt;\n    &lt;/form&gt;\n\n    {% if error %}\n    &lt;h2 style=&quot;color: red;&quot;&gt;Error: {{ error }}&lt;/h2&gt;\n    {% endif %}\n\n    {% if new_r2_score %}\n    &lt;h2&gt;The new R2 score is {{ new_r2_score }}&lt;/h2&gt;\n    {% endif %}\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>\n<p>My train_pipeline.py code:</p>\n<pre><code>import sys\nimport os\nimport shutil\nfrom src.exception import CustomException\nfrom src.logger import logging\nfrom src.components.data_ingestion import DataIngestion, DataIngestionConfig\nfrom src.components.data_transformation import DataTransformation, DataTransformationConfig\nfrom src.components.model_trainer import ModelTrainer, ModelTrainerConfig\n\nclass RetrainWithNewData():\n    def __init__(self, file):\n        self.file = file\n    \n    def delete_and_recreate_model(self):\n        try:\n            new_data = self.file\n            \n            new_raw_data_path = os.path.join(os.getcwd(), &quot;notebook/data&quot;)\n            artifacts_path = os.path.join(os.getcwd(), &quot;artifacts&quot;)\n            \n            # Remove existing directories if they exist\n            if os.path.exists(new_raw_data_path):\n                shutil.rmtree(new_raw_data_path)\n            \n            if os.path.exists(artifacts_path):\n                shutil.rmtree(artifacts_path)\n            \n            # Create necessary directories\n            os.makedirs(new_raw_data_path, exist_ok=True)\n            os.makedirs(artifacts_path, exist_ok=True)\n            \n            # Save the new data to a specific file path\n            new_raw_data_file_path = os.path.join(new_raw_data_path, &quot;stud.csv&quot;)\n            new_data.save(new_raw_data_file_path)\n            logging.info(&quot;Saved new raw data as CSV file&quot;)\n            \n            # Start the data ingestion process\n            obj = DataIngestion()\n            train_data, test_data = obj.initiate_data_ingestion()\n            \n            # Transform the data\n            data_transformation = DataTransformation()\n            train_arr, test_arr, _ = data_transformation.initiate_data_transformation(train_data, test_data)\n            \n            # Train the model\n            modelTrainer = ModelTrainer()\n            new_r2_score = float(modelTrainer.initiate_model_trainer(train_arr, test_arr))\n            \n            logging.info(f&quot;New R2 score is: {new_r2_score}&quot;)\n\n            return new_r2_score\n        \n        except Exception as e:\n            raise CustomException(e, sys)\n\n</code></pre>\n<p>I would be really grateful for your help.</p>\n<p>I expected an output R2 score to be displayed, or an error message to be displayed to the user about incorrect file input.</p>\n",
    "is_answered": false,
    "view_count": 99,
    "answer_count": 1
  },
  {
    "title": "Trying to scavanage data using Selenium but save button isn&#39;t working",
    "link": "https://stackoverflow.com/questions/78757629/trying-to-scavanage-data-using-selenium-but-save-button-isnt-working",
    "tags": [
      "selenium-webdriver",
      "data-science",
      "selenium-ide",
      "data-science-experience"
    ],
    "body": "<p>I am trying to save the booth addresses using Selenium for machine learning algorithms, but the save button is not getting clicked:</p>\n<p><img src=\"https://i.sstatic.net/TpQzrGmJ.png\" alt=\"the floppy sign\" /></p>\n<p>I tried all types of selectors, different methods, JavaScript, and everything else, but it still doesn't work.</p>\n<p>Here is the last code I tried.</p>\n<pre class=\"lang-py prettyprint-override\"><code>import time\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import Select\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.support.ui import WebDriverWait\n\nclass Download():\n    def setup_method(self, method):\n        self.driver = webdriver.Edge()\n        self.vars = {}\n        self.wait = WebDriverWait(self.driver, 10)  # Adjust timeout as needed\n    \n    def teardown_method(self, method):\n        self.driver.quit()\n    \n    def wait_for_window(self, timeout = 2):\n        time.sleep(timeout / 1000)\n        wh_now = self.driver.window_handles\n        wh_then = self.vars[&quot;window_handles&quot;]\n        if len(wh_now) &gt; len(wh_then):\n            return set(wh_now).difference(set(wh_then)).pop()\n    \n    def download_all(self):\n        self.driver.get(&quot;https://ceoelection.maharashtra.gov.in/SearchInfo/ListPSs.aspx&quot;)\n        self.driver.set_window_size(1248, 835)\n        \n        # Get all districts\n        district_select = Select(self.driver.find_element(By.ID, &quot;ctl00_content_DistrictList&quot;))\n        districts = [option.text for option in district_select.options if option.text != '-- Select District --']\n        \n        for district in districts:\n            # Select district\n            district_select.select_by_visible_text(district)\n            \n            # Wait for assembly dropdown to be visible and enabled\n            self.wait.until(EC.visibility_of_element_located((By.ID, &quot;ctl00_content_AssemblyList&quot;)))\n            assembly_select = Select(self.driver.find_element(By.ID, &quot;ctl00_content_AssemblyList&quot;))\n            \n            # Get all assembly constituencies for the selected district\n            assemblies = [option.text for option in assembly_select.options if option.text != '-- Select Assembly --']\n            \n            for assembly in assemblies:\n                # Select assembly constituency\n                assembly_select.select_by_visible_text(assembly)\n                \n                # Wait for language selection to be clickable\n                self.wait.until(EC.element_to_be_clickable((By.ID, &quot;ctl00_content_LangList_1&quot;)))\n                \n                # Select the language (assuming &quot;English&quot; is the language to be selected)\n                self.driver.find_element(By.ID, &quot;ctl00_content_LangList_1&quot;).click()\n                \n                \n                # Wait for generate report button to be clickable\n                \n                try:\n                   ** element = self.driver.find_element(By.ID, &quot;ctl00_content_ReportViewer1_ctl05_ctl04_ctl00_ButtonLink&quot;)\n                    self.driver.execute_script(&quot;arguments[0].click();&quot;, element)** \n                #also tried\n                #self.driver.find_element(By.x, &quot;ctl00_content_ReportViewer1_ctl05_ctl04_ctl00_ButtonImg&quot;).click()\n\n\n                except Exception as e:\n                    print(f&quot;Error clicking export dropdown: {e}&quot;)\n\n                # # Wait for the Excel download link to be visible\n                # self.wait.until(EC.visibility_of_element_located((By.LINK_TEXT, &quot;Excel&quot;)))\n                \n                # Click to download the Excel file\n                time.sleep(5)\n                self.vars[&quot;window_handles&quot;] = self.driver.window_handles\n                self.driver.find_element(By.LINK_TEXT, &quot;Excel&quot;).click()\n\n                self.vars[&quot;win2902&quot;] = self.wait_for_window(2000)\n                self.vars[&quot;root&quot;] = self.driver.current_window_handle\n                self.driver.switch_to.window(self.vars[&quot;win2902&quot;])\n                self.driver.close()\n                self.driver.switch_to.window(self.vars[&quot;root&quot;])\n\n                # Optional: wait for a bit before proceeding to the next download\n                time.sleep(1)\n\nif __name__ == &quot;__main__&quot;:\n    dl = Download()\n    dl.setup_method(None)\n    try:\n        dl.download_all()\n    finally:\n        dl.teardown_method(None)\n\n</code></pre>\n<p>I want to download them as Excel recursively.</p>\n<p>I want the Excel file to get downloaded. For that, the save icon should be clicked.</p>\n<p>Please help, stuck for two days.</p>\n",
    "is_answered": false,
    "view_count": 65,
    "answer_count": 1
  },
  {
    "title": "TaskRun failed to finish due to an error for Coretex BioInformatics workflow",
    "link": "https://stackoverflow.com/questions/78750613/taskrun-failed-to-finish-due-to-an-error-for-coretex-bioinformatics-workflow",
    "tags": [
      "r",
      "data-science",
      "bioinformatics",
      "mlops"
    ],
    "body": "<p>After starting bioinformatics workflow in Coretex, I am getting the following message even though data seems to be in order:\n&quot;<code>Failed to determine which column contains sampleIDs/names..</code>.&quot; and then the list of available names, but I am using one from the list.</p>\n<p>I am trying to run a microbiome sequencing task in Coretex, and I have used standard microbiome sequencing data in <code>.fastq.gz</code> format. Run should have been successful but it is failing every time.</p>\n<p>I've worked with this R code for uploading metadata:</p>\n<pre><code>loadMetadata &lt;- function(metadataSample) {\nmetadata_csv_path &lt;- builtins$str(\n    metadataSample$joinPath(&quot;metadata.csv&quot;)\n)\n\nif (file.exists(metadata_csv_path)) {\n    # Default SampleSheet.csv format\n    metadata &lt;- read.table(\n        metadata_csv_path,\n        sep = &quot;,&quot;,\n        header = TRUE,\n        check.names = TRUE\n    )\n} else {\n    # Format accepted by qiime2\n    metadata_tsv_path &lt;- builtins$str(\n        metadataSample$joinPath(&quot;metadata.tsv&quot;)\n    )\n\n    if (!file.exists(metadata_tsv_path)) {\n        stop(&quot;Metadata file not found&quot;)\n    }\n\n    metadata &lt;- read.table(\n        metadata_tsv_path,\n        sep = &quot;\\t&quot;,\n        header = TRUE,\n        check.names = TRUE\n    )\n\n    # qiime has 1 extra row after header which contains types\n    metadata &lt;- metadata[-1,]\n}\n\n# Remove leading and trailing whitespace\ncolnames(metadata) &lt;- lapply(colnames(metadata), trimws)\n\nstringColumns &lt;- names(metadata)[vapply(metadata, is.character, logical(1))]\nmetadata[, stringColumns] &lt;- lapply(metadata[, stringColumns], trimws)\n\nsampleIdColumn &lt;- getSampleIdColumnName(metadata)\nprint(paste(&quot;Matched metadata sample ID/name column to&quot;, sampleIdColumn))\n\nprint(&quot;Renaming metadata sample ID/name column to \\&quot;sampleId\\&quot;&quot;)\nnames(metadata)[names(metadata) == sampleIdColumn] &lt;- &quot;sampleId&quot;\n\nprint(&quot;Metadata&quot;)\nprint(colnames(metadata))\nprint(head(metadata))\n\nprint(metadata$sampleId)\n\n# assign the names of samples (01Sat1...) to metadata rows instead of 1,2,3...\nrow.names(metadata) &lt;- metadata$sampleId\nmetadata$sampleId &lt;- as.factor(metadata$sampleId)\n\nreturn(metadata)\n</code></pre>\n<p>}</p>\n",
    "is_answered": true,
    "view_count": 39,
    "answer_count": 1
  },
  {
    "title": "How to optimise hyperparameterss for RandomForestClassifier in Python for large datasets?",
    "link": "https://stackoverflow.com/questions/78731369/how-to-optimise-hyperparameterss-for-randomforestclassifier-in-python-for-large",
    "tags": [
      "scikit-learn",
      "dataset",
      "data-science",
      "random-forest"
    ],
    "body": "<blockquote>\n<p>I'm just working on this problem where I thought <code>RandomForestClassifier</code> from scikit-learn would be better solution for a large dataset. Only after trying with it for this, I found it to be not accurate. The model is either overfitting or underperforming, and sometimes the training time goes on forever.</p>\n</blockquote>\n<p>500000 samples and 50 features. My goal si to classify data into 3 categories.</p>\n<pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score\n\n# Load dataset\nX = ...  # Features\ny = ...  # Labels\n\n# Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize the classifier\nrf = RandomForestClassifier(random_state=42)\n\n# Define the parameter grid\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    'bootstrap': [True, False]\n}\n\n# Grid search\ngrid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\ngrid_search.fit(X_train, y_train)\n\n# Best parameters and model\nbest_params = grid_search.best_params_\nbest_rf = grid_search.best_estimator_\n\n# Predictions and accuracy\ny_pred = best_rf.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint(f&quot;Best Parameters: {best_params}&quot;)\nprint(f&quot;Accuracy: {accuracy}&quot;)\n</code></pre>\n<p>I tried many methods like manual hyperparameter tuning, grid search for a systematic approach and randomised search giving me inconsistent results.</p>\n<p>I want help in improving all these areas. Thanks in advance.</p>\n",
    "is_answered": true,
    "view_count": 48,
    "answer_count": 1
  },
  {
    "title": "Issue with H3: Uber\u2019s Hexagonal Hierarchical Spatial Index",
    "link": "https://stackoverflow.com/questions/78719884/issue-with-h3-uber-s-hexagonal-hierarchical-spatial-index",
    "tags": [
      "database",
      "data-science",
      "geospatial",
      "uber-api",
      "h3"
    ],
    "body": "<p>Currently, I am using the Uber H3 index library for my geospatial analysis. The version of the library is h3-3.7.7. If the version changes tomorrow, the corresponding H3 index for the given latitude and longitude might also change, affecting my entire dataset. How can I solve this issue?</p>\n",
    "is_answered": false,
    "view_count": 216,
    "answer_count": 1
  },
  {
    "title": "How to combine 3 annotated datasets into one file for further processing?",
    "link": "https://stackoverflow.com/questions/78705509/how-to-combine-3-annotated-datasets-into-one-file-for-further-processing",
    "tags": [
      "python",
      "sql",
      "data-science",
      "data-annotations",
      "data-preprocessing"
    ],
    "body": "<p>I have a dataset annotated by three people, so now I have three files. This dataset is about tweets annotation. How can I combine this dataset into one file for further processing.  The data set is an excel file and contains five columns:</p>\n<pre><code>name tweet id PPP,PTI,PMLN, full text\n</code></pre>\n<p>The annotations are done on <code>pti,pmln</code> each column of annotation is 0.1,-1 value</p>\n<p>I want to export a new file combing through a voting classifier or something like this</p>\n",
    "is_answered": true,
    "view_count": 80,
    "answer_count": 1
  },
  {
    "title": "Selenium doesn&#39;t find the popup button",
    "link": "https://stackoverflow.com/questions/78689186/selenium-doesnt-find-the-popup-button",
    "tags": [
      "python",
      "selenium-webdriver",
      "data-science",
      "html-parsing"
    ],
    "body": "<p>I have this webpage (<a href=\"https://goldapple.ru/\" rel=\"nofollow noreferrer\">https://goldapple.ru/</a>) on which I want to parse some data about cosmetics. However, when I open the webpage, the popup button appears, and I want to click the left &quot;\u0414\u0430, \u0432\u0435\u0440\u043d\u043e&quot;. Unfortunately, the TimeoutException(message, screen, stacktrace) error appears, so the problem is that the machine doesn't see the button.</p>\n<p>The webpage HTML code:</p>\n<pre class=\"lang-html prettyprint-override\"><code>&lt;button type=&quot;button&quot; class=&quot;MMwlC KQN8E yVRvi I1E8J PrKjg Rt0VH&quot;&gt;&lt;span class=&quot;_1MvHE&quot;&gt;&lt;span class=&quot;nOERC&quot;&gt;\n      \u0414\u0430, \u0432\u0435\u0440\u043d\u043e\n    &lt;/span&gt; &lt;span class=&quot;SdR9G&quot; style=&quot;display: none;&quot;&gt;&lt;div class=&quot;o8w4X&quot;&gt;&lt;div class=&quot;-tctp&quot;&gt;&lt;/div&gt; &lt;div class=&quot;-tctp&quot;&gt;&lt;/div&gt; &lt;div class=&quot;-tctp&quot;&gt;&lt;/div&gt; &lt;!----&gt;&lt;/div&gt;&lt;/span&gt;&lt;/span&gt;&lt;/button&gt;\n</code></pre>\n<p>My code's like that:</p>\n<hr />\n<pre><code>#import some dependencies\nfrom selenium.webdriver import Chrome\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.common.by import By\nfrom time import sleep\n</code></pre>\n<pre><code>#actual code\nbrowser = Chrome()\nurl = &quot;https://goldapple.ru/&quot;\n\nbrowser.get(url)\n\nwait = WebDriverWait(browser, 10)\n\nwait.until(\n    EC.element_to_be_clickable((By.XPATH, &quot;//*[@id='__layout']/div/div[5]/div[2]/div[1]/div/div[2]/button[1]&quot;))\n).click()\n</code></pre>\n<p>Some facts you need to know imho:</p>\n<hr />\n<ol>\n<li>I used the WebDriverWait to get some time for a popup button to appear</li>\n<li>I used the element_to_be_clickable condition</li>\n<li>XPATH was used as a reference to the button, so there should be no problem with the address</li>\n<li>There's no shadow root on a webpage</li>\n</ol>\n<p>P.S. Some motivation:\nIf someone solves the problem, it would be considered (at least by me) as the Selenium breakthrough</p>\n",
    "is_answered": false,
    "view_count": 103,
    "answer_count": 1
  },
  {
    "title": "LLama3 model fine tunning issue",
    "link": "https://stackoverflow.com/questions/78687183/llama3-model-fine-tunning-issue",
    "tags": [
      "python-3.x",
      "data-science",
      "llama"
    ],
    "body": "<p>I trained the llama3 model with the following code for a question and answer dataset (around 50 Q&amp;A), but the retrained model does not give exact answers as per the trained dataset; either it gives a mix answer (combining multiple trained questions answers into a single) or it only gives exact answer to  that question, when I ask the model to generate new content based on the question answer, it always gives the same answer (overfitting issue here). If the training loss is minimal, the model is overfitting; attempting to reduce the hyperparameter values yields incorrect results. I want the model to provide precise responses if the question is the same, and if the new question is about generating new content based on the answer to the trained question, the model must create new content. Though RAG was the best option for this question and answer(only 50 q&amp;a's)task because the responses are on the same topic and have similar content, similarity search extracts many questions answers into context.  Retrieval is failing in that option.</p>\n<p>Code:</p>\n<pre><code>\nfrom unsloth import FastLanguageModel\nimport torch\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\n# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\nfourbit_models = [\n    &quot;unsloth/mistral-7b-v0.3-bnb-4bit&quot;,      # New Mistral v3 2x faster!\n    &quot;unsloth/mistral-7b-instruct-v0.3-bnb-4bit&quot;,\n    &quot;unsloth/llama-3-8b-bnb-4bit&quot;,           # Llama-3 15 trillion tokens model 2x faster!\n    &quot;unsloth/llama-3-8b-Instruct-bnb-4bit&quot;,\n    &quot;unsloth/llama-3-70b-bnb-4bit&quot;,\n    &quot;unsloth/Phi-3-mini-4k-instruct&quot;,        # Phi-3 2x faster!\n    &quot;unsloth/Phi-3-medium-4k-instruct&quot;,\n    &quot;unsloth/mistral-7b-bnb-4bit&quot;,\n    &quot;unsloth/gemma-7b-bnb-4bit&quot;,             # Gemma 2.2x faster!\n] # More models at https://huggingface.co/unsloth\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = &quot;unsloth/llama-3-8b-bnb-4bit&quot;,\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    # token = &quot;hf_...&quot;, # use one if using gated models like meta-llama/Llama-2-7b-hf\n)\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 16, # Choose any number &gt; 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [&quot;q_proj&quot;, &quot;k_proj&quot;, &quot;v_proj&quot;, &quot;o_proj&quot;,\n                      &quot;gate_proj&quot;, &quot;up_proj&quot;, &quot;down_proj&quot;,],\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = &quot;none&quot;,    # Supports any, but = &quot;none&quot; is optimized\n    # [NEW] &quot;unsloth&quot; uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = &quot;unsloth&quot;, # True or &quot;unsloth&quot; for very long context\n    random_state = 3407,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n  )\n  alpaca_prompt = &quot;&quot;&quot;Below is a question with an answer that provides a clear explanation.\n\n   ### Question:\n   {}\n\n   ### Response:\n   {}\n   &quot;&quot;&quot;\n\n   EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n\n   def formatting_prompts_func(examples):\n    questions = examples[&quot;Question&quot;]\n    answers = examples[&quot;Answer&quot;]\n    texts = []\n    for question, answer in zip(questions, answers):\n        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n        text = alpaca_prompt.format(question, answer) + EOS_TOKEN\n        texts.append(text)\n    return {&quot;text&quot;: texts}\n\n   from datasets import load_dataset\n   dataset = load_dataset(&quot;csv&quot;, data_files=&quot;training-data.csv&quot;)\n   dataset = dataset.map(formatting_prompts_func, batched=True)\n   from trl import SFTTrainer\n   from transformers import TrainingArguments\n   from unsloth import is_bfloat16_supported\n\n   trainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset['train'],\n    dataset_text_field = &quot;text&quot;,\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n    packing = False, # Can make training 5x faster for short sequences.\n    args = TrainingArguments(\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 4,\n        warmup_steps = 5,\n        max_steps = 90,\n        learning_rate = 2e-4,\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 1,\n        optim = &quot;adamw_8bit&quot;,\n        weight_decay = 0.01,\n        lr_scheduler_type = &quot;linear&quot;,\n        seed = 3407,\n        output_dir = \n\ntype here\n\n&quot;outputs&quot;,\n    ),\n   )\n    trainer_stats = trainer.train()\n</code></pre>\n",
    "is_answered": false,
    "view_count": 291,
    "answer_count": 0
  },
  {
    "title": "How to resolve TypeError: &#39;numpy.float64&#39; object is not callable when calculating r2_score?",
    "link": "https://stackoverflow.com/questions/78685188/how-to-resolve-typeerror-numpy-float64-object-is-not-callable-when-calculatin",
    "tags": [
      "python",
      "scikit-learn",
      "regression",
      "data-science"
    ],
    "body": "<p>I am encountering the following error:</p>\n<pre><code>TypeError: 'numpy.float64' object is not callable\n</code></pre>\n<p>when I try to calculate the R2 score using scikit-learn's <code>r2_score</code> function. Below is the relevant portion of my code:</p>\n<pre><code>from sklearn.metrics import r2_score, mean_squared_error\n\n# Assuming Y is the actual values and ypipe is the predicted values\nprint('MSE for multi-variable polynomial pipeline is: ', mean_squared_error(Y, ypipe))\nprint('R^2 for multi-variable polynomial pipeline is: ', r2_score(Y, ypipe))\n\n</code></pre>\n<p>However, I receive the following error:</p>\n<pre><code>TypeError: 'numpy.float64' object is not callable\n</code></pre>\n<p><a href=\"https://i.sstatic.net/H3FHmCTO.png\" rel=\"nofollow noreferrer\">enter image description here</a></p>\n<p>This is also appearing when calculating r2_score for polynomial regression</p>\n<p><a href=\"https://i.sstatic.net/H3PbnPvO.png\" rel=\"nofollow noreferrer\">enter image description here</a></p>\n",
    "is_answered": false,
    "view_count": 129,
    "answer_count": 0
  },
  {
    "title": "When I use Filtered_tg in R. The console show this error :- Error in arrange(filtered_tg, len) : could not find function &quot;arrange&quot;",
    "link": "https://stackoverflow.com/questions/76859900/when-i-use-filtered-tg-in-r-the-console-show-this-error-error-in-arrangefil",
    "tags": [
      "google-analytics",
      "data-science",
      "data-analysis"
    ],
    "body": "<p><a href=\"https://i.sstatic.net/3nmgt.png\" rel=\"nofollow noreferrer\">Error show in Rstdio I try to fix it but and watch video in youtube but I can't found where is the problem</a></p>\n<p>I code this:-</p>\n<pre><code>data(&quot;ToothGrowth&quot;)\nView(ToothGrowth)\n\nfiltered_tg &lt;- filter(ToothGrowth,dose==0.5)\nView(filtered_tg)\narrange(filtered_tg, len)\narrange(filter(ToothGrowth,dose==0.5),len)\n</code></pre>\n<p>output :-</p>\n<pre><code>View(filtered_tg)\nError in View : object 'filtered_tg' not found\n&gt; View(filter_tg)\nError in View : object 'filter_tg' not found\n&gt; View(filtered_tg)\nError in View : object 'filtered_tg' not found\n&gt; View(filtered_tg)\nError in View : object 'filtered_tg' not found\n&gt; arrange(filtered_tg, len)\n</code></pre>\n",
    "is_answered": false,
    "view_count": 91,
    "answer_count": 1
  },
  {
    "title": "How can we map catagorical codes in a dataframe back to the original data points in the original dataframe?",
    "link": "https://stackoverflow.com/questions/76844343/how-can-we-map-catagorical-codes-in-a-dataframe-back-to-the-original-data-points",
    "tags": [
      "python",
      "python-3.x",
      "dataframe",
      "machine-learning",
      "data-science"
    ],
    "body": "<p>I have a simple dataframe that looks like this.</p>\n<pre><code>import pandas as pd\n \n# Intitialise data of lists\ndata = [{'Year': 2020, 'Airport':2000, 'Casino':5000, 'Stadium':9000, 'Size':'Small'}, \n       {'Year': 2019, 'Airport':3000, 'Casino':4000, 'Stadium':12000, 'Size':'Medium'},\n       {'Year': 2018, 'Airport':5000, 'Casino':9000, 'Stadium':10000, 'Size':'Medium'},\n       {'Year': 2017, 'Airport':5000, 'Casino':10000, 'Stadium':15000, 'Size':'Large'}]\ndf = pd.DataFrame(data)\n\n\ndf = df.set_index(['Year'])\ndf\n\ndf_fin = pd.DataFrame({col: df[col].astype('category').cat.codes for col in df}, index=df.index)\nprint(df_fin.columns)\ndf_fin\n</code></pre>\n<p><a href=\"https://i.sstatic.net/5GUr5.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/5GUr5.png\" alt=\"enter image description here\" /></a></p>\n<p>Then, I convert everything to categorical codes, like this.</p>\n<pre><code>df_fin = pd.DataFrame({col: df[col].astype('category').cat.codes for col in df}, index=df.index)\nprint(df_fin.columns)\ndf_fin\n</code></pre>\n<p><a href=\"https://i.sstatic.net/d6MHG.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/d6MHG.png\" alt=\"enter image description here\" /></a></p>\n<p>Then, I am doing a basic basic classification experiment, like this.</p>\n<pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_moons\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\n\nX = df_fin[['Airport', 'Casino', 'Stadium', ]]\ny = df_fin['Size']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nclf = AdaBoostClassifier(n_estimators=100)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\naccuracy_score(y_test, y_pred)\n</code></pre>\n<p>Finally, if I want to make a prediction, I can do this.</p>\n<pre><code>print(clf.predict([[2, 3, 3]]))\n</code></pre>\n<p>The result is '0' for size, which is what I would expect for 2017. However, I don't want to use the categorical codes, I want to use the original records from the original 'df'.</p>\n<p>How can I make a prediction like this?</p>\n<pre><code>print(clf.predict([[5000,10000,15000]]))\n</code></pre>\n<p>So I can get a prediction of 'Large'. Somehow I need to map the categorical codes back to the records in the original 'df'. How can I do this?</p>\n",
    "is_answered": true,
    "view_count": 58,
    "answer_count": 1
  },
  {
    "title": "I&#39;m getting an import error with ydata-profiling-4.4.0: `BaseSettings` has been moved to the `pydantic-settings` package",
    "link": "https://stackoverflow.com/questions/76844229/im-getting-an-import-error-with-ydata-profiling-4-4-0-basesettings-has-been",
    "tags": [
      "python",
      "pandas",
      "machine-learning",
      "data-science"
    ],
    "body": "<p>I know that Pydantic V2 introduced new things which make it incompatible with V1, so I switched from pandas_profiling to ydata_profiling. Because of that, I had to switch versions of the dependencies, but now I'm getting a complex error which makes it seem like there's now way to resolve all three of my resulting errors at once:</p>\n<pre><code>pydantic-settings 2.0.2 requires pydantic&gt;=2.0.1, but you have pydantic 1.8.1 which is incompatible.\nydata-profiling 4.4.0 requires pydantic&lt;2,&gt;=1.8.1, but you have pydantic 2.1.1 which is incompatible.\n</code></pre>\n<p>Is there anyway to make this code work using ydata or should I switch over to a different library. Currently on Windows 11 and python version 3.11.0.</p>\n<pre><code>from ydata_profiling import ProfileReport\n\nchoice = st.radio(&quot;Navigation&quot;, [&quot;Upload&quot;, &quot;Profiling&quot;, &quot;ML&quot;, &quot;Download&quot;, &quot;Predictions&quot;])\n\nif choice == &quot;Profiling&quot;:\n    profile_report = ProfileReport(df, title=&quot;Profiling Report&quot;)\n    st_profile_report(profile_report)\n#Imported the ProfileReport function from ydata_profiling and resulted in the error previously showed.\n</code></pre>\n",
    "is_answered": false,
    "view_count": 989,
    "answer_count": 1
  },
  {
    "title": "Deploy chatbot to AzureFunction",
    "link": "https://stackoverflow.com/questions/76836255/deploy-chatbot-to-azurefunction",
    "tags": [
      "python",
      "azure",
      "azure-functions",
      "data-science",
      "chatbot"
    ],
    "body": "<p>Im trying to deploy Chatbot to Azure Function.</p>\n<p>Im using vs code to deploy to code and also debugging with Azure Tools and Azurite. Locally everything is working.\nAfter deploying a got this error:</p>\n<pre class=\"lang-cs prettyprint-override\"><code> &quot; import duckdb ImportError: generic_type: type &quot;ExplainType&quot; is already registered&quot;\n</code></pre>\n<p>So i tried to remove the duckdb, but when i remove the duckdb library i got this error:</p>\n<pre class=\"lang-cs prettyprint-override\"><code>&quot;import duckdb ImportError: initialization failed&quot;\n</code></pre>\n<p>It is a part of code where langchain should parse docs to create index.</p>\n<p>Python 3.9.0 and these libraries</p>\n<ul>\n<li>azure-functions</li>\n<li>langchain</li>\n<li>duckdb</li>\n<li>chromadb==0.3.29</li>\n<li>pandas</li>\n<li>openai</li>\n<li>tiktoken</li>\n<li>unstructured</li>\n<li>python-magic</li>\n<li>tabulate</li>\n<li>pdf2image</li>\n<li>pytesseract&amp;\nLocal: WIN 10 and venv</li>\n</ul>\n<p>Thx for advice and help</p>\n<p>Tried multiple version of libraries.</p>\n",
    "is_answered": false,
    "view_count": 313,
    "answer_count": 1
  },
  {
    "title": "Question about the graphs of marginal effect in interaction terms and its hypothesis test in ggpredict()",
    "link": "https://stackoverflow.com/questions/76833379/question-about-the-graphs-of-marginal-effect-in-interaction-terms-and-its-hypoth",
    "tags": [
      "r",
      "data-science",
      "visualization",
      "margin",
      "interaction"
    ],
    "body": "<p>I met a problem when trying to visualize the marginal effect of my model.</p>\n<p>My code is as follows:</p>\n<pre><code>fit7am=svyglm(formula=visa~age+male+pregnant+foreigner+fujian+migrant+agent+disadveth+ifoplive+otherpaidtrip+geotravex_oecd+ocpr0+lsal+satsal+educyear+schpc9foreig+lwealthwosqm+like+married+numsib+jobyear+chimin+parret+famtiegravsponsor+conf+offwhite+offm+geotravex_educyear + geotravex_educyear+ age*chimin+ male_chimin+ age_male+ married*age+ married_chimin+ married_male+ parret_numsib+ parret_age+parret_chimin,\n             design=survey_design,\n             family=binomial(),\n             rescale=TRUE)\n\nlibrary(ggeffects)\nresult &lt;- ggpredict(fit7am, c(&quot;age&quot;, &quot;chimin&quot;))\nhypothesis_test(result)\n</code></pre>\n<p><a href=\"https://i.sstatic.net/9i7Kb.png\" rel=\"nofollow noreferrer\">result for hypothesis_test()</a></p>\n<p>(chimin here is a binary variable to categorize whether a person has a minor child)</p>\n<p>I wonder why the reported contrast (and its p-value) here is different from the coefficient before the age*chimin in my regression model. Shouldn\u2019t the slope difference between group chimin=0 &amp;1 for predicting the probability of getting visa be equivalent to the coefficient before their interaction term in the regression, holding all other variables equal.</p>\n<p>Am I understanding this graph and test correctly? Can we still use it for reporting our results?</p>\n",
    "is_answered": true,
    "view_count": 99,
    "answer_count": 1
  },
  {
    "title": "in python pandas data frame shift values from Category to Rating, Rating to Reviews, ............ ,Current Ver to Android Ver",
    "link": "https://stackoverflow.com/questions/76828158/in-python-pandas-data-frame-shift-values-from-category-to-rating-rating-to-revi",
    "tags": [
      "python",
      "pandas",
      "dataframe",
      "data-science"
    ],
    "body": "<p>This is my main data:\n<img src=\"https://i.sstatic.net/hqDEa.png\" alt=\"This is my main data\" /></p>\n<p>I need to change this line of data:\n<img src=\"https://i.sstatic.net/sXDCF.png\" alt=\"I need to change this line of data\" /></p>\n<p>This is my output:\n<img src=\"https://i.sstatic.net/Ih6rb.png\" alt=\"I got output like this\" /></p>\n<p>In python pandas data frame shift values from Category to Rating, Rating to Reviews, etc. Current Ver to Android Ver. App column should be constant.\nI tried a lot. But its not giving me the proper output. Please help me</p>\n",
    "is_answered": true,
    "view_count": 33,
    "answer_count": 1
  },
  {
    "title": "Microsoft Graph api connection",
    "link": "https://stackoverflow.com/questions/76826329/microsoft-graph-api-connection",
    "tags": [
      "outlook",
      "microsoft-graph-api",
      "data-science"
    ],
    "body": "<p>I am trying to get the email details from Outlook emails using Python. we got the access token, But While trying to get the email details, we are facing different issues like OrganizationGuidNotfound, and MailboxNotEnabledForRESTAPI.</p>\n<p>we tried to get email details using Postman also but the same issues. Can anyone share some insights on how to overcome this issue?</p>\n",
    "is_answered": true,
    "view_count": 117,
    "answer_count": 1
  },
  {
    "title": "How to change a string value with comma (343,543) to int in a dataframe with Python",
    "link": "https://stackoverflow.com/questions/76824984/how-to-change-a-string-value-with-comma-343-543-to-int-in-a-dataframe-with-pyt",
    "tags": [
      "python",
      "pandas",
      "dataframe",
      "data-science",
      "data-analysis"
    ],
    "body": "<p>I have a CVS file <a href=\"https://en.wikipedia.org/wiki/IMDb\" rel=\"nofollow noreferrer\">IMDb</a> list. The number of votes is written like <em>345,545</em>, and Python sees this like a string. I want to change this to a number value for using operations like &lt;,+,% and I want to add those values in a new column.</p>\n<pre><code>def change_int(x):\n    y = x.split(&quot;,&quot;)\n    z = int(y[0] + y[1])\n    return z\n\ndf[&quot;imdbVotes2&quot;] = df.imdbVotes.apply(change_int(df[&quot;imdbVotes&quot;]))\n</code></pre>\n<p>I tried to use a function like this.</p>\n<p>I expect:</p>\n<pre class=\"lang-none prettyprint-override\"><code>0    343,564    343564\n1    676,565    676565\n</code></pre>\n",
    "is_answered": true,
    "view_count": 86,
    "answer_count": 3
  },
  {
    "title": "Is there a way to generate a completely new text column for a pandas dataframe?",
    "link": "https://stackoverflow.com/questions/76824269/is-there-a-way-to-generate-a-completely-new-text-column-for-a-pandas-dataframe",
    "tags": [
      "python",
      "pandas",
      "nlp",
      "data-science",
      "huggingface-transformers"
    ],
    "body": "<p>I have a pandas dataframe that contains multiple features such as age, gender , many symptoms with the values 0 or 1 indicating if the patient has that particular symptom or not and the target being his final diagnosis. I want to add a new column being the patient describing his symptoms based on the other features i have. Fo eg, if the features fever and cough have the values 1 i need the description to be &quot;I have been experiencing a fever and severe coughs&quot;. Is there a way to do this without having to do it manually ? Because I have over 50 000 rows in my dataset.\nAny help or suggestion would be much appreciated ! Thank you !</p>\n<p>I tried using transformers's gpt-2 but it keeps giving me an error so another way would be much appreciated.</p>\n",
    "is_answered": false,
    "view_count": 143,
    "answer_count": 1
  },
  {
    "title": "How is this plot plotted?",
    "link": "https://stackoverflow.com/questions/76813915/how-is-this-plot-plotted",
    "tags": [
      "python",
      "matplotlib",
      "plot",
      "data-science"
    ],
    "body": "<p>I know about TPR values, FPR values, roc curve and the associated auc score. The roc curve plots fpr and tpr. I use python sklearn library for all these. But I came across this plot which I could not understand. For different thresholds we have different fpr and tpr and we plot them to get roc curve and the associated auc score. But in this plot, I see curves generated for different desired auc score like 0.01, 0.001 and 0.0001. How this is done? Did I correctly describe the plot?</p>\n<p><img src=\"https://i.sstatic.net/CayXB.png\" alt=\"AUC plot\" /></p>\n<p>Write me some suggestions or codes using sklearn to do so. Detection performance is described as &quot; The area under the ROC curve (AUC) as a single continuous measure for the detection performance that yields a minimal and maximal value of 0.0 and 1.0, respectively.&quot;</p>\n<pre><code>import numpy as np\n\n# Sample TPR and FPR values for different thresholds (replace this with your data)\ntpr_values = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\nfpr_values = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n\n# Ensure that TPR and FPR values are in the correct order (increasing FPR)\ntpr_values = np.flip(tpr_values)\nfpr_values = np.flip(fpr_values)\n\n# Associated AUC score (replace this with your calculated AUC)\noriginal_auc = 0.75\n\n# Your desired AUC score\ndesired_auc = 0.001  # Replace this with your desired AUC value\n\n# Function to interpolate TPR and FPR values for the desired AUC at each threshold\ndef interpolate_tpr_fpr_for_auc(desired_auc, tpr_values, fpr_values, original_auc):\n    interpolated_tpr = []\n    interpolated_fpr = []\n\n    # Interpolate at each threshold to get TPR and FPR values for the desired AUC\n    for tpr, fpr in zip(tpr_values, fpr_values):\n        desired_tpr = np.interp(desired_auc, [original_auc, 0], [1, tpr])\n        desired_fpr = np.interp(desired_tpr, [1, tpr], [0, fpr])\n\n        interpolated_tpr.append(desired_tpr)\n        interpolated_fpr.append(desired_fpr)\n\n    return interpolated_tpr, interpolated_fpr\n\n# Interpolate TPR and FPR for the desired AUC at each threshold\ndesired_tpr_values, desired_fpr_values = interpolate_tpr_fpr_for_auc(desired_auc, tpr_values, fpr_values, original_auc)\n\n# Print the results\nprint(f&quot;Desired TPR values for AUC {desired_auc:.3f}: {desired_tpr_values}&quot;)\nprint(f&quot;Desired FPR values for AUC {desired_auc:.3f}: {desired_fpr_values}&quot;)\n\n</code></pre>\n<p>The auc score will be between 0 to 1. But the plot is for auc(0.001) and so on.</p>\n<p>I tried to use numpy library interpolate function and try to get tpr and fpr at a certain threshold for my desired auc like 0.01, 0.001 and so on. I am not confident enough how the plot is generated and my way of doing as well.</p>\n",
    "is_answered": false,
    "view_count": 114,
    "answer_count": 1
  },
  {
    "title": "How to download XLSX file from DOI link?",
    "link": "https://stackoverflow.com/questions/76812704/how-to-download-xlsx-file-from-doi-link",
    "tags": [
      "python",
      "data-science",
      "doi"
    ],
    "body": "<p>I want to download two files automatically from Python for a reproducible statistical analysis.</p>\n<p>These links</p>\n<ul>\n<li><a href=\"https://doi.org/10.1371/journal.pone.0282068.s001\" rel=\"nofollow noreferrer\">https://doi.org/10.1371/journal.pone.0282068.s001</a></li>\n<li><a href=\"https://doi.org/10.1371/journal.pone.0282068.s002\" rel=\"nofollow noreferrer\">https://doi.org/10.1371/journal.pone.0282068.s002</a></li>\n</ul>\n<p>I tried</p>\n<pre class=\"lang-py prettyprint-override\"><code>import requests\n\nurl = 'https://doi.org/10.1371/journal.pone.0282068.s001'\n\nresponse = requests.get(url)\n</code></pre>\n<p>I suspect that the file is actually the content of <code>response.content</code>, which appears to be a bunch of encoded information (e.g. <code>\\xe2\\x81a\\xe4\\x1dq\\xbe9~3\\x94\\x885\\xba\\xc8\\x9bz\\'~\\x1c)X&gt;\\xaaXyg\\x929\\xf84\\xc2\\x06\\t\\n x5\\</code>).</p>\n<p>How do I download these files and save them as XLSX files?</p>\n",
    "is_answered": true,
    "view_count": 51,
    "answer_count": 1
  },
  {
    "title": "Python 3 ipykernel hangs on a particular line of code",
    "link": "https://stackoverflow.com/questions/76799005/python-3-ipykernel-hangs-on-a-particular-line-of-code",
    "tags": [
      "jupyter-notebook",
      "data-science",
      "jupyter-irkernel"
    ],
    "body": "<p>I tried training a Linear regression model using model.fit on jupyter notebook as usual but my kernel is not executing the particular line of code and no error message.</p>\n<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\ndataset=pd.read_csv(r'C:\\Users\\USER\\Downloads\\melb_data.csv\\melb_data.csv')\ndataset.head()\ncol_to_use=['Suburb','Address','Rooms','Type','Price','Method','SellerG','Distance','Bedroom2','Car','Bathroom','Landsize','BuildingArea','CouncilArea','Regionname']\ndataset=dataset[col_to_use]\ndataset.head()\n\ncol_to_fill_zero=['CouncilArea','Car']\ndataset[col_to_fill_zero]=dataset[col_to_fill_zero].fillna(0)\n\ndataset=pd.get_dummies(dataset,drop_first=True\nX=dataset.drop('Price', axis=1)\ny=dataset['Price']\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n\nfrom sklearn.linear_model import LinearRegression\nmodel=LinearRegression()\nmodel.fit(X_train, y_train)\n</code></pre>\n<p>at this point it shows this [*] and doesn't execute.\nI tried interrupting and restarting the kernel still not working</p>\n",
    "is_answered": false,
    "view_count": 156,
    "answer_count": 1
  },
  {
    "title": "How to perform Time series forecasting in short Interval of time data (only 8 years are given) for multiple locations?",
    "link": "https://stackoverflow.com/questions/76797633/how-to-perform-time-series-forecasting-in-short-interval-of-time-data-only-8-ye",
    "tags": [
      "python",
      "machine-learning",
      "time-series",
      "data-science",
      "forecasting"
    ],
    "body": "<p>I want to find the <strong>production of field</strong> forecasting in year 2018 and 2019 at multiple location for a short interval of data?</p>\n<p>At each Harvesting Site(donated by index in the image and by its Latitute and Longitude) , Production of the field  can be forecasted using historical data.</p>\n<p><a href=\"https://i.sstatic.net/NlArn.png\" rel=\"nofollow noreferrer\">Look at the sample data</a></p>\n<ol>\n<li>How can I find the forecasting for short interval of time and for all these locations?</li>\n<li>Should I train a single model for all locations or different location for different models?</li>\n</ol>\n<p>There are 2418 rows means differnet 2418 field location for which we have to find out the forecasting in incoming years.</p>\n",
    "is_answered": false,
    "view_count": 84,
    "answer_count": 1
  },
  {
    "title": "My test and train data has the same number of columns but OneHotEncoder creates different size of matrixes",
    "link": "https://stackoverflow.com/questions/76781507/my-test-and-train-data-has-the-same-number-of-columns-but-onehotencoder-creates",
    "tags": [
      "pandas",
      "machine-learning",
      "scikit-learn",
      "data-science",
      "random-forest"
    ],
    "body": "<p>I am trying to create a model with train and test datasets which are seperate. They have same number of columns. When I try to encode categorical features the created matrix by OneHotEncoder is comes with different size So I cannot predict my test data since number of features are different.</p>\n<p>both dataframes have same number of columns</p>\n<pre><code>one_hot= OneHotEncoder()\ntransformer= ColumnTransformer([(&quot;one_hot&quot;,one_hot,categorical_features)],\n                              remainder=&quot;passthrough&quot;)\n\ntrain_transformed_X = transformer.fit_transform(pd.DataFrame(bigdata_zeros,columns=categorical_features))\ntrain_transformed_X_test=transformer.fit_transform(pd.DataFrame(bigdata_test_zeros,columns=categorical_features))\n</code></pre>\n<p>Then I build my model</p>\n<pre><code>model = RandomForestRegressor()\nmodel.fit(train_transformed_X,y_train)\n</code></pre>\n<p>Then I want to see the predictions with the test data</p>\n<pre><code>model.predict(train_transformed_X_test)\n</code></pre>\n<p>but I got the following error</p>\n<pre><code>ValueError: X has 256 features, but RandomForestRegressor is expecting 268 features as input.\n</code></pre>\n<p>When I check  train_transformed_X and train_transformed_X_test I can see their size is different.</p>\n<p>I tried to create a ML model with train and test data seperated</p>\n",
    "is_answered": true,
    "view_count": 867,
    "answer_count": 1
  },
  {
    "title": "Is there anything to increase the accuracy of this predictive model?",
    "link": "https://stackoverflow.com/questions/76763488/is-there-anything-to-increase-the-accuracy-of-this-predictive-model",
    "tags": [
      "python",
      "machine-learning",
      "deep-learning",
      "data-science"
    ],
    "body": "<p>I want to improve the accuracy of my trained model. I tried to create an ML model to predict whether a test sample belongs to someone with or without disease, based on the gene expression profiling, if it is possible at all.</p>\n<p>I looked up some resources online and tried to write some code and here I am, stuck at around ~69% accuracy (output below)</p>\n<pre><code>Epoch 1/100\n45049/45049 [==============================] - 106s 2ms/step - loss: 0.6041 - accuracy: 0.6888 - val_loss: 0.6004 - val_accuracy: 0.6928\nEpoch 2/100\n45049/45049 [==============================] - 106s 2ms/step - loss: 0.6016 - accuracy: 0.6905 - val_loss: 0.5996 - val_accuracy: 0.6881\nEpoch 3/100\n45049/45049 [==============================] - 108s 2ms/step - loss: 0.6013 - accuracy: 0.6912 - val_loss: 0.5994 - val_accuracy: 0.6934\nEpoch 4/100\n45049/45049 [==============================] - 105s 2ms/step - loss: 0.6013 - accuracy: 0.6913 - val_loss: 0.5996 - val_accuracy: 0.6881\nEpoch 5/100\n45049/45049 [==============================] - 109s 2ms/step - loss: 0.6010 - accuracy: 0.6919 - val_loss: 0.5999 - val_accuracy: 0.6949\nEpoch 6/100\n45049/45049 [==============================] - 111s 2ms/step - loss: 0.6009 - accuracy: 0.6917 - val_loss: 0.5998 - val_accuracy: 0.6937\nEpoch 7/100\n45049/45049 [==============================] - 133s 3ms/step - loss: 0.6019 - accuracy: 0.6913 - val_loss: 0.6000 - val_accuracy: 0.6894\nEpoch 8/100\n45049/45049 [==============================] - 132s 3ms/step - loss: 0.6014 - accuracy: 0.6918 - val_loss: 0.5987 - val_accuracy: 0.6959\nEpoch 9/100\n45049/45049 [==============================] - 121s 3ms/step - loss: 0.6007 - accuracy: 0.6925 - val_loss: 0.5994 - val_accuracy: 0.6946\nEpoch 10/100\n45049/45049 [==============================] - 126s 3ms/step - loss: 0.6007 - accuracy: 0.6929 - val_loss: 0.6000 - val_accuracy: 0.6941\nEpoch 11/100\n45049/45049 [==============================] - 137s 3ms/step - loss: 0.6019 - accuracy: 0.6918 - val_loss: 0.5999 - val_accuracy: 0.6883\nEpoch 12/100\n45049/45049 [==============================] - 136s 3ms/step - loss: 0.6009 - accuracy: 0.6925 - val_loss: 0.5985 - val_accuracy: 0.6957\nEpoch 13/100\n45049/45049 [==============================] - 137s 3ms/step - loss: 0.6013 - accuracy: 0.6922 - val_loss: 0.5987 - val_accuracy: 0.6958\nEpoch 14/100\n45049/45049 [==============================] - 138s 3ms/step - loss: 0.6006 - accuracy: 0.6931 - val_loss: 0.5996 - val_accuracy: 0.6939\nEpoch 15/100\n45049/45049 [==============================] - 137s 3ms/step - loss: 0.6006 - accuracy: 0.6928 - val_loss: 0.6001 - val_accuracy: 0.6868\nEpoch 16/100\n45049/45049 [==============================] - 136s 3ms/step - loss: 0.6007 - accuracy: 0.6927 - val_loss: 0.5990 - val_accuracy: 0.6956\nEpoch 17/100\n45049/45049 [==============================] - 138s 3ms/step - loss: 0.6008 - accuracy: 0.6926 - val_loss: 0.6003 - val_accuracy: 0.6921\nEpoch 18/100\n45049/45049 [==============================] - 138s 3ms/step - loss: 0.6011 - accuracy: 0.6918 - val_loss: 0.5992 - val_accuracy: 0.6892\nEpoch 19/100\n45049/45049 [==============================] - 138s 3ms/step - loss: 0.6010 - accuracy: 0.6924 - val_loss: 0.6000 - val_accuracy: 0.6886\nEpoch 20/100\n45049/45049 [==============================] - 137s 3ms/step - loss: 0.6007 - accuracy: 0.6925 - val_loss: 0.6001 - val_accuracy: 0.6885\nEpoch 21/100\n45049/45049 [==============================] - 141s 3ms/step - loss: 0.6012 - accuracy: 0.6912 - val_loss: 0.5990 - val_accuracy: 0.6896\nEpoch 22/100\n45049/45049 [==============================] - 138s 3ms/step - loss: 0.6010 - accuracy: 0.6917 - val_loss: 0.5994 - val_accuracy: 0.6889\n12514/12514 [==============================] - 21s 2ms/step - loss: 0.5988 - accuracy: 0.6957\nANN Test accuracy: 0.6957491040229797\n</code></pre>\n<p>As for what I've written, I'll attach the code down below, to make things clearer.</p>\n<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# Load data from the dataset file\ndataset_file = 'concatenated_dataset.csv'\ndf = pd.read_csv(dataset_file)\n\n# Check if there are any missing values in the 'VALUE' column\nif df['VALUE'].isnull().any():\n    # Handling missing values with SimpleImputer\n    imputer = SimpleImputer(strategy='mean')\n    df['VALUE'] = imputer.fit_transform(df['VALUE'].values.reshape(-1, 1))\n\n# Split the data into features (X) and target variable (y)\nX = df['VALUE'].values.reshape(-1, 1)\ny = df['Target'].values\n\n# Step 2: Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Step 3: Feature Scaling (optional, but recommended for neural networks)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Step 4: Build the ANN model\ninput_dim = X_train_scaled.shape[1]\n\nmodel = keras.Sequential([\n    layers.Dense(units=256, activation='relu', input_shape=(input_dim,)),\n    layers.Dropout(0.3),\n    layers.Dense(units=128, activation='relu'),\n    layers.Dropout(0.2),\n    layers.Dense(units=64, activation='relu'),\n    layers.Dropout(0.1),\n    layers.Dense(units=1, activation='sigmoid')  # For binary classification\n])\n\n# Step 5: Compile the model\noptimizer = keras.optimizers.Adam(learning_rate=0.001)\nmodel.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n\n# Step 6: Train the ANN model with early stopping\nearly_stopping = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\nhistory = model.fit(X_train_scaled, y_train, epochs=100, batch_size=32,\n                    validation_split=0.1, callbacks=[early_stopping])\n\n# Step 7: Evaluate the ANN model on the test set\nann_loss, ann_accuracy = model.evaluate(X_test_scaled, y_test)\nprint(&quot;ANN Test accuracy:&quot;, ann_accuracy)\n</code></pre>\n<p>How can I improve the ANN test accuracy from 69% to around 90%?</p>\n",
    "is_answered": false,
    "view_count": 68,
    "answer_count": 1
  },
  {
    "title": "How to simulate ARIMA model data in python? using statsmodels.tsa.arima.model.ARIMA.simulate()",
    "link": "https://stackoverflow.com/questions/76762396/how-to-simulate-arima-model-data-in-python-using-statsmodels-tsa-arima-model-ar",
    "tags": [
      "python",
      "data-science",
      "statsmodels",
      "arima"
    ],
    "body": "<p>Need to know how to simulate data in python using</p>\n<pre><code>statsmodels.tsa.arima.model.ARIMA.simulate()\n</code></pre>\n<p>Hitting error :</p>\n<pre><code>TypeError: MLEModel.simulate() missing 1 required positional argument: 'self'\n</code></pre>\n<p>self=? IDK why ?\nProcess=?\nnsimulation=?</p>\n",
    "is_answered": false,
    "view_count": 630,
    "answer_count": 1
  },
  {
    "title": "How to allow user to reorder list when presented with one?",
    "link": "https://stackoverflow.com/questions/76759294/how-to-allow-user-to-reorder-list-when-presented-with-one",
    "tags": [
      "python",
      "pandas",
      "sorting",
      "for-loop",
      "data-science"
    ],
    "body": "<p>I want to create a list of unique values from a column of a DataFrame and then allow the user to sort the list arbitrarily based on their preference of the items in the list. For example:</p>\n<pre><code>fruit_info = pd.read_csv('Fruit Information.csv')\nfruit_list = (fruit_info['Fruit Name'].unique().tolist())\nprint(fruit_list)\n\n['Apple','Banana','Cherry','Orange','Watermelon']\n\nsorted_fruit_list = [favorite, 2nd favorite, 3rd favorite, ..., least favorite]\n</code></pre>\n<p>I have tried creating a dictionary with the fruits as keys and passing ranked integers as values and then sorting the dictionary by values and converting to a list, but it seems clunky. Is there a better way to get user input and then pass the new order to the list? Thank you!</p>\n",
    "is_answered": false,
    "view_count": 90,
    "answer_count": 1
  },
  {
    "title": "Creating Dynamic Folders based on Filename in Azure Data Factory DataFlow",
    "link": "https://stackoverflow.com/questions/76753693/creating-dynamic-folders-based-on-filename-in-azure-data-factory-dataflow",
    "tags": [
      "azure",
      "data-science",
      "azure-data-factory"
    ],
    "body": "<p>I am working on an Azure Data Factory data flow pipeline where I have a Sink activity. One of the columns in the sink contains filename information in the format &quot;2023-07-19_diane_12345.csv&quot;. I want to use a pattern to extract specific data from the filename and create folders based on the following pattern yyyy/mm/dd.</p>\n<p>I tried using the substring and lastIndexOf functions in the expression, but it is not working as expected.\nBelow parameter contains the filename</p>\n<p><a href=\"https://i.sstatic.net/GW7vy.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/GW7vy.png\" alt=\"enter image description here\" /></a></p>\n<p>and in the SINK activity, I am trying to use expression builder to do that job but it says column not found.</p>\n<p><a href=\"https://i.sstatic.net/IRxxs.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/IRxxs.png\" alt=\"enter image description here\" /></a></p>\n<p>It seems the expression I am building is for CopyActivity not for Dataflow I guess. Is there any other way to perform the job and create folders dynamically?</p>\n<p>The pipeline looks like this</p>\n<p><a href=\"https://i.sstatic.net/OKqI0.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/OKqI0.png\" alt=\"enter image description here\" /></a></p>\n<p><strong>Also the sink settings:</strong></p>\n<p><a href=\"https://i.sstatic.net/1RFiH.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/1RFiH.png\" alt=\"enter image description here\" /></a></p>\n",
    "is_answered": true,
    "view_count": 702,
    "answer_count": 1
  },
  {
    "title": "Folium popup not working when rendering HTML",
    "link": "https://stackoverflow.com/questions/76752259/folium-popup-not-working-when-rendering-html",
    "tags": [
      "python",
      "data-science",
      "folium"
    ],
    "body": "<p>I want to do HTML formatting into a folium map popup. When I try to render html by using <code>def format_popup_content(row)</code> function then the map does not display. How do I format popup?</p>\n<p>This is what I have tried so far</p>\n<pre><code>def format_popup_content(row):\n    # Format the popup content using HTML\n    popup_content = f&quot;&quot;&quot;\n    &lt;div style=&quot;font-family: Arial, sans-serif;&quot;&gt;\n        &lt;h2 style=&quot;margin-bottom: 5px; text-align: center;&quot;&gt;District: {row['district']}&lt;/h2&gt;\n        &lt;table style=&quot;width: 100%;&quot;&gt;\n            &lt;tr&gt;\n                &lt;td style=&quot;font-weight: bold;&quot;&gt;Diagnose:&lt;/td&gt;\n                &lt;td&gt;{row['provisionaldiagnose']}&lt;/td&gt;\n            &lt;/tr&gt;\n            &lt;tr&gt;\n                &lt;td style=&quot;font-weight: bold;&quot;&gt;Age:&lt;/td&gt;\n                &lt;td&gt;{row['age']}&lt;/td&gt;\n            &lt;/tr&gt;\n            &lt;tr&gt;\n                &lt;td style=&quot;font-weight: bold;&quot;&gt;Gender:&lt;/td&gt;\n                &lt;td&gt;{row['pgender']}&lt;/td&gt;\n            &lt;/tr&gt;\n            &lt;!-- Add more information here if needed --&gt;\n        &lt;/table&gt;\n    &lt;/div&gt;\n    &quot;&quot;&quot;\n    return popup_content\n</code></pre>\n<p>This is the code of popup functionality</p>\n<pre><code>popup_text = folium.Html(format_popup_content(row), script=True) \niframe = branca.element.IFrame(html=popup_text, width=320, height=500)\npopup = folium.Popup(iframe, parse_html=True)\n\n# Update the color parameter to use the corresponding color from the color dictionary\nfolium.CircleMarker([lat, lon], radius=7, color=color, opacity=1.0, fill_color=color, popup=popup).add_to(district_layers[district])\n</code></pre>\n",
    "is_answered": false,
    "view_count": 430,
    "answer_count": 1
  },
  {
    "title": "Matplotlib: Plotting all columns on the x-axis and values on the y-axis grouped by a third variable",
    "link": "https://stackoverflow.com/questions/76733035/matplotlib-plotting-all-columns-on-the-x-axis-and-values-on-the-y-axis-grouped",
    "tags": [
      "python",
      "pandas",
      "matplotlib",
      "plot",
      "data-science"
    ],
    "body": "<p>I have data that looks like this:</p>\n<pre><code>group    var1    var2    var3\n0        0.6    0.001    0.11  \n1       -0.36  -0.007   -0.066481 \n</code></pre>\n<p>I want to create a dot graph where the 3 different variables form the x-axis and the y-axis represent the values in the data frame, with different symbols/colors for the dots depending on the group on the leftmost column.</p>\n<p>I tried melting the data into the format</p>\n<pre><code>group var  value\n0     var1 0.6\n1     var1 -0.36\n0     var2 0.001\n1     var2 -0.007\n0     var3 0.11\n1     var3 -0.066481\n</code></pre>\n<p>But I was only able to make separate plots by variable instead of one big plot. Any help would be appreciated, thank you!</p>\n",
    "is_answered": true,
    "view_count": 53,
    "answer_count": 1
  },
  {
    "title": "Why can\u2019t I use df.loc() within the drop() method?",
    "link": "https://stackoverflow.com/questions/76718445/why-can-t-i-use-df-loc-within-the-drop-method",
    "tags": [
      "python",
      "pandas",
      "dataframe",
      "data-science"
    ],
    "body": "<p>I\u2019m just getting started on pandas and I was trying to delete a sequence of rows.</p>\n<p>In my data frame, the index is the province names.</p>\n<p>Since <code>df.loc[\u2018NL\u2019:\u2019QC\u2019]</code> returns a list of rows, I thought I would feed that as an input to my drop function to specify which rows I want deleted, but it doesn\u2019t work.\nCan someone help me understand why? Thank you.</p>\n<p>This is what I wrote</p>\n<pre><code>df.drop(df.loc[\u2018NL\u2019:\u2019QC\u2019])\n</code></pre>\n<p>It threw an error that said could not find \u2018province\u2019 , \u2018province_name\u2019 etc on axis.\nThese are the names of my columns.</p>\n<p>I know that by default the <code>drop()</code> will have <code>axis =0</code> , so I\u2019m not sure why it\u2019s parsing through columns? Or why it \u201ccan\u2019t\u201d find it.</p>\n<p>If I change <code>axis = 1</code>, it works and all my columns disappear.\nJust my indices remain from NL to QC and nothing else.</p>\n<p>Apologies if I\u2019m not explaining it the best, I\u2019m just starting out and may not be using the best words to describe my problem.\nAppreciate the help, thank you.</p>\n",
    "is_answered": true,
    "view_count": 200,
    "answer_count": 2
  },
  {
    "title": "How to visualize the correlation of columns into regression alike charts?",
    "link": "https://stackoverflow.com/questions/76712627/how-to-visualize-the-correlation-of-columns-into-regression-alike-charts",
    "tags": [
      "python",
      "python-3.x",
      "regression",
      "seaborn",
      "data-science"
    ],
    "body": "<h1>Conditions</h1>\n<p>I have columns named, Shopping_Satisfaction and Rating_Accuracy</p>\n<p>They both store a value of rating range from 1 to 5, as below</p>\n<pre><code>import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\npath = '/kaggle/input/amazon-consumer-behaviour-dataset/Amazon Customer Behavior Survey.csv'\ndf = pd.read_csv(path)\ndf.groupby(['Shopping_Satisfaction', 'Rating_Accuracy']).count()['age']\n</code></pre>\n<p><strong>Group By Results</strong></p>\n<pre><code>Shopping_Satisfaction  Rating_Accuracy\n1                      1                   37\n                       2                   47\n                       3                   24\n                       4                    6\n                       5                    1\n2                      1                   10\n                       2                   92\n                       3                   79\n                       4                    9\n                       5                    3\n3                      1                   10\n                       2                   34\n                       3                  145\n                       4                   18\n                       5                    2\n4                      1                    1\n                       2                    5\n                       3                   34\n                       4                   21\n                       5                    6\n5                      2                    1\n                       3                    5\n                       4                    2\n                       5                    9\nName: age, dtype: int64\n</code></pre>\n<p>They are likely correlated, with 51% of correlation</p>\n<pre><code>                        Shopping_Satisfaction   Rating_Accuracy\nShopping_Satisfaction   1.000000                0.514387\nRating_Accuracy         0.514387                1.000000\n</code></pre>\n<h1>Questions</h1>\n<p>How To Translate These Insights for a question</p>\n<p><strong>More Shopping_Satisfaction = More Rating_Accuracy?</strong></p>\n<p>OR</p>\n<p><strong>More Shopping_Satisfaction != More Rating_Accuracy?</strong></p>\n<p>I tried using Regression Plot, but it didn't work well</p>\n<p>It looks like this</p>\n<pre><code>5    o   o   o   o   o\n4    o   o   o   o   o\n3    o   o   o   o   o\n2    o   o   o   o   o\n1    o   o   o   o   o\n0    1   2   3   4   5\n</code></pre>\n<p>Any idea?</p>\n",
    "is_answered": false,
    "view_count": 42,
    "answer_count": 1
  },
  {
    "title": "Python / Pandas: Shift entities of a row to the right (end)",
    "link": "https://stackoverflow.com/questions/76704112/python-pandas-shift-entities-of-a-row-to-the-right-end",
    "tags": [
      "python",
      "pandas",
      "dataframe",
      "data-science",
      "shift"
    ],
    "body": "<p>I have the following data frame (number of &quot;Date&quot; columns can vary):</p>\n<p><strong>Customer  Date1  Date2  Date3  Date4</strong>\n0        A     10   40.0    NaN   60.0</p>\n<p>1        B     20   50.0    NaN    NaN</p>\n<p>2        C     30    NaN    NaN    NaN</p>\n<p>If there is a &quot;NaN&quot; in the last column (as said, number of columns can vary), I want to right shift all the columns to the end of the data frame such that it then looks like this:</p>\n<p><strong>Customer  Date1  Date2  Date3  Date4</strong></p>\n<p>0        A     10   40.0    NaN   60.0</p>\n<p>1        B    NaN    NaN     20   50.0</p>\n<p>2        C    NaN    NaN    NaN     30</p>\n<p>All the values which remain empty can be set to NaN.</p>\n<p>How can I do that in Python?</p>\n<p>I tried this code but didn't work:</p>\n<pre><code>import numpy as np\nimport pandas as pd\n\ndata = {\n    'Customer': ['A', 'B', 'C'],\n    'Date1': [10, 20, 30],\n    'Date2': [40, 50, np.nan],\n    'Date3': [np.nan, np.nan, np.nan],\n    'Date4': [60, np.nan, np.nan]\n}\n\ndf = pd.DataFrame(data)\n\n\nfor i in range(1, len(df.columns)):\n    df.iloc[:, i] = df.iloc[:, i-1].shift(fill_value=np.nan)\n\nprint(df)\n</code></pre>\n",
    "is_answered": true,
    "view_count": 74,
    "answer_count": 2
  },
  {
    "title": "How to calculate time differences without a date and only with times?",
    "link": "https://stackoverflow.com/questions/76689953/how-to-calculate-time-differences-without-a-date-and-only-with-times",
    "tags": [
      "python",
      "pandas",
      "dataframe",
      "datetime",
      "data-science"
    ],
    "body": "<pre><code>import pandas as pd\n\nstoptimes_df = pd.DataFrame({\n    'trip_id': ['1', '1', '1', '2', '2', '2'], \n    'arrival_time': [&quot;12:10:00&quot;, &quot;12:20:00&quot;, &quot;12:30:00&quot;, &quot;27:32:00&quot;, &quot;27:39:00&quot;, &quot;27:45:00&quot;],\n    'departure_time': [&quot;12:10:00&quot;, &quot;12:20:00&quot;, &quot;12:30:00&quot;, &quot;27:32:00&quot;, &quot;27:39:00&quot;, &quot;27:45:00&quot;],\n    'stop_id': ['de:08437:48835:0:2', 'de:08426:6306', 'de:08426:6307', 'de:08116:6703', 'de:08116:3821', 'de:08415:28256:0:1']})\n</code></pre>\n<p>I have this dataframe given, which shows different bus lines (trip_id) and the different stops, and I would like to insert a new column which contains the difference between the arrival time of the following line and the departure time of the line before. Unfortunately I am not able to do this because when I change the datatype to <code>datetime.time()</code> I can not calculate which the times. This is only possible if I use the datatype <code>datetime.datetime()</code>, but then I have also a date in the columns &quot;arrival_time&quot; and &quot;departure time&quot; written, like &quot;1900-01-01 12:10:00&quot;, which I do not want. I have a similar problem when I use timedelta. So the point is I want to keep only the times without a date in the two given columns and in the new column there should be the time difference in minutes or seconds. For example in the last line it should say in the new column 6 (min) or 300 (sec). Does someone know how to do this?</p>\n<p>What I did so far in code:</p>\n<pre><code>def convert_to_datetime(time):\n    hours, minutes, seconds = map(int, time.split(':'))\n    hours = hours % 24  # change time format to 0-24 hours\n    time = str(hours) + ':' + str(minutes) + ':' + str(seconds)\n    time = datetime.strptime(time,&quot;%H:%M:%S&quot;).time()\n    # time_delta = timedelta(hours=hours, minutes=minutes, seconds=seconds)\n    return time\n\nstoptimes_df['arrival_time'] = stoptimes_df['arrival_time'].apply(convert_to_datetime)\nstoptimes_df['departure_time'] = stoptimes_df['departure_time'].apply(convert_to_datetime)\nstoptimes_df\n\n# tried first with only one column to calculate\nstoptimes_df['time_btw_stops'] = stoptimes_df.groupby('trip_id')['arrival_time'].diff()\nstoptimes_df\n</code></pre>\n<p>This leads to the following error:</p>\n<blockquote>\n<p>TypeError: unsupported operand type(s) for -: 'datetime.time' and\n'datetime.time'</p>\n</blockquote>\n",
    "is_answered": true,
    "view_count": 199,
    "answer_count": 2
  },
  {
    "title": "Why Azure Data Factory DataFlow is only processing one line per file?",
    "link": "https://stackoverflow.com/questions/76686845/why-azure-data-factory-dataflow-is-only-processing-one-line-per-file",
    "tags": [
      "azure",
      "data-science",
      "azure-data-factory"
    ],
    "body": "<p>I have configured a Dataflow in Azure Data Factory to process multiple files. The data in the files is in JSON format and contains multiple lines of content(array of objects). However, when the pipeline finishes running, the processed files only contain a single line of data instead of the expected entire data.I am processing the files to remove one of the Attributes highlighted below schema.\n<strong>Pipeline</strong>\n<a href=\"https://i.sstatic.net/uFh1W.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/uFh1W.png\" alt=\"enter image description here\" /></a></p>\n<p><strong>The schema of input source file</strong></p>\n<p><a href=\"https://i.sstatic.net/PZuJz.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/PZuJz.png\" alt=\"enter image description here\" /></a></p>\n<p><strong>and my Dataflow looks like this</strong>\n<a href=\"https://i.sstatic.net/CjwC2.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/CjwC2.png\" alt=\"enter image description here\" /></a></p>\n<p>Following are the steps that I am performing</p>\n<ol>\n<li>Take a derived column transformation and convert the attributes object to string using expression toString(attributes)</li>\n<li>Then parse the String attributes as below as (session as string,     orgToken as string, partnerToken as string) and leave the userID out of it.</li>\n<li>Then in select transformation delete the attributes object and rename the parsed object as attributes that only contains session,orgToken, partnerToken removing userID out of it.</li>\n</ol>\n<p><strong>Output</strong>: The output of the pipeline is expected but it only processes one line per file and not the whole content of the file means 6 files processed contains one line per file rather than whole data. I tried single document, array of objects, documents per line but no success as shown below</p>\n<pre><code>{&quot;date&quot;:&quot;2023-07-06\\t15:33:36.464&quot;,&quot;level&quot;:&quot;INFO&quot;,&quot;msg&quot;:&quot;Init&quot;,&quot;resource&quot;:{&quot;class&quot;:&quot;NuCaSessionManager&quot;,&quot;function&quot;:&quot;Init&quot;,&quot;line&quot;:167,&quot;module&quot;:&quot;NuCa&quot;,&quot;pid&quot;:24848,&quot;thread&quot;:&quot;[25824]&quot;},&quot;spanId&quot;:&quot;asfffwfe&quot;,&quot;traceId&quot;:&quot;acas&quot;,&quot;attributes&quot;:{}}\n</code></pre>\n",
    "is_answered": false,
    "view_count": 514,
    "answer_count": 1
  },
  {
    "title": "How to access child class from parent class in web scraping using python",
    "link": "https://stackoverflow.com/questions/76685550/how-to-access-child-class-from-parent-class-in-web-scraping-using-python",
    "tags": [
      "python",
      "web-scraping",
      "beautifulsoup",
      "data-science"
    ],
    "body": "<pre><code>from bs4 import BeautifulSoup\nimport requests\nimport pandas as pd\ndata={&quot;title&quot;:[],&quot;price&quot;:[]}\nheader={&quot;User-Agent&quot;:&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)         Chrome/114.0.0.0 Safari/537.36&quot;}\nurl=&quot;https://www.amazon.in/deals?ref_=nav_cs_gb&quot;\nresponse=requests.get(url,headers=header)\nsoup=BeautifulSoup(response.text,&quot;html.parser&quot;)\nwhole=soup.find(&quot;div&quot; ,{&quot;class&quot;:&quot;jumbo-tracker&quot;})\nname=whole.select(&quot;h4.sc-1hp8d8a-0.sc-lffWgi.flnmvC&quot;)\nprint(name)\n</code></pre>\n<p>This is what i have done so far. I can't figure out why I am getting nonetype when  there is already a class in zomato website and inspect element .\nplease go through the link in url and please help me to find the name of resturants .</p>\n",
    "is_answered": false,
    "view_count": 141,
    "answer_count": 0
  },
  {
    "title": "Segmentation with piecewise linear regression",
    "link": "https://stackoverflow.com/questions/76674414/segmentation-with-piecewise-linear-regression",
    "tags": [
      "python",
      "data-science",
      "breakpoints",
      "subsequence"
    ],
    "body": "<p>I am willing to segment a sequence into <code>n</code> subsequences (<code>n</code> known) where my points in each subsequence should be similar in a way that a piecewise linear function could fit the points (minimize the distance in each subsequence and the overall sequence).\nI have tried the package ruptures with the algo <strong>Binseg</strong> which allows to provide the number of segments and I have also tried numpy/scipy to directly fit <strong>piecewise linear functions</strong>.\nThen I realized I need to apply weights to my points, else what I want to achieve doesn't work great. How could I trick either solution, or do you know another solution that could directly take as an argument an <strong>array of weights</strong>?</p>\n<p><strong>Edit for more context:</strong></p>\n<ul>\n<li>The curve is usually flat, steepening, flattening, concave or convex, or a mix, and consists of at most 40 points (i.e. max sequence is 40)</li>\n<li>There can be 1 or 2 outliers (usually the first point but not necessarily)</li>\n<li>I am aiming at <code>n</code> between 4 and 8 (i.e. between 4 and 8 subsequences, defined as a parameter).</li>\n<li>My sequence is at most 40 points, and it's in a loop with ~20 iterations. It must take less than 30sec for the whole loop, so max ~1.5sec per sequence.</li>\n</ul>\n<p>Here is an example of the data:</p>\n<pre><code>df = pd.DataFrame({\n    'Term Dummy':[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32],\n    'Shock':[131.759276601612,-5.28111953539055,-5.30412333137685,6.19553924065018,-5.97658803726517,-7.8325986545673,-9.50784210778306,-15.7385664305344,-23.3182508381464,-29.4897840376819,-31.467551725682,-33.4723203935889,-34.6650947285782,-35.7471724234754,-36.4799776375108,-37.3264043303424,-37.4155331344124,-37.8155991350952,-38.7550833588797,-38.3608088160098,-36.7211814243519,-35.7477615422699,-34.1458248652337,-32.8287847811565,-31.4018236645802,-29.9742754473972,-28.6193854123123,-24.90985538625,-21.3217573325541,-18.7350606702909,-16.0799516664911,-16.1549305201347,-16.1433168994669],\n    'Weight':[1924,41170,120247,289092,311692,50265,127579,38255,225164,300420,96928,189792,177827,511969,417120,17840,72257,160679,89074,186051,102120,53770,662958,100838,765414,820977,533239,113092,60063,174082,238152,215960,115665]\n})\n</code></pre>\n<p><code>Shock</code> is the sequence that needs to be reduced to <code>n</code> subsequences.\nFor example, a solution with <code>n=6</code> could be (grouping the index/Term dummy):</p>\n<pre><code>[\n[0],\n[1,2],\n[3],\n[4,5,6,7,8,9,10,11,12,13,14,15,16,17,18],\n[19,20,21,22,23,24,25,26,27,28,29],\n[30,31,32]\n]\n</code></pre>\n<p><a href=\"https://i.sstatic.net/V2V81.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/V2V81.png\" alt=\"Shocks and example of segmentation\" /></a></p>\n<p>The plot below shows 3 curves of <code>Shock</code> on the top, and the <code>Sensitivity</code> (<code>Delta</code> in the plot) and the <code>Impact</code>. <code>Weight</code> is the absolute value of <code>Sensitivity</code>.</p>\n<p><a href=\"https://i.sstatic.net/F2MHR.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/F2MHR.png\" alt=\"Plot of 3 Shock (top), Sensitivity and Impact (bottom)\" /></a></p>\n",
    "is_answered": false,
    "view_count": 376,
    "answer_count": 1
  },
  {
    "title": "Add a column in R and compute difference time",
    "link": "https://stackoverflow.com/questions/76673270/add-a-column-in-r-and-compute-difference-time",
    "tags": [
      "r",
      "statistics",
      "data-science"
    ],
    "body": "<p>I have a data set with about 15 000 rows and ten columns. I want to create a new column &quot; difrence&quot; by using columns &quot; ID&quot; and &quot;DATE&quot; in my data set. The &quot;ID&quot; includes the identity number of people the ids are repetitive. How can I create a new column the for each person, and compute the difference of time?</p>\n<p>How can I code by R</p>\n",
    "is_answered": true,
    "view_count": 57,
    "answer_count": 1
  },
  {
    "title": "Fill NaN values in Polars using a custom-defined function for a specific column",
    "link": "https://stackoverflow.com/questions/76667920/fill-nan-values-in-polars-using-a-custom-defined-function-for-a-specific-column",
    "tags": [
      "python",
      "data-science",
      "python-polars"
    ],
    "body": "<p>I have this code in pandas:</p>\n<pre><code>df[col] = (\n            df[col]\n            .fillna(method=&quot;ffill&quot;, limit=1)\n            .apply(lambda x: my_function(x))\n        )\n</code></pre>\n<p>I want to re-write this in Polars.</p>\n<p>I have tried this:</p>\n<pre><code>df = df.with_columns(\n            pl.col(col)\n            .fill_null(strategy=&quot;forward&quot;, limit=1)\n            .map_elements(lambda x: my_function(x))\n        )\n</code></pre>\n<p>It does not work properly. It fills with forward strategy but ignores filling missing values with my defined function. What should I change in my code to get what I want?</p>\n<p>try this code:</p>\n<pre><code>df_polars = pl.DataFrame(\n    {&quot;A&quot;: [1, 2, None, None, None, None, 4, None], &quot;B&quot;: [5, None, None, None, None, 7, None, 9]}\n)\n\ndf_pandas = pd.DataFrame(\n    {&quot;A&quot;: [1, 2, None, None, None, None, 4, None], &quot;B&quot;: [5, None, None, None, None, 7, None, 9]}\n)\n\nlast_valid_data: int\n\n\ndef my_function(x):\n    global last_valid_data\n    if x == None or np.isnan(x):\n        result = last_valid_data * 10\n    else:\n        last_valid_data = x\n        result = x\n    return result\n\n\ncol = &quot;A&quot;\n\nlast_valid_data = df_pandas[col][0]\ndf_pandas[col] = df_pandas[col].fillna(method=&quot;ffill&quot;, limit=1).apply(lambda x: my_function(x))\n\nlast_valid_data = df_polars[col][0]\ndf_polars = df_polars.with_columns(\n    pl.col(col).fill_null(strategy=&quot;forward&quot;, limit=1).map_elements(lambda x: my_function(x))\n)\n</code></pre>\n<p>Desired output in pandas is:</p>\n<pre><code>      A    B\n0   1.0  5.0\n1   2.0  NaN\n2   2.0  NaN\n3  20.0  NaN\n4  20.0  NaN\n5  20.0  7.0\n6   4.0  NaN\n7   4.0  9.0\n</code></pre>\n<p>What I get in Polars is:</p>\n<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 A    \u2506 B    \u2502\n\u2502 ---  \u2506 ---  \u2502\n\u2502 i64  \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1    \u2506 5    \u2502\n\u2502 2    \u2506 null \u2502\n\u2502 2    \u2506 null \u2502\n\u2502 null \u2506 null \u2502\n\u2502 null \u2506 null \u2502\n\u2502 null \u2506 7    \u2502\n\u2502 4    \u2506 null \u2502\n\u2502 4    \u2506 9    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>\n",
    "is_answered": true,
    "view_count": 1293,
    "answer_count": 1
  },
  {
    "title": "How to simplify &quot;Percentage for each class&quot;",
    "link": "https://stackoverflow.com/questions/76666696/how-to-simplify-percentage-for-each-class",
    "tags": [
      "pandas",
      "dataframe",
      "numpy",
      "data-science"
    ],
    "body": "<p>I have the famous titanic data set</p>\n<pre><code>fr1 = pd.DataFrame({\n&quot;class&quot;: [&quot;1&quot;, &quot;2&quot;, &quot;2&quot;],\n&quot;survived&quot;: [0, 1, 1]})\n</code></pre>\n<p>I need to get the percentage of people who survived for each class, so first I separate applying a mask, then I group</p>\n<pre><code>fr2 = fr1[fr1[&quot;Survived&quot;] == 0]\nfr2 = fr2.groupby(&quot;Pclass&quot;, as_index=False)[&quot;Survived&quot;].agg([&quot;count&quot;])\nfr3 = fr1[fr1[&quot;Survived&quot;] == 1]\nfr3 = fr3.groupby(&quot;Pclass&quot;, as_index=False)[&quot;Survived&quot;].agg([&quot;count&quot;])\n</code></pre>\n<p>Now I merged the data Frame I got and create the percentage column to know how many people survived for each class</p>\n<pre><code>merged = pd.merge(fr2,fr3,left_index=True,right_index=True)\nmerged.columns = &quot;Survived Died&quot;.split()    \nmerged[&quot;Percentage&quot;] = merged[&quot;Survived&quot;] / (len(fr1))*100\n</code></pre>\n",
    "is_answered": true,
    "view_count": 24,
    "answer_count": 1
  },
  {
    "title": "How to simplify &quot;Get mean ages for women of every class&quot;",
    "link": "https://stackoverflow.com/questions/76666416/how-to-simplify-get-mean-ages-for-women-of-every-class",
    "tags": [
      "pandas",
      "dataframe",
      "numpy",
      "data-science"
    ],
    "body": "<p>I have the famous titanic dataset, called fr1.</p>\n<p>The class column contains the following: 1, 2, 3\nThe gender column: male, female</p>\n<p>I want to obtain the mean ages for females for each class, I think It should be a simpler way to code the following, imagine for example that there are 20 classes instead of 3:</p>\n<pre><code>c3 = fr1[fr1[&quot;class&quot;] ==3]\nc3w = fr1[fr1[&quot;gender&quot;] =='female']\nprint(round(c3w[&quot;Age&quot;].mean()))\n</code></pre>\n",
    "is_answered": true,
    "view_count": 35,
    "answer_count": 1
  },
  {
    "title": "Extracting/Removing/skipping Free Text from Json Files in Azure Data Factory",
    "link": "https://stackoverflow.com/questions/76660184/extracting-removing-skipping-free-text-from-json-files-in-azure-data-factory",
    "tags": [
      "azure",
      "data-science",
      "azure-data-factory"
    ],
    "body": "<p>I am trying to do some source transformation in ADF where I have alot server logs in the following format:</p>\n<pre><code>#PartnerName    QA Server\n#ApplicationName    T_GSPClient\n#AccountName    DoNotModifyDMS\n#SDK    desktop\n#ClientVersion  5.1.1894.3\n#InputChannel   DesktopMic\n#User   JohnDoe\n#NmsLogin   JohnDoe\n#SessionId  7ba732d6-3445-4b16-b7e8-345fgd4f5g4\n#ClientIP   209.122.69.109\n#SRTechnology   S2\n#SROptions  NoTextBefore\n#GeneralLogLevel    Trace\n#ModuleLogLevels    \n#ServerDateTimeUTC  2023-07-06 15:28:33.105\n{&quot;date&quot;:&quot;2023-07-06\\t15:28:09.653&quot;,&quot;level&quot;:&quot;TRACE&quot;,&quot;msg&quot;:&quot;DMVAServerMessage-Initialize:{&amp;quot;dmhMessage&amp;quot;:{&amp;quot;messageHeader&amp;quot;:{&amp;quot;messageType&amp;quot;:&amp;quot;unsubscribe&amp;quot;,&amp;quot;sessionId&amp;quot;:&amp;quot;00000000-0000-0000-0000-000000000000&amp;quot;,&amp;quot;messageId&amp;quot;:&amp;quot;aeb65e55-a7c0-4e96-a960-dc2a252d6b2c&amp;quot;,&amp;quot;transactionType&amp;quot;:&amp;quot;acknowledgement&amp;quot;,&amp;quot;clientType&amp;quot;:&amp;quot;ttsChannel&amp;quot;,&amp;quot;version&amp;quot;:&amp;quot;1.0&amp;quot;,&amp;quot;application&amp;quot;:&amp;quot;DesktopSDK&amp;quot;},&amp;quot;messageResponse&amp;quot;:{&amp;quot;resultCode&amp;quot;:&amp;quot;SERVER_ERROR&amp;quot;,&amp;quot;errorMessage&amp;quot;:&amp;quot;The subscription id does not exist, unsubscribe did not remove an entry&amp;quot;}}} &quot;,&quot;traceId&quot;:&quot;409e0d44-ad50-4f17-84c7-0521e01e11fc&quot;,&quot;spanId&quot;:&quot;e0750e44-ad50-4f17-90a9-3b6940e0294b&quot;,&quot;resource&quot;:{&quot;module&quot;:&quot;.NET&quot;,&quot;class&quot;:&quot;Nuance.SpeechAnywhere.Internal.DMVA.DMVAServerMessage&quot;,&quot;function&quot;:&quot;Initialize&quot;,&quot;line&quot;:70,&quot;pid&quot;:26612,&quot;thread&quot;:&quot;[28-27280]&quot;}}\n{&quot;date&quot;:&quot;2023-07-06\\t15:28:09.653&quot;,&quot;level&quot;:&quot;TRACE&quot;,&quot;msg&quot;:&quot;DMVAServerMessage-Initialize:{&amp;quot;dmhMessage&amp;quot;:{&amp;quot;messageHeader&amp;quot;:{&amp;quot;messageType&amp;quot;:&amp;quot;unsubscribe&amp;quot;,&amp;quot;sessionId&amp;quot;:&amp;quot;00000000-0000-0000-0000-000000000000&amp;quot;,&amp;quot;messageId&amp;quot;:&amp;quot;aeb65e55-a7c0-4e96-a960-dc2a252d6b2c&amp;quot;,&amp;quot;transactionType&amp;quot;:&amp;quot;acknowledgement&amp;quot;,&amp;quot;clientType&amp;quot;:&amp;quot;ttsChannel&amp;quot;,&amp;quot;version&amp;quot;:&amp;quot;1.0&amp;quot;,&amp;quot;application&amp;quot;:&amp;quot;DesktopSDK&amp;quot;},&amp;quot;messageResponse&amp;quot;:{&amp;quot;resultCode&amp;quot;:&amp;quot;SERVER_ERROR&amp;quot;,&amp;quot;errorMessage&amp;quot;:&amp;quot;The subscription id does not exist, unsubscribe did not remove an entry&amp;quot;}}} &quot;,&quot;traceId&quot;:&quot;409e0d44-ad50-4f17-84c7-0521e01e11fc&quot;,&quot;spanId&quot;:&quot;e0750e44-ad50-4f17-90a9-3b6940e0294b&quot;,&quot;resource&quot;:{&quot;module&quot;:&quot;.NET&quot;,&quot;class&quot;:&quot;Nuance.SpeechAnywhere.Internal.DMVA.DMVAServerMessage&quot;,&quot;function&quot;:&quot;Initialize&quot;,&quot;line&quot;:70,&quot;pid&quot;:26612,&quot;thread&quot;:&quot;[28-27280]&quot;}}\n{&quot;date&quot;:&quot;2023-07-06\\t15:28:09.653&quot;,&quot;level&quot;:&quot;TRACE&quot;,&quot;msg&quot;:&quot;DMVAServerMessage-Initialize:{&amp;quot;dmhMessage&amp;quot;:{&amp;quot;messageHeader&amp;quot;:{&amp;quot;messageType&amp;quot;:&amp;quot;unsubscribe&amp;quot;,&amp;quot;sessionId&amp;quot;:&amp;quot;00000000-0000-0000-0000-000000000000&amp;quot;,&amp;quot;messageId&amp;quot;:&amp;quot;aeb65e55-a7c0-4e96-a960-dc2a252d6b2c&amp;quot;,&amp;quot;transactionType&amp;quot;:&amp;quot;acknowledgement&amp;quot;,&amp;quot;clientType&amp;quot;:&amp;quot;ttsChannel&amp;quot;,&amp;quot;version&amp;quot;:&amp;quot;1.0&amp;quot;,&amp;quot;application&amp;quot;:&amp;quot;DesktopSDK&amp;quot;},&amp;quot;messageResponse&amp;quot;:{&amp;quot;resultCode&amp;quot;:&amp;quot;SERVER_ERROR&amp;quot;,&amp;quot;errorMessage&amp;quot;:&amp;quot;The subscription id does not exist, unsubscribe did not remove an entry&amp;quot;}}} &quot;,&quot;traceId&quot;:&quot;409e0d44-ad50-4f17-84c7-0521e01e11fc&quot;,&quot;spanId&quot;:&quot;e0750e44-ad50-4f17-90a9-3b6940e0294b&quot;,&quot;resource&quot;:{&quot;module&quot;:&quot;.NET&quot;,&quot;class&quot;:&quot;Nuance.SpeechAnywhere.Internal.DMVA.DMVAServerMessage&quot;,&quot;function&quot;:&quot;Initialize&quot;,&quot;line&quot;:70,&quot;pid&quot;:26612,&quot;thread&quot;:&quot;[28-27280]&quot;}}\n{&quot;date&quot;:&quot;2023-07-06\\t15:28:09.653&quot;,&quot;level&quot;:&quot;TRACE&quot;,&quot;msg&quot;:&quot;DMVAServerMessage-Initialize:{&amp;quot;dmhMessage&amp;quot;:{&amp;quot;messageHeader&amp;quot;:{&amp;quot;messageType&amp;quot;:&amp;quot;unsubscribe&amp;quot;,&amp;quot;sessionId&amp;quot;:&amp;quot;00000000-0000-0000-0000-000000000000&amp;quot;,&amp;quot;messageId&amp;quot;:&amp;quot;aeb65e55-a7c0-4e96-a960-dc2a252d6b2c&amp;quot;,&amp;quot;transactionType&amp;quot;:&amp;quot;acknowledgement&amp;quot;,&amp;quot;clientType&amp;quot;:&amp;quot;ttsChannel&amp;quot;,&amp;quot;version&amp;quot;:&amp;quot;1.0&amp;quot;,&amp;quot;application&amp;quot;:&amp;quot;DesktopSDK&amp;quot;},&amp;quot;messageResponse&amp;quot;:{&amp;quot;resultCode&amp;quot;:&amp;quot;SERVER_ERROR&amp;quot;,&amp;quot;errorMessage&amp;quot;:&amp;quot;The subscription id does not exist, unsubscribe did not remove an entry&amp;quot;}}} &quot;,&quot;traceId&quot;:&quot;409e0d44-ad50-4f17-84c7-0521e01e11fc&quot;,&quot;spanId&quot;:&quot;e0750e44-ad50-4f17-90a9-3b6940e0294b&quot;,&quot;resource&quot;:{&quot;module&quot;:&quot;.NET&quot;,&quot;class&quot;:&quot;Nuance.SpeechAnywhere.Internal.DMVA.DMVAServerMessage&quot;,&quot;function&quot;:&quot;Initialize&quot;,&quot;line&quot;:70,&quot;pid&quot;:26612,&quot;thread&quot;:&quot;[28-27280]&quot;}}\n</code></pre>\n<p>My objective is remove/skip the first free text lines and keep the other json data and then move it to another blob for further transformation. I tried source transformation with derived column but my Data Flow is still showing json format error. I also used Copy Data activity via ForEach with &quot;enableSkipIncompatibleRow&quot;: true, it doesn't work. It only works if I just work with single file, not when I try to iterate over many files and skip/remove those lines.</p>\n",
    "is_answered": true,
    "view_count": 183,
    "answer_count": 1
  },
  {
    "title": "how can i add custom prompt template on ConversationalRetrievalChain",
    "link": "https://stackoverflow.com/questions/76653061/how-can-i-add-custom-prompt-template-on-conversationalretrievalchain",
    "tags": [
      "data-science",
      "openai-api",
      "langchain"
    ],
    "body": "<pre><code>qa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0.8,model_name='gpt-3.5-turbo-16k'),db.as_retriever(), memory=memory)\n</code></pre>\n<p>creating a chatbot for replying in a document. I want add prompt to it that it must only reply from the document and avoid making up the answer</p>\n",
    "is_answered": true,
    "view_count": 2048,
    "answer_count": 1
  },
  {
    "title": "Difference between predict() and predict_proba() functions in scikit learn",
    "link": "https://stackoverflow.com/questions/76650079/difference-between-predict-and-predict-proba-functions-in-scikit-learn",
    "tags": [
      "python",
      "scikit-learn",
      "statistics",
      "data-science"
    ],
    "body": "<p>Greetings data science community! How's going? So, I'm studying classification Tree and scikit-learning and during my studyings i come across this &quot;issue&quot;:</p>\n<p>After training a tree (clf = DecisionTreeClassifier()) and training it (<code>clf.fit(Xtrain, ytrain)</code>) i have decided to test its performance on the training data itself (just to compare, later, with the test data, in terms of Sensitivity Specificity and ROC-AUC).<br />\nBut instead to only apply the <code>predict()</code> I also applied the <code>predict_proba()</code> on the X_train data.</p>\n<p>As you can se by the image, the observation 4 has 50 % of probability to give zero and 50%  to give one (according to <code>predict_proba()</code> function) however the <code>predict()</code> function classified it as zero</p>\n<p><a href=\"https://i.sstatic.net/Cjlo0.png\" rel=\"nofollow noreferrer\">Image with the dataframe where the first column is the result from predict_proba() function and the second column is the result from predict() column</a></p>\n<p>Did the predict() function sort as ZERO by &quot;chance&quot; or since it's zero or one, does it sort as zero because it comes first (as if order matters)?</p>\n<p>I could not solve my doubts when analyzing the documentation of the functions (source: <a href=\"https://github.com/scikit-learn/scikit-learn/blob/7f9bad99d/sklearn/tree/%5C_classes.py#L476\" rel=\"nofollow noreferrer\">https://github.com/scikit-learn/scikit-learn/blob/7f9bad99d/sklearn/tree/\\_classes.py#L476</a>)</p>\n<p>Thanks in advance!</p>\n",
    "is_answered": true,
    "view_count": 242,
    "answer_count": 1
  },
  {
    "title": "Using linear optimisation, how do I minimize the Total Cost in a dataframe",
    "link": "https://stackoverflow.com/questions/76647505/using-linear-optimisation-how-do-i-minimize-the-total-cost-in-a-dataframe",
    "tags": [
      "python",
      "pandas",
      "data-science",
      "linear-programming"
    ],
    "body": "<p>I have a Pandas dataframe with 3 columns (Product, Weight, Total Cost) as follows (expanded to make it clearer):</p>\n<pre><code>df = {\n    'Product': ['Product 1', 'Product 2', 'Product 3', 'Product 4',\n                'Product 1', 'Product 2', 'Product 3', 'Product 4',\n                'Product 1', 'Product 2', 'Product 3', 'Product 4',\n                'Product 1', 'Product 2', 'Product 3', 'Product 4',\n                'Product 1', 'Product 2', 'Product 3', 'Product 4',\n                'Product 1', 'Product 2', 'Product 3', 'Product 4',\n                'Product 1', 'Product 2', 'Product 3', 'Product 4',\n                'Product 1', 'Product 2', 'Product 3', 'Product 4'],\n    'Pack Size': [1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7, 7, 7, 7, 8, 8, 8, 8],\n    'Total Cost': [2773.47, 2665.23, 23421.24, 17666.58, 1592.09, 1678.04, 12798.46, 9425.80, 1246.30, 1396.98,\n                   9377.70, 6621.21, 1109.40, 1292.45, 7491.82, 5275.17, 1056.07, 1258.53, 6482.69, 4517.04,\n                   1044.51, 1259.92, 5845.94, 4035.62, 1056.82, 1281.48, 5411.69, 3712.32, 1084.06, 1315.65,\n                   5104.00, 3487.85]\n}\n</code></pre>\n<p>The dataframe holds the Total Cost of each potential Weight of each Product. Every Product and Weight combination have a different cost so if there's 1000 Product and Weight combinations, there are 1000 different Total Costs.</p>\n<p>I need to determine which 2 Weights (which must be selected from 'Weight' in df) I should choose in order to achieve the lowest Total Cost across all products (referring to the total cost for that product and weight combination).</p>\n<p>Every product must have 1 weight assigned to it and it can only have one of these 2 weights.</p>\n<p>The output should look as follows:</p>\n<pre><code>Product 1: Weight=3\nProduct 2: Weight=3\nProduct 3: Weight=3\nProduct 4: Weight=5\n</code></pre>\n<p>As you can see every product has a weight, and in total there are only 2 weights selected across all products, 3 and 5.</p>\n<p>Another way to demonstrate it is to display it this way (assume there were 10 products)</p>\n<pre><code>Weight 6: Product 1, Product 4, Product 5\nWeight 18: Product 2, Product 3, Product 6, Product 8, Product 9, Product 7, Product 10\n</code></pre>\n<p>Again, only 2 Weights selected across all products (6 and 18) and every product is assigned to one of them.</p>\n<p>The objective is to determine which weight should be chosen for each product (and its respective total cost) in order to minimise the total cost across all products.</p>\n<p>The dataframe will always be populated with different amounts of Products (and therefore rows), potentially over 100 Products.</p>\n<p>There will always be 48 Weights per Product on the dataframe which are 1 ... 48 (therefore if there were 100 products, there would be 4800 rows)</p>\n<p>Hoping someone could help me with this one. Thanks!</p>\n<p>I've tried a variety of approaches but I don't know how to approach this with a dataframe.</p>\n<p>The solution provided by Sebastian Wozny using pulp was just about what I needed.</p>\n",
    "is_answered": false,
    "view_count": 193,
    "answer_count": 2
  },
  {
    "title": "8bit Quantization: Prediction outputs uncorrelated to underlying model",
    "link": "https://stackoverflow.com/questions/76645320/8bit-quantization-prediction-outputs-uncorrelated-to-underlying-model",
    "tags": [
      "tensorflow",
      "data-science",
      "quantization",
      "tflite",
      "8-bit"
    ],
    "body": "<p>I  quantized a basic TFLite regression model to int8 but the prediction output seems to be highly uncorrelated with the actual underlying model prior to quantizing it.</p>\n<p>All the code and steps taken to train and quantize the model are seen below to make it easy to replicate the issue (just copy and paste it :).</p>\n<p>I am working with the famous boston_housing dataset, which can be downloaded here <a href=\"https://www.kaggle.com/code/prasadperera/the-boston-housing-dataset/input\" rel=\"nofollow noreferrer\">https://www.kaggle.com/code/prasadperera/the-boston-housing-dataset/input</a></p>\n<p>The steps I took are as follows:</p>\n<ol>\n<li>Trained the linear regression model without quantization (which worked fine)</li>\n<li>Created a new quantization aware model using the previous model\n3)Converted the quantization-aware model to TFlite (setting the input and output tensor to int8)</li>\n<li>Converted the validation dataset to int8</li>\n<li>Used  the quantized model to make predictions (predictions are  significantly off and seem uncorrelated to the underlying model)</li>\n</ol>\n<p>i</p>\n<pre><code>import pandas as pd\nimport numpy as np\nfrom numpy import loadtxt\n# load data\ndataset = loadtxt('boston_housing.csv', delimiter=&quot;,&quot;)\n# split into inputs and outputs\ndataset_x = dataset[:, :-1]\ndataset_y = dataset[:, -1]\n\n\ndataset_x = np.float32(dataset_x )\ndataset_y = np.float32(dataset_y )\n\nfrom sklearn.model_selection import train_test_split\ntraining_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size = 0.20)\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nmodel = Sequential()\nmodel.add(Dense(100, input_dim = 13, activation='relu'))\nmodel.add(Dense(1,))\nmodel.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\nhist = model.fit(training_dataset_x, training_dataset_y, batch_size=32, epochs=500, validation_split=0.2)\n\n\n#Quantize the model\nimport tensorflow_model_optimization as tfmot\n\nquantize_model = tfmot.quantization.keras.quantize_model\nq_aware_model = quantize_model(model)\n\n# 'quantize_model' requires a recompile\nq_aware_model.compile(optimizer='rmsprop',\n                  loss='mse',\n                  metrics=['mae'])\n\nq_aware_model.summary()\n history = q_aware_model.fit(training_dataset_x, training_dataset_y, batch_size=32, epochs=500, validation_split=0.2)\n\nprint(test_dataset_y[1])\nprint(q_aware_model.predict(test_dataset_x[1].reshape(1, -1)))\n\n\n\n\n\n\n\n#Convert the model to TFLite\nimport tensorflow as tf\n\n# Create a converter\nconverter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n\n # Indicate that you want to perform default optimizations,\n # which include quantization\n converter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n # Define a generator function that provides your test data's numpy arrays\n def representative_data_gen():\n     for i in range(10500):\n         yield [test_dataset_x[i:i+1]]\n\n # Use the generator function to guide the quantization process\n converter.representative_dataset = representative_data_gen\n\n # Ensure that if any ops can't be quantized, the converter throws an error\n converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n\n  # Set the input and output tensors to int8\n  converter.inference_input_type = tf.int8\n  converter.inference_output_type = tf.int8\n\n # Convert the model\n tflite_model = converter.convert()\n\n  # Save the model to disk\n  open(&quot;q_aware_model.tflite&quot;, &quot;wb&quot;).write(tflite_model)\n\n\n \n\n\n\n\n\n #Testing the quantized model\n\n # Load the TFLite model and allocate tensors.\n interpreter = tf.lite.Interpreter(model_path=&quot;q_aware_model.tflite&quot;)\n interpreter.allocate_tensors()\n\n\n  # Get input and output tensors.\n  input_details = interpreter.get_input_details()\n  output_details = interpreter.get_output_details()\n\n  print(input_details)\n  print(output_details )\n\n  test_x1 = (test_dataset_x-128).astype(np.int8)\n  print(test_x1)\n\n\n\n\n  predictions=[]\n  for i in range(len(test_x1)):\n      test_values = np.expand_dims(test_x1[i].flatten(), axis=0)\n\n      # Set the value for the input tensor\n      interpreter.set_tensor(input_details[0]['index'], test_values)\n\n      # Run the inference\n      interpreter.invoke()\n\n      output = interpreter.get_tensor(output_details[0]['index'])\n      predictions.append(output)\n\n\n  \n  print(predictions) ### prediction values = 50-127, expected values 0-40\n</code></pre>\n",
    "is_answered": false,
    "view_count": 195,
    "answer_count": 0
  },
  {
    "title": "How to merge columns with same value and add values together from different columns using python",
    "link": "https://stackoverflow.com/questions/76644039/how-to-merge-columns-with-same-value-and-add-values-together-from-different-colu",
    "tags": [
      "python",
      "pandas",
      "dataframe",
      "data-science"
    ],
    "body": "<p><a href=\"https://i.sstatic.net/5mxYM.png\" rel=\"nofollow noreferrer\">enter image description here</a></p>\n<p>Hi, I'm taking a data science course and not familiar with python.\nI was wondering if it is possible to merge the same years together and adding the global sales for each year togehter?</p>\n<p>after googling, I tried using .groupby but that didn't work, and I also tried .aggregate and it also failed.</p>\n<p>maybe I'm using the commands wrong, or maybe I'm using the wrong commands for the task, because I'm very new to this I am not sure</p>\n",
    "is_answered": true,
    "view_count": 617,
    "answer_count": 1
  },
  {
    "title": "There is a problem in the encoding the string variable to float or integer in sklearn during pipeline building",
    "link": "https://stackoverflow.com/questions/76643281/there-is-a-problem-in-the-encoding-the-string-variable-to-float-or-integer-in-sk",
    "tags": [
      "machine-learning",
      "scikit-learn",
      "data-science",
      "pipeline",
      "one-hot-encoding"
    ],
    "body": "<p>I was building a pipeline in sklearn using the column transformer ,I was using the column transform for the encoding and the code runs well but during training it is showing error that &quot;could not convert string to float: 'male' &quot;. This error is showing after building the pipeline during training. The encoding part work well but something went wrong during the training.</p>\n<p>I was using the titanic dataset(train.csv) which is easily available in Kaggle.</p>\n<pre class=\"lang-py prettyprint-override\"><code>import numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.pipeline import Pipeline,make_pipeline\nfrom sklearn.feature_selection import SelectKBest,chi2\nfrom sklearn.compose import ColumnTransformer \n\na = pd.read_csv(&quot;C:\\\\Users\\\\SURAJ SINGH\\\\OneDrive\\\\Desktop\\\\train.csv&quot;)\na = a.drop(columns  = [&quot;PassengerId&quot;,&quot;Name&quot;,&quot;Ticket&quot;,&quot;Cabin&quot;],axis = 1)\na\n\nxtrain,xtest,ytrain,ytest = train_test_split(a.drop(&quot;Survived&quot;,axis = 1),a[&quot;Survived&quot;],test_size = .2,random_state = 3)\nxtrain\n\ntf1 = ColumnTransformer([\n    (&quot;impute_age&quot;,SimpleImputer(),[2]),\n    (&quot;impute_embarked&quot;,SimpleImputer(strategy = &quot;most_frequent&quot;),[6])],\n    remainder = &quot;passthrough&quot;)\n\ntf2 = ColumnTransformer([\n    (&quot;embarke_sex&quot;,OneHotEncoder(sparse_output = False,handle_unknown = &quot;ignore&quot;),[1,6])],\n    remainder = &quot;passthrough&quot;)\n\ntf3 = ColumnTransformer([\n    (&quot;scaled_age_fare&quot;,MinMaxScaler(),slice(0,10))],remainder = &quot;passthrough&quot;)\n\ntf4 = SelectKBest(score_func = chi2, k = 5)\n\ntf5 = DecisionTreeClassifier()\n\npipeline_obj2 = make_pipeline(tf1,tf2,tf3,tf4,tf5)\n\nfrom sklearn import set_config\nset_config(display = &quot;diagram&quot;)\n\npipeline_obj2.fit(xtrain,ytrain) ```\n\n\nI already used the label encoding but still not working. Please try to answer in more compressive way because I was new to ML and data science. Any help will be appreciated.\n</code></pre>\n",
    "is_answered": false,
    "view_count": 33,
    "answer_count": 1
  },
  {
    "title": "Problem with login to a journal website with selenium",
    "link": "https://stackoverflow.com/questions/76642129/problem-with-login-to-a-journal-website-with-selenium",
    "tags": [
      "python",
      "selenium-webdriver",
      "web-scraping",
      "data-science"
    ],
    "body": "<p>I ran this code to login into website of journals with my institutional access:</p>\n<pre><code>import pandas as pd\n\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.common.action_chains import ActionChains\nfrom selenium.webdriver.support.ui import Select\nimport undetected_chromedriver as uc\n\nimport time\nimport os\n\n\n\noutput = pd.read_excel(&quot;C:\\\\Users\\\\97254\\\\Downloads\\\\output.xlsx&quot;)\n\nurl = &quot;https://www.tandfonline.com/loi/nvpp20&quot;\n\n\n\n\ndriver = uc.Chrome()\ndriver.maximize_window()\n\ndriver.get(url)\ntime.sleep(5)\n\ndriver.find_element(By.CLASS_NAME,&quot;sign-in-link&quot;).click()\ntime.sleep(5)\ndriver.find_element(&quot;tag name&quot;,&quot;a&quot;).find_element(&quot;xpath&quot;,'//*[@id=&quot;frmLogin&quot;]/div[2]/ul/li/a').click()\ntime.sleep(3)\nelem = driver.find_element(&quot;xpath&quot;,'//*[@id=&quot;shibboleth_search&quot;]/div/input').send_keys(&quot;Bar-Ilan University&quot;)\n</code></pre>\n<p>but google chrome didn't pass me to the identification by password screen. I got kicked out of website , with this output on python :<a href=\"https://i.sstatic.net/Ww9PD.png\" rel=\"nofollow noreferrer\">output</a>. has this occured because I used undetected_chromedriver library? How do I handle this to move on to password identification screen?\nThanks</p>\n",
    "is_answered": false,
    "view_count": 86,
    "answer_count": 2
  },
  {
    "title": "cookies and verification problem in scraping",
    "link": "https://stackoverflow.com/questions/76640223/cookies-and-verification-problem-in-scraping",
    "tags": [
      "python",
      "selenium-webdriver",
      "data-science",
      "screen-scraping",
      "data-mining"
    ],
    "body": "<p>After running this code as part of a project, in order to enter login screen :</p>\n<pre><code>import pandas as pd\n\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.common.action_chains import ActionChains\nfrom selenium.webdriver.support.ui import Select\nimport time\nimport os\n\n\n\noutput = pd.read_excel(&quot;C:\\\\Users\\\\97254\\\\Downloads\\\\output.xlsx&quot;)\n\nurl = &quot;https://www.tandfonline.com/loi/nvpp20&quot;\n\n\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.common.action_chains import ActionChains\n\n\ndriver = webdriver.Chrome()\ndriver.maximize_window()\n\ndriver.get(url)\n\n\ndriver.find_element(By.CLASS_NAME,&quot;sign-in-link&quot;).click()\ntime.sleep(5)\n</code></pre>\n<p>This screen  [human verification] appeared. moreover, when the website was loaded, there was a screen asking to accept cookies [enter image description here]. Is there a selenium method dealing with these problem?\nThanks</p>\n",
    "is_answered": false,
    "view_count": 51,
    "answer_count": 1
  },
  {
    "title": "How to remove specific fields from log files before transferring using Azure Data Factory Data Flows",
    "link": "https://stackoverflow.com/questions/76637605/how-to-remove-specific-fields-from-log-files-before-transferring-using-azure-dat",
    "tags": [
      "azure",
      "data-science",
      "azure-data-factory"
    ],
    "body": "<p>I am new to ADF but I have managed to do some activity. My problem statement is: I have log files generated by a server, and I need to transfer them to another location for further processing. However, before transferring, I want to remove certain fields from the log files such as &quot;partnerToken,&quot; &quot;orgToken,&quot; and &quot;phi_msg.&quot; I have already tested transferring the files using Azure Data Factory Copy Data, but I'm unsure how to achieve the filtering part. Should I introduce Data Flows in Azure Data Factory, and if yes, what expression can I use to drop the specified fields from the log files?</p>\n<p>Schema:</p>\n<pre><code>    {\n  &quot;type&quot;: &quot;object&quot;,\n  &quot;properties&quot;: {\n    &quot;date&quot;: {\n      &quot;type&quot;: &quot;string&quot;,\n      &quot;format&quot;: &quot;date-time&quot;\n    },\n    &quot;level&quot;: {\n      &quot;type&quot;: &quot;string&quot;\n    },\n    &quot;msg&quot;: {\n      &quot;type&quot;: &quot;string&quot;\n    },\n    &quot;logId&quot;: {\n      &quot;type&quot;: &quot;integer&quot;\n    },\n    &quot;traceId&quot;: {\n      &quot;type&quot;: &quot;string&quot;\n    },\n    &quot;spanId&quot;: {\n      &quot;type&quot;: &quot;string&quot;\n    },\n    &quot;resource&quot;: {\n      &quot;type&quot;: &quot;object&quot;,\n      &quot;properties&quot;: {\n        &quot;appName&quot;: {\n          &quot;type&quot;: &quot;string&quot;\n        },\n        &quot;module&quot;: {\n          &quot;type&quot;: &quot;string&quot;\n        },\n        &quot;class&quot;: {\n          &quot;type&quot;: &quot;string&quot;\n        },\n        &quot;function&quot;: {\n          &quot;type&quot;: &quot;string&quot;\n        },\n        &quot;line&quot;: {\n          &quot;type&quot;: &quot;integer&quot;\n        },\n        &quot;pid&quot;: {\n          &quot;type&quot;: &quot;integer&quot;\n        },\n        &quot;thread&quot;: {\n          &quot;type&quot;: &quot;string&quot;\n        }\n      }\n    },\n    &quot;attributes&quot;: {\n      &quot;type&quot;: &quot;object&quot;,\n      &quot;properties&quot;: {\n        &quot;session&quot;: {\n          &quot;type&quot;: &quot;string&quot;\n        },\n        &quot;orgToken&quot;: {\n          &quot;type&quot;: &quot;string&quot;\n        },\n        &quot;partnerToken&quot;: {\n          &quot;type&quot;: &quot;string&quot;\n        },\n        &quot;userId&quot;: {\n          &quot;type&quot;: &quot;string&quot;\n        },  \n        &quot;phi_msg&quot;: {\n          &quot;type&quot;: &quot;string&quot;\n        }\n      }\n    }\n  }\n}\n</code></pre>\n<p>Any guidance on how to achieve this using Azure Data Factory Data Flows and the appropriate expression for dropping the specified fields would be highly appreciated. Thank you in advance!</p>\n",
    "is_answered": true,
    "view_count": 404,
    "answer_count": 1
  },
  {
    "title": "How to convert Pandas Dataframe to the shape of a correlation matrix",
    "link": "https://stackoverflow.com/questions/76632984/how-to-convert-pandas-dataframe-to-the-shape-of-a-correlation-matrix",
    "tags": [
      "python",
      "pandas",
      "dataframe",
      "data-science",
      "correlation"
    ],
    "body": "<p>I have a pandas dataframe which looks vaguely like this:</p>\n<pre><code>Out[130]: \n     xvar            yvar                   meanRsquared\n0    filled_water    precip                 0.119730\n1    filled_water    snow                   0.113214\n2    filled_water    filled_wetland         0.119529\n3    filled_wetland  precip                 0.104826\n4    filled_wetland  snow                   0.121540\n5    filled_wetland  filled_water           0.121540\n[676 rows x 3 columns]\n\n</code></pre>\n<p>I would like to transform it's shape into a more traditional correlation matrix, where the columns and the index are the variables, and the values are the meanRsquared.</p>\n<p>Is there any easy way to do this? I've been playing around for an hour and can't figure out how I could do this.</p>\n<p>DISCLAIMER: Yes, I know pandas has a built in function for creating a correlation matrix. However my current df is the average of hundreds of correlation matrices over many watersheds, so I cannot use that.</p>\n<p>This is my best attempt, but obviously the logic failed towards the end.</p>\n<pre><code>listOfdicts = []\nfor xvar in df['xvar'].unique():\n    for yvar in df['yvar'].unique():\n        adict = {}\n        adict['index'] = xvar \n        adict[yvar] = yvar\n        adict['r'] = df['insert r value here']\n        listOfdicts.append(adict)\nanswer = pd.Dataframe.from_dict(listOfdicts)\n</code></pre>\n<p>I don't expect this to work, but this was my best shot.</p>\n",
    "is_answered": true,
    "view_count": 392,
    "answer_count": 1
  },
  {
    "title": "Clicking a link with selenium library",
    "link": "https://stackoverflow.com/questions/76632422/clicking-a-link-with-selenium-library",
    "tags": [
      "python",
      "selenium-webdriver",
      "web-scraping",
      "data-science",
      "data-mining"
    ],
    "body": "<p>I have a project scraping data from <a href=\"https://www.tandfonline.com/loi/nvpp20\" rel=\"nofollow noreferrer\">tandfonline</a> which I have institutional access to.\nTo get the data from each article I need to click the issue button each time with selenium library, which has a unique class or id. I tried this code:</p>\n<pre><code>driver = webdriver.Chrome()\ndriver.get(url)\ndriver.implicitly_wait(3)\nmy_element = driver.find_element(By.ID,&quot;vnvpp20-18&quot;).click()\n</code></pre>\n<p>but got an error:<a href=\"https://i.sstatic.net/HxzFG.png\" rel=\"nofollow noreferrer\">enter image description here</a></p>\n<pre><code>Traceback (most recent call last):\n  File &quot;C:\\Users\\97254\\PycharmProjects\\pythonProject\\venv\\Final_assignment.py&quot;, line 49, in &lt;module&gt;\n    my_element = driver.find_element(By.ID,&quot;vnvpp20-18&quot;).click()\n  File &quot;C:\\Users\\97254\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py&quot;, line 740, in find_element\n    return self.execute(Command.FIND_ELEMENT, {&quot;using&quot;: by, &quot;value&quot;: value})[&quot;value&quot;]\n  File &quot;C:\\Users\\97254\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py&quot;, line 346, in execute\n    self.error_handler.check_response(response)\n  File &quot;C:\\Users\\97254\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py&quot;, line 245, in check_response\n    raise exception_class(message, screen, stacktrace)\nselenium.common.exceptions.NoSuchElementException: Message: no such element: Unable to locate element: {&quot;method&quot;:&quot;css selector&quot;,&quot;selector&quot;:&quot;[id=&quot;vnvpp20-18&quot;]&quot;}\n  (Session info: chrome=114.0.5735.199); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\nBacktrace:\n    GetHandleVerifier [0x0039A813+48355]\n    (No symbol) [0x0032C4B1]\n    (No symbol) [0x00235358]\n    (No symbol) [0x002609A5]\n    (No symbol) [0x00260B3B]\n    (No symbol) [0x0028E232]\n    (No symbol) [0x0027A784]\n    (No symbol) [0x0028C922]\n    (No symbol) [0x0027A536]\n    (No symbol) [0x002582DC]\n    (No symbol) [0x002593DD]\n    GetHandleVerifier [0x005FAABD+2539405]\n    GetHandleVerifier [0x0063A78F+2800735]\n    GetHandleVerifier [0x0063456C+2775612]\n    GetHandleVerifier [0x004251E0+616112]\n    (No symbol) [0x00335F8C]\n    (No symbol) [0x00332328]\n    (No symbol) [0x0033240B]\n    (No symbol) [0x00324FF7]\n    BaseThreadInitThunk [0x75E57D59+25]\n    RtlInitializeExceptionChain [0x7752B74B+107]\n    RtlClearBits [0x7752B6CF+191]\n</code></pre>\n<p>which id, class name, or tag on the page from these shown in the screenshot name should be in the selenium library I use and how should I build the iteration to click an issue each time after I finish grabbing the data I need from each article?\nThanks</p>\n",
    "is_answered": false,
    "view_count": 106,
    "answer_count": 2
  },
  {
    "title": "scraping data from articles(author name)",
    "link": "https://stackoverflow.com/questions/76631921/scraping-data-from-articlesauthor-name",
    "tags": [
      "python",
      "selenium-chromedriver",
      "data-science",
      "screen-scraping"
    ],
    "body": "<p>as a part from a project,\nI tried to extract author name from a journal I have institutional access. I need to iterate over every journal in each issue. firstly, I tried to extract author name from the tag I thought is most suitable, but got an error. The code I ran:</p>\n<pre><code>import requests\nfrom bs4 import BeautifulSoup\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.common.action_chains import ActionChains\nfrom selenium.webdriver.support.ui import Select\nimport time\nimport os\n\n\n\noutput = pd.read_excel(&quot;C:\\\\Users\\\\97254\\\\Downloads\\\\output.xlsx&quot;)\n\n\nurl = &quot;https://www.tandfonline.com/doi/full/10.1080/17452759.2022.2111585&quot;\n\nresponse = requests.get(url)\n\nsoup = BeautifulSoup(response.text, &quot;html.parser&quot;)\n\nparent_div = soup.find(&quot;div&quot;, class_=&quot;NLM_contrib-group&quot;)\nprint(parent_div)\nauthor_link = parent_div.find_all(&quot;a.href&quot;)\n\nauthor_name = author_link.text\n\nprint(author_name)\n</code></pre>\n<p>I got the following error:</p>\n<pre><code>Traceback (most recent call last):\n  File &quot;C:\\Users\\97254\\PycharmProjects\\pythonProject\\venv\\Final_assignment.py&quot;, line 55, in &lt;module&gt;\n    author_link = parent_div.find_all(&quot;a.href&quot;)\nAttributeError: 'NoneType' object has no attribute 'find_all'.\n</code></pre>\n<p>Which is the relevant tag from these <a href=\"https://i.sstatic.net/3XgGu.png\" rel=\"nofollow noreferrer\">tags</a> in the article page and what is the correct method to find this tag and add it as a string to a list in with selenium library.</p>\n",
    "is_answered": false,
    "view_count": 125,
    "answer_count": 1
  },
  {
    "title": "How to avoid NaN values when I use frame[&#39;Colum&#39;].map(dict)",
    "link": "https://stackoverflow.com/questions/76624813/how-to-avoid-nan-values-when-i-use-framecolum-mapdict",
    "tags": [
      "pandas",
      "dataframe",
      "numpy",
      "data-science"
    ],
    "body": "<p>I have the following dataset frame1</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Color</th>\n<th>Item</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Red</td>\n<td>Shirt</td>\n</tr>\n<tr>\n<td>White</td>\n<td>Shoes</td>\n</tr>\n<tr>\n<td>Yellow</td>\n<td>Shirt</td>\n</tr>\n<tr>\n<td>Green</td>\n<td>Shoes</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>I want to set all the colors for Shoes item to be &quot;Blue&quot;, I use map</p>\n<pre><code>\n\nx = {&quot;Shoes&quot;: &quot;Blue&quot;}\nfr1[&quot;Color&quot;] = fr1[&quot;Item&quot;].map(x)\n</code></pre>\n<p>I expected the following result</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Color</th>\n<th>Item</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Red</td>\n<td>Shirt</td>\n</tr>\n<tr>\n<td>Blue</td>\n<td>Shoes</td>\n</tr>\n<tr>\n<td>Yellow</td>\n<td>Shirt</td>\n</tr>\n<tr>\n<td>Blue</td>\n<td>Shoes</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>Instead I got this</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Color</th>\n<th>Item</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>NaN</td>\n<td>Shirt</td>\n</tr>\n<tr>\n<td>Blue</td>\n<td>Shoes</td>\n</tr>\n<tr>\n<td>NaN</td>\n<td>Shirt</td>\n</tr>\n<tr>\n<td>Blue</td>\n<td>Shoes</td>\n</tr>\n</tbody>\n</table>\n</div>",
    "is_answered": true,
    "view_count": 27,
    "answer_count": 1
  },
  {
    "title": "combine 2 df with a for loop to make projections",
    "link": "https://stackoverflow.com/questions/76621713/combine-2-df-with-a-for-loop-to-make-projections",
    "tags": [
      "r",
      "dataframe",
      "data-science",
      "projection"
    ],
    "body": "<p>I have 2 data frames, both with strings, dates, and numbers. df1 is data for 2020 and df2 is data for 2021-2025. I will use df2 (column H) as a growth rate on df1. I need to multiply all numbers of df1 (columns D, E, F) by df2 (H[i,]) for each year in df2, from 2021 to 2025.</p>\n<p>I have structured a function, but I am still thinking about how to address it.\nCould you please check my code and provide me with some ideas to complete the function?\nI really appreciate your help.</p>\n<pre><code>df1 &lt;- read.csv(&quot;df1.csv&quot;, check.names=FALSE)\ndf2 &lt;- read.csv(&quot;df2.csv&quot;, check.names=FALSE)\n\ndf1:\nA   B   year    D   E   F\nabc ab  2020    0   1   2\ndef cd  2020    3   4   0\nghi ef  2020    0   5   6\njkl gh  2020    7   8   0\nmno ij  2020    0   9   10\n\ndf2: \nyear    H\n2021    1.1\n2022    1.2\n2023    1.3\n2024    1.4\n2025    1.5\n\ndf3 &lt;- data.frame()\nfor (i in 1:length(df2)){\n  df3 = rbind(df1, df2 %&gt;% \n        mutate(df1$all_columns_with_numbers = all_columns_with_numbers[i,] * df2$H[i,] ))\n}\ndf3\n\nA    B     C    D    E      F\nabc ab  2021    0    1.1    2.2\nabc ab  2022    3.6  4.8    0\nabc ab  2023    0    6.5    7.8\nabc ab  2024    9.8  11.2   0\nabc ab  2025    0    13.5   15\ndef cd  2021    \u2026    \u2026      \u2026\n</code></pre>\n",
    "is_answered": true,
    "view_count": 71,
    "answer_count": 2
  },
  {
    "title": "How to fix a data fetching function in python using sql query?",
    "link": "https://stackoverflow.com/questions/76620772/how-to-fix-a-data-fetching-function-in-python-using-sql-query",
    "tags": [
      "python",
      "sql",
      "data-science",
      "analytics",
      "dremio"
    ],
    "body": "<p>I have developed a function which uses parametrized sql based on user input to fetch records. However, I am facing a type mismatch problem in the where clause of the query. please help me fix it.\nStart_Date and End_Date are string stype whereas record_date is datetime64[ns].\nThe Error is stating the the comparison between record_date and start_date and end_date has a type mismatch.</p>\n",
    "is_answered": false,
    "view_count": 78,
    "answer_count": 1
  },
  {
    "title": "Not_Fitted_Error : This LabelEncoder instance is not fitted yet. Call &#39;fit&#39; with appropriate arguments before using this estimator",
    "link": "https://stackoverflow.com/questions/76617953/not-fitted-error-this-labelencoder-instance-is-not-fitted-yet-call-fit-with",
    "tags": [
      "python",
      "pandas",
      "numpy",
      "machine-learning",
      "data-science"
    ],
    "body": "<p><code>I am new to Ml_Modelling</code>\n<code>I am working on a solution to create a model with three columns nature_of_business , industry, products.</code></p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th style=\"text-align: left;\">nature_of_business</th>\n<th style=\"text-align: left;\">industry</th>\n<th style=\"text-align: left;\">products</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\">Manufacturer</td>\n<td style=\"text-align: left;\">capitalgoods</td>\n<td style=\"text-align: left;\">product1</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">Services</td>\n<td style=\"text-align: left;\">Mining</td>\n<td style=\"text-align: left;\">product2</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">Other</td>\n<td style=\"text-align: left;\">capitalgoods</td>\n<td style=\"text-align: left;\">product3</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">Manufacturer</td>\n<td style=\"text-align: left;\">commercialservices</td>\n<td style=\"text-align: left;\">product4</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p><code>Now, with the label encoder I'm encoding the entire Data Frame.</code></p>\n<pre><code>df = df.apply(LabelEncoder().fit_transform)\n</code></pre>\n<p><code>After this i have splitted the data into  test and test and created the model</code>.</p>\n<p><code>Now, I'm trying to predict the Product using the model for a new Dataframe. I'm using the same Label encoder as used earlier.</code></p>\n<pre><code>df_Predict = pd.DataFrame({\n'Industry_nature_of_business': ['Manufacturer'], \n'Industry_Category': ['Capital Goods']})\n df_encoded_Predict = df_Predict.apply(LabelEncoder().fit_transform)\n predictions_train = knc.predict(df_encoded_Predict)\n print(LabelEncoder().inverse_transform(predictions_train))\n</code></pre>\n",
    "is_answered": false,
    "view_count": 143,
    "answer_count": 1
  },
  {
    "title": "Psycopg inserting only two rows on data being passed via for loop",
    "link": "https://stackoverflow.com/questions/76615437/psycopg-inserting-only-two-rows-on-data-being-passed-via-for-loop",
    "tags": [
      "python",
      "data-science",
      "gis",
      "postgis",
      "psycopg2"
    ],
    "body": "<p>I'm using a function to insert data to a PostGIS table using psycopg.</p>\n<pre><code>def shptoPosGIS(a,b,c):\n  fieldRecord = &quot;&quot;&quot; insert into 'tablename' &quot;&quot;&quot;\n   \n  fieldValue = (a,b,c)\n\n  db_cursor.execute(fieldRecord, fieldValue)\n  db_connection.commit()\n\nfor i in range (20000):\n   shptoPostGIS(data[&quot;a&quot;].iloc[i], \n                data[&quot;b&quot;].iloc[i], \n                data[&quot;c&quot;].iloc[i])\n</code></pre>\n<p>My Python script is inserting only two rows rather than 20000 to the PostGIS table. I'm guessing that the problem is related to synchronization. My for loop may run faster than the db_cursor.execute() method. Could anyone give a direction on how to solve this?</p>\n",
    "is_answered": true,
    "view_count": 39,
    "answer_count": 1
  },
  {
    "title": "How can I filter outliers while keeping rows where the target variable is of a certain value?",
    "link": "https://stackoverflow.com/questions/76607974/how-can-i-filter-outliers-while-keeping-rows-where-the-target-variable-is-of-a-c",
    "tags": [
      "pandas",
      "dataframe",
      "conditional-statements",
      "data-science"
    ],
    "body": "<p>I have a large dataset with 30 predictor variables and 1 target variable. The data was calculated to be very imbalanced, with 280,000 values showing false and 450 values true. I am trying to filter out the outliers based on the other 30 columns while keeping all rows where the target value is equal to 1.</p>\n<pre><code>def detect_outlier(df_in, col_name):\n    for row in df_in['Class']:\n        if row != 1:\n            q1 = df_in[col_name].quantile(0.25)\n            q3 = df_in[col_name].quantile(0.75)\n            iqr = q3-q1\n            fence_low  = q1-1.5*iqr\n            fence_high = q3+1.5*iqr\n            df_out = df_in.index[(df_in[col_name] &gt; fence_low) &amp; (df_in[col_name] &lt; fence_high)]\n            return df_out\n\ndef remove(df, ls):\n    ls = sorted(set(ls))\n    df = df.drop(ls)\n    return df\n\ndef remove(df, ls):\n    ls = sorted(set(ls))\n    df = df.drop(ls)\n    return df\n</code></pre>\n<pre><code>list = detect_outlier(cc_df,'Amount')\nlist\n</code></pre>\n<pre><code>cc_df2 = remove(cc_df,list)\ncc_df2\n</code></pre>\n",
    "is_answered": false,
    "view_count": 38,
    "answer_count": 1
  },
  {
    "title": "Selecting row with highest value based on two different columns",
    "link": "https://stackoverflow.com/questions/76606393/selecting-row-with-highest-value-based-on-two-different-columns",
    "tags": [
      "python",
      "pandas",
      "dataframe",
      "numpy",
      "data-science"
    ],
    "body": "<p>I have a dataframe with 3 columns:\nI want to make a rule that if for a same city and same id, pick the maximum value and drop the row with lower value.</p>\n<p>eg:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>City</th>\n<th>ID</th>\n<th>Value</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>London</td>\n<td>1</td>\n<td>12.45</td>\n</tr>\n<tr>\n<td>Amsterdam</td>\n<td>1</td>\n<td>14.56</td>\n</tr>\n<tr>\n<td>Paris</td>\n<td>1</td>\n<td>16.89</td>\n</tr>\n<tr>\n<td>New York</td>\n<td>1</td>\n<td>23.86</td>\n</tr>\n<tr>\n<td>Chicago</td>\n<td>1</td>\n<td>14.56</td>\n</tr>\n<tr>\n<td>Chicago</td>\n<td>1</td>\n<td>20.76</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>Expected Output : Select highest value for same city and ID.\nHere Chicago has 2 entries with same ID, I want to select the row with highest value.</p>\n<p>Expected Output</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>City</th>\n<th>ID</th>\n<th>Value</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>London</td>\n<td>1</td>\n<td>12.45</td>\n</tr>\n<tr>\n<td>Amsterdam</td>\n<td>1</td>\n<td>14.56</td>\n</tr>\n<tr>\n<td>Paris</td>\n<td>1</td>\n<td>16.89</td>\n</tr>\n<tr>\n<td>New York</td>\n<td>1</td>\n<td>23.86</td>\n</tr>\n<tr>\n<td>Chicago</td>\n<td>1</td>\n<td>20.76</td>\n</tr>\n</tbody>\n</table>\n</div>",
    "is_answered": true,
    "view_count": 54,
    "answer_count": 1
  },
  {
    "title": "Converting data in JSON format to string format in python",
    "link": "https://stackoverflow.com/questions/76606268/converting-data-in-json-format-to-string-format-in-python",
    "tags": [
      "python",
      "dataframe",
      "data-science"
    ],
    "body": "<p>I have a IMDB dataset. It has a feature 'geners' which contains data in the form of JSON (i guess so) and to cary out further analysis, i need to convert that to string or any other format. How can i do that?</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>gener</th>\n<th>original_language</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>[{'id': 35, 'name': 'Comedy'}]</td>\n<td>en</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>I need to deal with such data but I dont know how</p>\n<p>i was expecting some usable output like storing the 'name' (from geners column) to some other column or anything that will be more usable than this format</p>\n",
    "is_answered": false,
    "view_count": 122,
    "answer_count": 3
  },
  {
    "title": "How to prune a classification decision tree based on classification thresholds",
    "link": "https://stackoverflow.com/questions/76604183/how-to-prune-a-classification-decision-tree-based-on-classification-thresholds",
    "tags": [
      "scikit-learn",
      "data-science",
      "classification",
      "decision-tree",
      "pruning"
    ],
    "body": "<p>I'm using sklearn to try to train a binary classification decision tree to classify spam vs not spam. My classification threshold is 50% (i.e I'll flag it as Spam if I think there's a 50%+ chance that it is). Assume the classes aren't imbalanced.</p>\n<p>Imagine one branch of my tree has 5000 non-spam samples and 100 spam. The tree continues to split this down further, for example leaf A has 1000 non-spam and 70 spam, leaf B has 4000 non-spam and 30 spam. This split doesn't get pruned because it significantly reduces the gini, but based on my 50% classification threshold this split doesn't actually change any predictions - everything will still be predicted as non-spam.</p>\n<p>It feels like logically there should be some way of automatically pruning a classification tree based on a classification threshold, but other than manually inspecting the tree I can't think of how to do this and I've been unable to turn up any solutions through Google. I could decrease the max_depth or increase the min_impurity_decrease, but both of those would penalise other branches by removing useful splits.</p>\n",
    "is_answered": false,
    "view_count": 174,
    "answer_count": 1
  },
  {
    "title": "Self-querying retrieval in langchain returning only 4 results",
    "link": "https://stackoverflow.com/questions/76603178/self-querying-retrieval-in-langchain-returning-only-4-results",
    "tags": [
      "python",
      "python-3.x",
      "machine-learning",
      "data-science",
      "langchain"
    ],
    "body": "<p>I want all the articles related to specific tag example sustainability from documents. But it is only returing me Four articles. There are total 7 articles related to sustainability in vectorstore out of 20 articles.</p>\n<p>Here is my code:</p>\n<pre><code>import pinecone\nfrom langchain.schema import Document\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.vectorstores import Pinecone\nimport os\nfrom langchain.llms import OpenAI\nfrom langchain.retrievers.self_query.base import SelfQueryRetriever\nfrom langchain.chains.query_constructor.base import AttributeInfo\n\n\npinecone_api_key = &quot;xxxxxxxx&quot;\npinecone_env = &quot;xxxxxxxxxx&quot;\n# pinecone.init(api_key=os.environ[&quot;PINECONE_API_KEY&quot;], environment=os.environ[&quot;PINECONE_ENV&quot;])\npinecone.init(api_key=pinecone_api_key, environment=pinecone_env)\n\nembeddings = OpenAIEmbeddings()\n\nindex_name=&quot;langchain-self-retriever-ppo&quot;\nindex = pinecone.Index(index_name)\ntext_field = &quot;text&quot;\n\n# vectorstore = Pinecone.from_documents(\n#     docs, embeddings, index_name=&quot;langchain-self-retriever-demo&quot;\n# )\n\nvectorstore = Pinecone(\n    index, embeddings.embed_query, text_field\n)\n\nmetadata_field_info=[\n    AttributeInfo(\n        name=&quot;headline&quot;,\n        description=&quot;The headline of the news article&quot;, \n        type=&quot;string or list[string]&quot;, \n    ),\n    AttributeInfo(\n        name=&quot;date&quot;,\n        description=&quot;The date, news article was published&quot;, \n        type=&quot;integer&quot;, \n    ),\n    AttributeInfo(\n        name=&quot;publication&quot;,\n        description=&quot;The name of the publication which published this news article&quot;, \n        type=&quot;string&quot;, \n    ),\n    AttributeInfo(\n        name=&quot;domain&quot;,\n        description=&quot;The domain of the news article&quot;,\n        type=&quot;float&quot;\n    ),\n]\ndocument_content_description = &quot;Brief summary of a news article&quot;\nllm = OpenAI(temperature=0)\n# retriever = SelfQueryRetriever.from_llm(llm, vectorstore, document_content_description, metadata_field_info, verbose=True)\n\nretriever = SelfQueryRetriever.from_llm(\n    llm, \n    vectorstore, \n    document_content_description, \n    metadata_field_info, \n    enable_limit=True,\n    verbose=True\n)\n\n# This example only specifies a relevant query\nretrieved_docs = retriever.get_relevant_documents(&quot;Articles which are related to sustainability&quot;)\nprint(retrieved_docs)\nprint(len(retrieved_docs))\n</code></pre>\n<p>I have gone inside <code>get_relevant_documents</code> method here it uses <code>self.vectorstore.search</code> which calls <code>self.similarity_search</code> method which by defaults sets limit to 4 if not given.</p>\n<p>I tried setting limit to 7 it returned 7 <code>sustainability</code> articles.\nBut I wouldn't know how much articles will be related to <code>sustainability</code> so I can't by default set the limit.</p>\n",
    "is_answered": true,
    "view_count": 3901,
    "answer_count": 3
  },
  {
    "title": "What happens while predicting using a model when preprocessing layer is inside the model itself?",
    "link": "https://stackoverflow.com/questions/76595794/what-happens-while-predicting-using-a-model-when-preprocessing-layer-is-inside-t",
    "tags": [
      "pandas",
      "tensorflow",
      "machine-learning",
      "data-science",
      "tensorflow-lite"
    ],
    "body": "<p>While training if we apply preprocessing like horizontal flip inside the model itself won't it affect the data while its predicting?</p>\n<p>so whats a work around to this or will it not matter if the augmentations are done on predictions as well?</p>\n",
    "is_answered": false,
    "view_count": 23,
    "answer_count": 1
  },
  {
    "title": "Getting an error from hdbscan while importing bertopic",
    "link": "https://stackoverflow.com/questions/76589137/getting-an-error-from-hdbscan-while-importing-bertopic",
    "tags": [
      "python",
      "anaconda",
      "data-science",
      "topic-modeling"
    ],
    "body": "<p>I'm trying to import bertopic but it gives the following error. I tried different versions and re create a new environment. But it's still same. I'm using Apple M2 Pro processor</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>lib</th>\n<th>version</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>BERTopic</td>\n<td>0.15.0</td>\n</tr>\n<tr>\n<td>HDBSCAN</td>\n<td>0.8.29</td>\n</tr>\n<tr>\n<td>umap-learn</td>\n<td>0.5.3\u00a0</td>\n</tr>\n</tbody>\n</table>\n</div><hr />\n<pre><code>TypeError                                 Traceback (most recent call last)\nCell In[3], line 4\n      2 import pandas as pd\n      3 # import matplotlib.pyplot as plt\n----&gt; 4 from bertopic import BERTopic\n      5 import gensim\n      6 import gensim.corpora as corpora\n\nFile ~/miniforge3/envs/bertopic/lib/python3.8/site-packages/bertopic/__init__.py:1\n----&gt; 1 from bertopic._bertopic import BERTopic\n      3 __version__ = &quot;0.15.0&quot;\n      5 __all__ = [\n      6     &quot;BERTopic&quot;,\n      7 ]\n\nFile ~/miniforge3/envs/bertopic/lib/python3.8/site-packages/bertopic/_bertopic.py:37\n     34 from typing import List, Tuple, Union, Mapping, Any, Callable, Iterable\n     36 # Models\n---&gt; 37 import hdbscan\n     38 from umap import UMAP\n     39 from sklearn.preprocessing import normalize\n\nFile ~/miniforge3/envs/bertopic/lib/python3.8/site-packages/hdbscan/__init__.py:1\n----&gt; 1 from .hdbscan_ import HDBSCAN, hdbscan\n      2 from .robust_single_linkage_ import RobustSingleLinkage, robust_single_linkage\n      3 from .validity import validity_index\n\nFile ~/miniforge3/envs/bertopic/lib/python3.8/site-packages/hdbscan/hdbscan_.py:40\n     37 from .plots import CondensedTree, SingleLinkageTree, MinimumSpanningTree\n     38 from .prediction import PredictionData\n---&gt; 40 FAST_METRICS = KDTree.valid_metrics + BallTree.valid_metrics + [&quot;cosine&quot;, &quot;arccos&quot;]\n     42 # Author: Leland McInnes &lt;leland.mcinnes@gmail.com&gt;\n     43 #         Steve Astels &lt;sastels@gmail.com&gt;\n     44 #         John Healy &lt;jchealy@gmail.com&gt;\n     45 #\n     46 # License: BSD 3 clause\n     47 from numpy import isclose\n\nTypeError: unsupported operand type(s) for +: 'builtin_function_or_method' and 'builtin_function_or_method'\n</code></pre>\n",
    "is_answered": true,
    "view_count": 897,
    "answer_count": 1
  },
  {
    "title": "Compare number in numpy array",
    "link": "https://stackoverflow.com/questions/76583637/compare-number-in-numpy-array",
    "tags": [
      "arrays",
      "algorithm",
      "numpy",
      "data-science"
    ],
    "body": "<p>Suppose I have an array that stores a total of 800,000 integers of type 'int8'. I want to compare every pair of numbers in the array, and the result should be 1 if the numbers in those two positions are equal, and 0 if the numbers in those two positions are not equal. The results should be stored in an array of size (800000,800000) of type 'int8'. What are some efficient ways to compute this while optimizing speed and minimizing memory usage?</p>\n<p>Ex.</p>\n<pre><code>array = [52, 41, 62 , 52]\nresult\n[1,0,0,1\n 0,1,0,0\n 0,0,1,0\n 1,0,0,1]\n</code></pre>\n",
    "is_answered": true,
    "view_count": 62,
    "answer_count": 2
  },
  {
    "title": "How do obtain the local minimum and maximum points of a stock using yfinance in python?",
    "link": "https://stackoverflow.com/questions/76560259/how-do-obtain-the-local-minimum-and-maximum-points-of-a-stock-using-yfinance-in",
    "tags": [
      "python",
      "database",
      "data-science",
      "finance",
      "stock"
    ],
    "body": "<p>I'am trying to predict if a stock is a buying point or a selling point using logistical regressio. In order to do this, I require the local minimum and maximum points of a stock. For example,</p>\n<p><a href=\"https://i.sstatic.net/30WKH.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/30WKH.png\" alt=\"enter image description here\" /></a></p>\n<p>Image from: <a href=\"https://finance.yahoo.com/chart/MRNA?showOptin=1\" rel=\"nofollow noreferrer\">https://finance.yahoo.com/chart/MRNA?showOptin=1</a></p>\n<p>0: represents local minimum and 1: represents local maximum</p>\n<p>How do I insert these values into a dataframe. For example if a stock was a local minimum point yesterday how would I use yfinance in python to put 0 in my dataframe. Basically what I'am asking is how do I obtain this specific point? Is it is shown as red or green in the graph. However how do I insert it as 1 or 0 in my dataframe using yfinance.</p>\n<p>I couldn't find any info on how to obtain this data. I heard it was on TD ameritrade however it is not of my best interest to sign up for it. Furthermore, the local minimum and maximum points are shown as red or green on the yahoofinance charts online. However I would like these red and green points to be ones and zeros in my data frame. In addition, Yahoo finance does not provide the local maximum or minimum info on their spreadsheets of historical data aswell.</p>\n",
    "is_answered": true,
    "view_count": 493,
    "answer_count": 1
  },
  {
    "title": "sklearn OrdinalEncoder default ordering (categories=&#39;auto&#39;)",
    "link": "https://stackoverflow.com/questions/76557111/sklearn-ordinalencoder-default-ordering-categories-auto",
    "tags": [
      "scikit-learn",
      "data-science"
    ],
    "body": "<p>What is the default rule used by <code>sklearn</code> <code>OrdinaleEcoder</code> to determine the order of the categories when <code>categories='auto'</code>?</p>\n<p>Is it just sorted lexicographically? couldn't find it in the docs</p>\n",
    "is_answered": true,
    "view_count": 483,
    "answer_count": 1
  },
  {
    "title": "Do clustering on dataset but don&#39;t divide items with the same label",
    "link": "https://stackoverflow.com/questions/76539887/do-clustering-on-dataset-but-dont-divide-items-with-the-same-label",
    "tags": [
      "python",
      "machine-learning",
      "data-science",
      "cluster-analysis"
    ],
    "body": "<p>I have a dataset with 10k+ points. Some of the data is labeled into 100+ labels. The rest is to classify.</p>\n<p>I cannot classify the new data directly as the number of labels is too high and the result doesn't look good enough.</p>\n<p>So I want to learn clustering algorithm on the labeled dataset and divide roughly into 3-8 bins. I don't care which labels go into which group, but I want them to be concise. So the new data will be assigned into the correct cluster. And later on classified within model trained on a single bin.</p>\n<p>Question: How to cluster dataset but force the algorithm to keep the different points in the same label in the same cluster?</p>\n<p>Data:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Id</th>\n<th>Label</th>\n<th>Feature 1</th>\n<th>Feature 2</th>\n<th>[...]</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>A</td>\n<td>1</td>\n<td>3</td>\n<td>...</td>\n</tr>\n<tr>\n<td>2</td>\n<td>A</td>\n<td>7</td>\n<td>7</td>\n<td>...</td>\n</tr>\n<tr>\n<td>3</td>\n<td>B</td>\n<td>10</td>\n<td>9</td>\n<td>...</td>\n</tr>\n<tr>\n<td>4</td>\n<td>B</td>\n<td>50</td>\n<td>11</td>\n<td>...</td>\n</tr>\n<tr>\n<td>5</td>\n<td>C</td>\n<td>91</td>\n<td>15</td>\n<td>...</td>\n</tr>\n<tr>\n<td>6</td>\n<td>C</td>\n<td>31</td>\n<td>17</td>\n<td>...</td>\n</tr>\n<tr>\n<td>7</td>\n<td>D</td>\n<td>0</td>\n<td>19</td>\n<td>...</td>\n</tr>\n<tr>\n<td>...</td>\n<td>...</td>\n<td>...</td>\n<td>...</td>\n<td>...</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>Expected outcome:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Id</th>\n<th>Label</th>\n<th><strong>Cluster</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>A</td>\n<td><strong>1</strong></td>\n</tr>\n<tr>\n<td>2</td>\n<td>A</td>\n<td><strong>1</strong></td>\n</tr>\n<tr>\n<td>3</td>\n<td>B</td>\n<td><strong>2</strong></td>\n</tr>\n<tr>\n<td>4</td>\n<td>B</td>\n<td><strong>2</strong></td>\n</tr>\n<tr>\n<td>5</td>\n<td>C</td>\n<td><strong>1</strong></td>\n</tr>\n<tr>\n<td>6</td>\n<td>C</td>\n<td><strong>1</strong></td>\n</tr>\n<tr>\n<td>7</td>\n<td>D</td>\n<td><strong>2</strong></td>\n</tr>\n<tr>\n<td>...</td>\n<td>...</td>\n<td>...</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>I was trying to find a way to include the division in the loss function</p>\n",
    "is_answered": true,
    "view_count": 198,
    "answer_count": 2
  },
  {
    "title": "I am trying to get top5 products from a list of products",
    "link": "https://stackoverflow.com/questions/76537670/i-am-trying-to-get-top5-products-from-a-list-of-products",
    "tags": [
      "python",
      "pandas",
      "dataframe",
      "machine-learning",
      "data-science"
    ],
    "body": "<p>So i have a data like this</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th style=\"text-align: left;\">Category</th>\n<th style=\"text-align: left;\">subcategory</th>\n<th style=\"text-align: left;\">crn</th>\n<th style=\"text-align: left;\">product1</th>\n<th style=\"text-align: left;\">product2</th>\n<th style=\"text-align: left;\">product3</th>\n<th style=\"text-align: left;\">product4</th>\n<th style=\"text-align: left;\">product5</th>\n<th style=\"text-align: left;\">product6</th>\n<th style=\"text-align: left;\">product7</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\">A</td>\n<td style=\"text-align: left;\">X</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">A</td>\n<td style=\"text-align: left;\">Y</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1 1</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\"></td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">A</td>\n<td style=\"text-align: left;\">Z</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">B</td>\n<td style=\"text-align: left;\">X 1</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\"></td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>And i want to showcase output like this</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th style=\"text-align: left;\">Category</th>\n<th style=\"text-align: left;\">subcategory</th>\n<th style=\"text-align: left;\">crncount</th>\n<th style=\"text-align: left;\">topproduct1</th>\n<th style=\"text-align: left;\">topproduct2</th>\n<th style=\"text-align: left;\">topproduct3</th>\n<th style=\"text-align: left;\">topproduct4</th>\n<th style=\"text-align: left;\">topproduct5</th>\n<th style=\"text-align: left;\">sumofproduct1</th>\n<th style=\"text-align: left;\">sumof product2</th>\n<th style=\"text-align: left;\">sumofprduct3</th>\n<th style=\"text-align: left;\">sumofproduct4</th>\n<th style=\"text-align: left;\">sumofprodct5</th>\n<th style=\"text-align: left;\">sumofproduct6</th>\n<th style=\"text-align: left;\">sumofprodct7</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\">A</td>\n<td style=\"text-align: left;\">x</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">product3</td>\n<td style=\"text-align: left;\">product2</td>\n<td style=\"text-align: left;\">product4</td>\n<td style=\"text-align: left;\">product5</td>\n<td style=\"text-align: left;\">product6</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">A</td>\n<td style=\"text-align: left;\">y</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">product3</td>\n<td style=\"text-align: left;\">product4</td>\n<td style=\"text-align: left;\">product5</td>\n<td style=\"text-align: left;\">product6</td>\n<td style=\"text-align: left;\">product7</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">B</td>\n<td style=\"text-align: left;\">x</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">product1</td>\n<td style=\"text-align: left;\">product3</td>\n<td style=\"text-align: left;\">product4</td>\n<td style=\"text-align: left;\">product5</td>\n<td style=\"text-align: left;\">product6</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">B</td>\n<td style=\"text-align: left;\">y</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">product3</td>\n<td style=\"text-align: left;\">product2</td>\n<td style=\"text-align: left;\">product4</td>\n<td style=\"text-align: left;\">product5</td>\n<td style=\"text-align: left;\">product6</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: left;\">1</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>I have tried this code\ndf.groupby([category, subcategory)]\ngroup.agg({crn: count, product1:sum, product2:sum, product3:sum......)</p>\n<p>Topproduct = df.iloc[:,1:].sum().nlargest(5).index.tolist()</p>\n",
    "is_answered": true,
    "view_count": 127,
    "answer_count": 1
  },
  {
    "title": "How can I convert an onnx model to coreml?",
    "link": "https://stackoverflow.com/questions/76536106/how-can-i-convert-an-onnx-model-to-coreml",
    "tags": [
      "python",
      "pytorch",
      "data-science",
      "onnx",
      "coremltools"
    ],
    "body": "<p>I am trying to convert an onnx model to a .mlmodel for use in ios. My ultimate goal is to use an ml model from Huggingface on ios. So far, the only way to convert a ml model to a .mlmodel file is by using <code>coremltools.converters</code>. I made a script to convert the model from huggingface to onnx, and onnx to mlmodel. It seems to be converting to an onnx fine, but then it runs into an error with coremltools.converters.onnx.convert. After some research, it turns out that you have to use just coremltools.convert, but I am running into a different error.  <code>spec.ParseFromString(f.read()) google.protobuf.message.DecodeError: Error parsing message with type 'CoreML.Specification.Model'</code></p>\n<p>Here's how I am converting from onnx to coreml:</p>\n<pre><code>import coremltools\n# print(coremltools.converters.onnx.convert)\n# Load the ONNX model\nonnx_model_path = &quot;model.onnx&quot;\nonnx_model = coremltools.utils.load_spec(onnx_model_path)\n\n\nmodel = coremltools.convert(onnx_model)\n\n# Set the model's input and output descriptions (optional)\ncoreml_model.input_description[&quot;input&quot;] = &quot;Input image&quot;\ncoreml_model.output_description[&quot;output&quot;] = &quot;Output class label&quot;\n\n# Save the Core ML model to disk\ncoreml_model.save(&quot;model.mlmodel&quot;)\n\n\n</code></pre>\n",
    "is_answered": true,
    "view_count": 7957,
    "answer_count": 1
  },
  {
    "title": "Label a certain x,y data point on a validation curve",
    "link": "https://stackoverflow.com/questions/76534055/label-a-certain-x-y-data-point-on-a-validation-curve",
    "tags": [
      "scikit-learn",
      "data-science",
      "cross-validation"
    ],
    "body": "<pre><code>valid1 = plot_validation_curve(rand_search.best_estimator_, X_train, y_train,\n                               cv=StratifiedKFold(n_splits=5), param_range=np.arange(2,100,2),\n                               param_name = 'max_depth', scoring='f1')\n\n</code></pre>\n<p>I am using the plot_validation_curve fxn from sklearn. Let's say I pick out a spot on the curve that I think is the best. Now I want to confirm the actual x, y values. How do I label the points on the curve itself? Thanks!</p>\n<p>I tried to label data points as one would with matplotlib but it didnt work.</p>\n",
    "is_answered": false,
    "view_count": 53,
    "answer_count": 1
  },
  {
    "title": "How to convert the Non-Monday column to Monday in Pandas Dataframe",
    "link": "https://stackoverflow.com/questions/76528548/how-to-convert-the-non-monday-column-to-monday-in-pandas-dataframe",
    "tags": [
      "python",
      "pandas",
      "dataframe",
      "datetime",
      "data-science"
    ],
    "body": "<p>I want to write a function that if a date is Tue.Wed.Tur. then convert it as this week's Monday, if a date is  Fri. Sat. Sun. then convert it as next week's Monday. If its a Monday, just leave as it is.</p>\n<p>The example input Dataframe is like this:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Model#</th>\n<th>Order Category</th>\n<th>2022/4/18</th>\n<th>2022/5/10</th>\n<th>2022/5/18</th>\n<th>2022/5/26</th>\n<th>2022/6/24</th>\n<th>2022/7/16</th>\n<th>2022/7/24</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>A</td>\n<td>Open</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>1</td>\n<td>1</td>\n<td>2</td>\n</tr>\n<tr>\n<td>B</td>\n<td>Close</td>\n<td>1</td>\n<td>1</td>\n<td>0</td>\n<td>0</td>\n<td>1</td>\n<td>2</td>\n<td>2</td>\n</tr>\n<tr>\n<td>C</td>\n<td>Open</td>\n<td>0</td>\n<td>0</td>\n<td>1</td>\n<td>1</td>\n<td>0</td>\n<td>1</td>\n<td>2</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>The ideal output is like this:</p>\n<p>All the date headers are converted to Monday based on the rules I specified.</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Model#</th>\n<th>Order Category</th>\n<th>2022/4/18</th>\n<th>2022/5/9</th>\n<th>2022/5/16</th>\n<th>2022/5/23</th>\n<th>2022/6/27</th>\n<th>2022/7/18</th>\n<th>2022/7/25</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>A</td>\n<td>Open</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>1</td>\n<td>1</td>\n<td>2</td>\n</tr>\n<tr>\n<td>B</td>\n<td>Close</td>\n<td>1</td>\n<td>1</td>\n<td>0</td>\n<td>0</td>\n<td>1</td>\n<td>2</td>\n<td>2</td>\n</tr>\n<tr>\n<td>C</td>\n<td>Open</td>\n<td>0</td>\n<td>0</td>\n<td>1</td>\n<td>1</td>\n<td>0</td>\n<td>1</td>\n<td>2</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>Really appreciate your help!</p>\n",
    "is_answered": true,
    "view_count": 71,
    "answer_count": 2
  },
  {
    "title": "How to get company earning announcements data API?",
    "link": "https://stackoverflow.com/questions/76528084/how-to-get-company-earning-announcements-data-api",
    "tags": [
      "python",
      "data-science",
      "real-time",
      "finance"
    ],
    "body": "<p>I want to get real-time earning announcements data API.\nI tried yfinance but it doesn't work currently.</p>\n<p>So is there any other replacement API?\nMost of API need to pay money, But I just want to try for my personal projects so it's little pressured</p>\n<p>Does Yahoo Finance no longer offer API? I heard that yfinance is not yahoo finance API.</p>\n<p>I tried yfinance, yahooquery, other API's</p>\n<p>I want real-time earning announcements data api</p>\n",
    "is_answered": true,
    "view_count": 911,
    "answer_count": 1
  },
  {
    "title": "have two dictionary columns with different lengths want to use series explode but it mismatch",
    "link": "https://stackoverflow.com/questions/76521211/have-two-dictionary-columns-with-different-lengths-want-to-use-series-explode-bu",
    "tags": [
      "python",
      "pandas",
      "dataframe",
      "data-science",
      "data-analysis"
    ],
    "body": "<p>I have tow columns in my data frame that have multiple dictionarise in it and I want to expand it into multiple columns but the problem when I use explode series it mismatch</p>\n<p>example:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Column A</th>\n<th>Column B</th>\n<th>Columns C</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Cell 1</td>\n<td><code>{&quot;a&quot;:0.5} {&quot;b&quot;:0.5}</code></td>\n<td><code>{&quot;d1&quot;:0.25}{&quot;d2&quot;:0.25}{&quot;d3&quot;:0.25}{&quot;d4&quot;:0.25}</code></td>\n</tr>\n<tr>\n<td>Cell 2</td>\n<td><code>{&quot;c&quot;:1.0}  </code></td>\n<td><code>{&quot;t1&quot;:0.5} {&quot;t2&quot;:0.5} </code></td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>desirable output:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Column A</th>\n<th>Column B1</th>\n<th>Columns B2</th>\n<th>Columns C1</th>\n<th>Columns C2</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Cell 1</td>\n<td>&quot;a&quot;</td>\n<td>0.5</td>\n<td>&quot;d1&quot;</td>\n<td>0.25</td>\n</tr>\n<tr>\n<td>Cell 1</td>\n<td>&quot;a&quot;</td>\n<td>0.5</td>\n<td>&quot;d2&quot;</td>\n<td>0.25</td>\n</tr>\n<tr>\n<td>Cell 1</td>\n<td>&quot;b&quot;</td>\n<td>0.5</td>\n<td>&quot;d3&quot;</td>\n<td>0.25</td>\n</tr>\n<tr>\n<td>Cell 1</td>\n<td>&quot;b&quot;</td>\n<td>0.5</td>\n<td>&quot;d4&quot;</td>\n<td>0.25</td>\n</tr>\n<tr>\n<td>Cell 2</td>\n<td>&quot;c&quot;</td>\n<td>1.0</td>\n<td>&quot;t1&quot;</td>\n<td>0.5</td>\n</tr>\n<tr>\n<td>Cell 2</td>\n<td>&quot;c&quot;</td>\n<td>1.0</td>\n<td>&quot;t2&quot;</td>\n<td>0.5</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>i used to use</p>\n<pre class=\"lang-py prettyprint-override\"><code>df1 = ( \n       df.assign(Columns B2 =lambda df: df.Columns B.apply(lambda x: x.values()))    \n     \n       .reset_index()\n       .apply(pd.Series.explode)\n     \n            )\noutput = ( \n       df1.assign(Columns C2 =lambda df: df.Columns C.apply(lambda x: x.values()))    \n     \n       .reset_index()\n       .apply(pd.Series.explode)\n     \n            )\n</code></pre>\n<p>the problem is that it mismatches the rows something in column C1 supposed to be in row 2 go to 3</p>\n",
    "is_answered": true,
    "view_count": 54,
    "answer_count": 1
  },
  {
    "title": "Time column and DateTime Format (DateTime object vs String Object)",
    "link": "https://stackoverflow.com/questions/76505802/time-column-and-datetime-format-datetime-object-vs-string-object",
    "tags": [
      "python",
      "data-science",
      "amibroker"
    ],
    "body": "<p>How can I change the time without turning it into a string object (which dt.strftime does)? Like if I want to turn (default time format and time object)%H:%M:%S into %I:%M:%S %p or turn %H:%M:99(Default under time column in csv file data) into %H:%M:00?</p>\n<p>Strftime helps resolve the problem, but it also results in data conversion (from datetime format to a string format) as per my instructor.</p>\n",
    "is_answered": false,
    "view_count": 68,
    "answer_count": 1
  },
  {
    "title": "Combine two Pandas rows into one with duplicated columns for time series",
    "link": "https://stackoverflow.com/questions/76502238/combine-two-pandas-rows-into-one-with-duplicated-columns-for-time-series",
    "tags": [
      "python",
      "pandas",
      "dataframe",
      "data-science",
      "data-manipulation"
    ],
    "body": "<p>I have the following problem that I am trying to solve. I have two Pandas Dataframe rows with the same columns:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Column A</th>\n<th>Column B</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Cell 1</td>\n<td>Cell 2</td>\n</tr>\n<tr>\n<td>Cell 3</td>\n<td>Cell 4</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>I want to combine both rows into one single row by appending the columns:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Column A_1</th>\n<th>Column B_1</th>\n<th>Column A_2</th>\n<th>Column B_2</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Cell 1</td>\n<td>Cell 2</td>\n<td>Cell 3</td>\n<td>Cell 4</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>This operation is used to create a time series row with window size 2 for training a machine learning model. Therefore, I am doing this operation millions of times which should require a small operational cost.</p>\n<p>Thanks in advance!</p>\n<p>I tried using pandas concat but is is just too slow and requires a lot of ram</p>\n",
    "is_answered": true,
    "view_count": 86,
    "answer_count": 4
  },
  {
    "title": "Why Remove Trend and Seasonality in Time Series Forecasting?",
    "link": "https://stackoverflow.com/questions/76497300/why-remove-trend-and-seasonality-in-time-series-forecasting",
    "tags": [
      "python",
      "time-series",
      "data-science",
      "forecasting"
    ],
    "body": "<p>I am struggling to understand why we need to remove trend and seasonality components from non-stationary time series data when performing time series forecasting in Python. Won't removing these components affect the accuracy of the forecasted data, since we actually want to retain trend and seasonality in the final output? Although I realize that we must first make the data stationary to apply models like ARIMA and SARIMAX, I am unsure whether these models will still be able to predict future trend and seasonality accurately. I have researched this extensively on Google and on YouTube but have not found a satisfactory answer yet.</p>\n",
    "is_answered": true,
    "view_count": 3416,
    "answer_count": 2
  },
  {
    "title": "How to fix unexpected KeyError while mapping DataFrame column value to dictionary within loop in Python?",
    "link": "https://stackoverflow.com/questions/76493683/how-to-fix-unexpected-keyerror-while-mapping-dataframe-column-value-to-dictionar",
    "tags": [
      "python",
      "dataframe",
      "dictionary",
      "data-science",
      "facebook-prophet"
    ],
    "body": "<p>I'm trying to conduct a multivariate time series analysis using Facebook's Prophet library in Python. My goal is to predict the circumference of various muscles of an athlete over time.</p>\n<p>My data consists of measurements of different muscles (such as 'Neck', 'Arm', 'Shoulder', 'Chest', etc.) along with their corresponding timestamps.</p>\n<p>Firstly, I calculated correlations between different muscle measurements to use as regressors in the model. Here's an example of my code for this:</p>\n<pre><code>df_selected=df[[ 'Taille', 'Neck', 'Arm', 'Shoulder', 'Chest', 'Waist', 'Hips','Leg']]\n\n# Calculate correlation matrix\ncorrelation_matrix = df_selected.corr()\n\n# Prepare a dictionary to store results\ncorrelation_dict = {}\nregressors={}\n# For each target variable\nfor target in df_selected.columns:\n    # Find variables that are strongly correlated with target\n    strong_correlations = correlation_matrix[target].abs() &gt; 0.40\n\n    # Filter down to just these variables\n    strong_correlators = strong_correlations[strong_correlations].index.tolist()\n\n    # Remove the target from the list, as it's the variable of interest\n    if target in strong_correlators:\n        strong_correlators.remove(target)\n    \n        # Remove the target from the list, as it's the variable of interest\n    if target in strong_correlators:\n        strong_correlators.remove(target)\n\n    # Add to dictionary only if there are strongly correlated variables\n    if strong_correlators:\n        regressor[target] = strong_correlators\n\n# Print the correlation dictionary\nfor target, correlators in regressor.items():\n    print(f&quot;{target}: {correlators}&quot;)\n</code></pre>\n<p>Next, I created a function that takes a data group for a specific muscle and a dictionary of regressors. This function initializes a Prophet model, adds relevant regressors to the model, trains the model on the given data, and returns forecasted results:</p>\n<pre><code>def train_and_forecast(group, muscle, regressors):\n    # Initiate the model\n    m2 = Prophet()\n    \n    # Get the relevant regressors for the current muscle\n    relevant_regressors = regressors.get(muscle, [])\n\n    # Filter the group data to include only relevant columns\n    necessary_columns = ['ds', 'y'] + relevant_regressors\n    filtered_group = group[necessary_columns].copy()\n    filtered_group['Muscle'] = muscle  # Add the 'Muscle' column back\n\n    # Add each relevant regressor to the model\n    for regressor in relevant_regressors:\n        m2.add_regressor(regressor)\n\n    # Fit the model\n    m2.fit(filtered_group)\n    \n    # Make predictions\n    future = m2.make_future_dataframe(periods=45)\n    forecast = m2.predict(future)[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]\n    forecast['Muscle'] = muscle\n    \n    # Return the forecasted results\n    return forecast[['ds', 'Muscle', 'yhat', 'yhat_upper', 'yhat_lower']]\n</code></pre>\n<p>Finally, I iterated over each muscle, retrieved the relevant data group, and called the forecasting function. The returned forecast is concatenated to a DataFrame which holds the forecasts for all muscles:</p>\n<pre class=\"lang-py prettyprint-override\"><code>necessary_columns = ['ds', 'Muscle', 'y'] + regressors[Muscle]\n# Create an empty dataframe\nfor_loop_forecast = pd.DataFrame()\n# Loop through each Muscle\nfor Muscle in Muscle_list:\n  # Get the data for the mucle\n  group = group[necessary_columns]\n  # Make forecast\n  forecast = train_and_forecast(group,regressor)\n  # Add the forecast results to the dataframe\n  for_loop_forecast = pd.concat((for_loop_forecast, forecast))\n\n# Take a look at the data\nfor_loop_forecast.head()\n</code></pre>\n<p>And then it crash</p>\n<pre class=\"lang-py prettyprint-override\"><code>\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nCell In[259], line 1\n----&gt; 1 necessary_columns = ['ds', 'Muscle', 'y'] + regressors[Muscle]\n      2 # Create an empty dataframe\n      3 for_loop_forecast = pd.DataFrame()\n\nKeyError: 'Neck'\n</code></pre>\n<p>However, I'm encountering a KeyError in the line where I try to determine the necessary_columns for the current group of data. The error message suggests that 'Neck' is not found in my regressors dictionary, although it should be.</p>\n<p>I suspect that there might be a problem in the way I'm integrating these dynamic regressors with the Prophet model, or perhaps an issue with my data manipulation before the forecasting step.</p>\n<p>How to fix this?</p>\n",
    "is_answered": false,
    "view_count": 62,
    "answer_count": 0
  },
  {
    "title": "template.serveable() not working as expected in Python panel",
    "link": "https://stackoverflow.com/questions/76492815/template-serveable-not-working-as-expected-in-python-panel",
    "tags": [
      "pandas",
      "web-applications",
      "data-science",
      "panel",
      "hvplot"
    ],
    "body": "<p>I have the four <a href=\"https://en.wikipedia.org/wiki/Comma-separated_values\" rel=\"nofollow noreferrer\">CSV</a> files stored in local. Each corresponds to some store details which has ItemNumber, ItemName, ItemBrand, and ItemCost.</p>\n<p>I am using a Python panel to display the CSV files in a browser after converting the CSV as an interactive data frame. When I execute the code, the app is launched in browser. But when I change the dropdown value, the contents of the webpage is not updated. The error says:</p>\n<blockquote>\n<p>2023-06-16 23:38:24,500 ERROR: panel.reactive - Callback failed for object named &quot;Select File Name&quot; changing property {'value': 'File2'} . OSError: [WinError 10048] Only one usage of each socket address (protocol/network address/port) is normally permitted</p>\n</blockquote>\n<p>I have added some logic to stop the template and start it with the new content each time the drop down value is changed, to prevent the simultaneous use of same port number. But somehow , it is not working. Please help me to fix this issue:</p>\n<pre><code>    #!/usr/bin/env python3\n\n    import pandas as pd\n    from pathlib import Path\n    import panel as pn\n    import hvplot.pandas\n\n    fileName = &quot;File1&quot;\n    df_file_name = fileName + &quot;_df.csv&quot;\n    df_filepath = Path('C:\\\\Users\\\\user1\\\\MyPythonApp\\\\' + df_file_name)\n    serveable = None\n\n    def dfToDisplay(file):\n        global serveable  # Access the 'serveable' variable\n\n        # Stop the previous 'servable' if it exists\n        if serveable is not None:\n            serveable.stop()\n            time.sleep(10)\n        if type(file) == str:\n            filenm = file\n        else:\n            filenm = file[-2]\n\n        df_path = Path(&quot;C:\\\\Users\\\\user1\\\\MyPythonApp\\\\&quot; + filenm + &quot;_df.csv&quot;)\n        displayDF = pd.read_csv(df_path)\n        interactivedf = displayDF.interactive()\n        itable = interactivedf.pipe(pn.widgets.Tabulator, pagination='local', page_size=50, layout='fit_data', theme='default', disabled=True, widths={'index': '70', 'ItemNumber': '130', 'ItemName': '200', 'ItemBrand': '150', 'ItemCost': '900'})\n        template = pn.template.FastListTemplate(title='Blah blah Details', sidebar=['Filename', dropdown], logo='https://abccorp.com/logo.svg', main=[itable.panel()])\n        serveable = pn.serve(template, port=20001, show=False)\n\n    dropdown = pn.widgets.Select(name='Select File Name', options=['File1', 'File2', 'File3', 'File4'])\n    dropdown.param.watch(dfToDisplay, 'value')\n    dfToDisplay(fileName)\n</code></pre>\n<p><a href=\"https://i.sstatic.net/Fwngr.png\" rel=\"nofollow noreferrer\">Application screenshot</a></p>\n<p>I tried it using template.show() method. In this case, each time a new server is launched in a random port numbers. But I need the data across different values of dropdown to be displayed on a fixed port number.</p>\n",
    "is_answered": false,
    "view_count": 593,
    "answer_count": 0
  },
  {
    "title": "How to increase pandas explode function performance?",
    "link": "https://stackoverflow.com/questions/76491187/how-to-increase-pandas-explode-function-performance",
    "tags": [
      "python",
      "pandas",
      "dataframe",
      "numpy",
      "data-science"
    ],
    "body": "<p>I have data as below in a dataframe</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>FID</th>\n<th>SID_START</th>\n<th>SID_END</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>404915</td>\n<td>1</td>\n<td>3</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>and this should be expanded as below</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>FID</th>\n<th>SID</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>404915</td>\n<td>1</td>\n</tr>\n<tr>\n<td>404915</td>\n<td>2</td>\n</tr>\n<tr>\n<td>404915</td>\n<td>3</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>So I can group by SID to get the count</p>\n<p>I have around  480 million rows and I am using explode function in pandas</p>\n<pre class=\"lang-py prettyprint-override\"><code>df['SID'] = [pd.Series(range(left,right+1)) for left, right in \n             zip(df['SID_START'],df['SID_END'])]\n\n\ndf= df.explode('SID').drop(['SID_START', 'SID_END'], axis=1)\n</code></pre>\n<p>and its taking around 10 minutes for 10 million records is there a fasetr way in python to handle this\n?</p>\n",
    "is_answered": true,
    "view_count": 302,
    "answer_count": 1
  },
  {
    "title": "I am getting &quot;Key Error&quot; Exception in Python",
    "link": "https://stackoverflow.com/questions/76488380/i-am-getting-key-error-exception-in-python",
    "tags": [
      "python",
      "arrays",
      "pandas",
      "image",
      "data-science"
    ],
    "body": "<p><em>Getting <strong>Key Error 0</strong> in the following code where some_digit = X[0]</em></p>\n<pre><code>import pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\n#import the dataset\nfrom sklearn.datasets import fetch_openml\nmnist = fetch_openml('mnist_784', version = 1)\nmnist.keys()\n\n#shape of the data\nX, y = mnist[&quot;data&quot;], mnist[&quot;target&quot;]\nX.shape\n\n#there are 70000 images and each has 784 features because each image has 28*28 pixel from which we are displaying one image\n\nsome_digit = X[0]\nsome_digit_image = some_digit.reshape(28,28)\nplt.imshow(some_digit_image, cmap=&quot;binary&quot;)\nplt.axis(&quot;off&quot;)\nplt.show()\n</code></pre>\n<p><strong>The dataset contains 7000 images of 28 * 28 pixels each. I am trying to fetch an image at location X[0].</strong>\nplease help</p>\n<p><a href=\"https://i.sstatic.net/gWru7.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/gWru7.png\" alt=\"enter image description here\" /></a></p>\n",
    "is_answered": true,
    "view_count": 209,
    "answer_count": 1
  },
  {
    "title": "How does one create a pandas column (binary) based on whether or not another column contains dates",
    "link": "https://stackoverflow.com/questions/76485562/how-does-one-create-a-pandas-column-binary-based-on-whether-or-not-another-col",
    "tags": [
      "python",
      "pandas",
      "dataframe",
      "data-science"
    ],
    "body": "<p>I am analyzing customer churn. My dataset contains years of customers that have stayed or left. I need to create a ['CHURN_FLAG'] column based on whether or not there is a date in the ['CHURN_DATE'] column. If there is a date in the ['CHURN_DATE'] column then the customer churned.</p>\n<p>Current data frame:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>CHURNdate</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>2023-1-1</td>\n</tr>\n<tr>\n<td>NaT</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>Desired:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>CHURNdate</th>\n<th>CHURNflag</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>2023-1-1</td>\n<td>1</td>\n</tr>\n<tr>\n<td>NaT</td>\n<td>0</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>I created a column ['TODAY_DATE'] and have attempted to solve by assessing if the ['CHURNdate'] &lt; ['TODAY_DATE'] then the binary 1 would populate, else 0. Here is the code:</p>\n<pre><code>df2['CHURNflag'] = np.where(df2['CHURNdate']&lt;df2['TODAY_DATE'], 0, 1)\n</code></pre>\n<p>Naturally, it didn't work. :( The datatypes are datetime64</p>\n",
    "is_answered": true,
    "view_count": 52,
    "answer_count": 1
  },
  {
    "title": "Pandas groupby(pd.Grouper) is throwing error for datetime but im running it on a datetime object",
    "link": "https://stackoverflow.com/questions/76479625/pandas-groupbypd-grouper-is-throwing-error-for-datetime-but-im-running-it-on-a",
    "tags": [
      "python",
      "pandas",
      "dataframe",
      "datetime",
      "data-science"
    ],
    "body": "<p>I'm using pandas in python, and am trying to group a set of dates by month, and determine the highest value in the dates_and_grades[&quot;Grade_Values&quot;] column for each month. I wrote the following code attempting to do this:</p>\n<pre><code>data = pd.read_csv(input_filepath)\ndata['Date'] = pd.to_datetime(data['Date'], format = 'ISO8601')\n\nroped = [&quot;Sport&quot;, &quot;Trad&quot;]\n\nYDS_DICT={&quot;N/A&quot;:&quot;N/A&quot;,'3-4':0,'5':1,'5.0':1,'5.1':2,'5.2':3,'5.3':4,'5.4':5,\n      '5.5':6,'5.6':7,'5.7':8,'5.8':9,'5.9':10,\n      '5.10a':11,'5.10b':12, '5.10': 12, '5.10c':13,'5.10d':14,\n      '5.11a':15,'5.11b':16, '5.11':16, '5.11c':17,'5.11d':18,\n      '5.12a':19,'5.12b':20,'5.12c':21,'5.12d':22,\n      '5.13a':23,'5.13b':24,'5.13c':25,'5.13d':26,\n      '5.14a':27,'5.14b':28,'5.14c':29,'5.14d':30,\n      '5.15a':31,'5.15b':32,'5.15c':33,'5.15d':34}\n\nroped_only_naive = data.loc[data['Route Type'].isin(roped)].copy()\nroped_only_naive[&quot;Rating&quot;] = roped_only_naive['Rating'].map(slash_grade_converter)\nroped_only_naive[&quot;Rating&quot;] = roped_only_naive['Rating'].map(flatten_plus_and_minus_grades)\nroped_only_naive[&quot;Rating&quot;] = roped_only_naive['Rating'].map(remove_risk_ratings)\ndates_and_grades = roped_only_naive[['Date', 'Rating']]\nprint(dates_and_grades.dtypes)\ndates_and_grades[&quot;Grade_Values&quot;] = dates_and_grades[&quot;Rating&quot;].map(lambda data: YDS_DICT[data])\nprint(dates_and_grades.dtypes)\ndates_and_grades['Date'] = dates_and_grades['Date'].groupby(pd.Grouper(freq='M'))\nprint(dates_and_grades)\n</code></pre>\n<p>However, I get the following error when run.</p>\n<pre><code>TypeError: Only valid with DatetimeIndex, TimedeltaIndex or PeriodIndex, but got an instance of 'Index'\n</code></pre>\n<p>What is strange is that when I check the types on my dataframe using</p>\n<pre><code>print(dates_and_grades.dtypes)\n</code></pre>\n<p>I get the following printout</p>\n<pre><code>Date            datetime64[ns]\nRating                  object\nGrade_Values             int64\n</code></pre>\n<p>So it looks like my Date column is indeed a datetime object.</p>\n<p>My question is then, why doesn't the groupby(pd.Grouper(freq='M')) function work on my dates_and_grades['Date'] column if it does seem like dates_and_grades['Date'] is actually a datetime type?</p>\n",
    "is_answered": true,
    "view_count": 436,
    "answer_count": 1
  },
  {
    "title": "Why does the KS-Test give a p-value of 1 if the distribution is different",
    "link": "https://stackoverflow.com/questions/76474969/why-does-the-ks-test-give-a-p-value-of-1-if-the-distribution-is-different",
    "tags": [
      "python",
      "statistics",
      "data-science",
      "kolmogorov-smirnov"
    ],
    "body": "<p>Let's take two sets:</p>\n<pre><code>a = [5,5,5,5,5,4,4,4,4,3,3,3,2,2,1]\nb = [5,4,3,2,1]\n</code></pre>\n<p>We perform the KS-Test using Python:</p>\n<pre><code>from scipy import stats\nstats.ks_2samp(b,a)\nKstestResult(statistic=0.2, pvalue=0.9979360165118678, statistic_location=2, statistic_sign=1)\n</code></pre>\n<p>Why is the result a p-value of 0.9979? This means that the distribution of the values in the two sets is almost identical. But it's not! What do I missunderstand?</p>\n<p>Kind regards.</p>\n",
    "is_answered": true,
    "view_count": 1571,
    "answer_count": 2
  },
  {
    "title": "How can I get history subscriber count in Youtube? E.g. How to see what is the subscriber count at the end of 2022 vs 2023",
    "link": "https://stackoverflow.com/questions/76474620/how-can-i-get-history-subscriber-count-in-youtube-e-g-how-to-see-what-is-the-s",
    "tags": [
      "google-api",
      "youtube-api",
      "data-science",
      "data-analysis",
      "youtube-data-api"
    ],
    "body": "<p>Apologies, if my question is unclear as I am new to this space.</p>\n<p>I am trying to do a data analysis project for my portfolio about youtube channel.</p>\n<p>I would like to compare the youtube channel growth over the years.</p>\n<p>Is there anyway to get history data?</p>\n<p>I can only get the current subscriber count when I call the API.\n<a href=\"https://developers.google.com/youtube/v3/docs/subscriptions/list\" rel=\"nofollow noreferrer\">https://developers.google.com/youtube/v3/docs/subscriptions/list</a></p>\n",
    "is_answered": false,
    "view_count": 193,
    "answer_count": 0
  },
  {
    "title": "ModuleNotFoundError: No module named &#39;tapas.utils&#39;",
    "link": "https://stackoverflow.com/questions/76473828/modulenotfounderror-no-module-named-tapas-utils",
    "tags": [
      "machine-learning",
      "data-science",
      "artificial-intelligence"
    ],
    "body": "<p>I am getting the above error while trying to import these method. I am using Collab.</p>\n<p>from tapas.utils import tf_example_utils</p>\n<p>from tapas.protos import interaction_pb2</p>\n<p>from tapas.utils import number_annotation_utils</p>\n<p>from tapas.scripts import prediction_utils</p>\n<p><strong>I tried to install utils but it didn't work</strong></p>\n",
    "is_answered": false,
    "view_count": 173,
    "answer_count": 0
  },
  {
    "title": "Difference between Word2Vec and contextual embedding",
    "link": "https://stackoverflow.com/questions/76471584/difference-between-word2vec-and-contextual-embedding",
    "tags": [
      "machine-learning",
      "deep-learning",
      "nlp",
      "data-science"
    ],
    "body": "<p>am trying to understand the difference between word embedding and contextual embedding.</p>\n<p>below is my understanding, please add if you find any corrections.</p>\n<p>word embedding algorithm has a global vocabulary (dictionary) of words. when we are performing word2vec then the input corpus(unique words) maps with the global dictionary and it will return the embeddings.</p>\n<p>contextual embedding is used to learn sequence-level semantics by considering the sequence of all words in the documents.</p>\n<p>but I don't understand where we considered the context in a word embedding.</p>\n",
    "is_answered": false,
    "view_count": 1676,
    "answer_count": 2
  },
  {
    "title": "How do I combine my dataframes while iterating through rows and adding a new column everytime",
    "link": "https://stackoverflow.com/questions/76469276/how-do-i-combine-my-dataframes-while-iterating-through-rows-and-adding-a-new-col",
    "tags": [
      "python",
      "pandas",
      "dataframe",
      "csv",
      "data-science"
    ],
    "body": "<p>I have a raw dataframe, similar to below:</p>\n<h4>Original DF I have</h4>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>index</th>\n<th>text</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0</td>\n<td>i am happy today ...</td>\n</tr>\n<tr>\n<td>1</td>\n<td>i am confused because ...</td>\n</tr>\n<tr>\n<td>2</td>\n<td>i would love to do ...</td>\n</tr>\n<tr>\n<td>...</td>\n<td>...</td>\n</tr>\n<tr>\n<td>1000000</td>\n<td>i am exhausted about ...</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>So I have to run all these texts through different models which each produce a score. Thereafter, I need to combine them into one dataframe as below:</p>\n<h4>Processed DF i want</h4>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>index</th>\n<th>text</th>\n<th>score_1</th>\n<th>score_2</th>\n<th>score 3</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0</td>\n<td>i am happy today ...</td>\n<td>0.2</td>\n<td>0.4</td>\n<td>0.238</td>\n</tr>\n<tr>\n<td>1</td>\n<td>i am confused because ...</td>\n<td>0.8</td>\n<td>0.3</td>\n<td>0.64</td>\n</tr>\n<tr>\n<td>2</td>\n<td>i would love to do ...</td>\n<td>0.67</td>\n<td>0.546</td>\n<td>0.35</td>\n</tr>\n<tr>\n<td>...</td>\n<td>...</td>\n<td>...</td>\n<td>...</td>\n<td>...</td>\n</tr>\n<tr>\n<td>1000000</td>\n<td>i am exhausted about ...</td>\n<td>0.21</td>\n<td>0.41</td>\n<td>0.8</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>So i have to load individual models for each one (which isn't instant) and because there are so many rows, I have to split it up into batches (of 100 for example). After that I have to combine my dataframes. My code is something like this:</p>\n<pre><code>full_df = pd.read_csv('fulldf.csv')\nbatch_size = 100\nnum_batches = len(full_df)/100 # assume it's a round number\ndf_list = []\nnew_df = []\n\nfor i in range(num_batches): \n    # Breaking up the main dataframe\n    df_list.append(full_df.iloc[i*batch_size:(i+1)*batch_size]\n\nfor model in list_of_models:\n    model.load() # Time consuming step so I only do it once per model\n    for df in df_list:\n        df = df.reset_index()\n        # Some code to generate scores for each row of df subset\n        df['score_' + model_number] = score\n        df.reset_index(drop = True, inplace = True)\n        new_df.append(df)\n\ntotal_df = pd.concat(new_df)\n</code></pre>\n<p>However, the results appear somewhat incorrectly.</p>\n<h4>DF I am getting with above code</h4>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>index</th>\n<th>text</th>\n<th>score_1</th>\n<th>score_2</th>\n<th>score 3</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0</td>\n<td>i am happy today ...</td>\n<td>0.2</td>\n<td>NA</td>\n<td>NA</td>\n</tr>\n<tr>\n<td>1</td>\n<td>i am confused because ...</td>\n<td>0.8</td>\n<td>NA</td>\n<td>NA</td>\n</tr>\n<tr>\n<td>...</td>\n<td>...</td>\n<td>...</td>\n<td>...</td>\n<td>...</td>\n</tr>\n<tr>\n<td>1000000</td>\n<td>i am exhausted about ...</td>\n<td>0.21</td>\n<td>NA</td>\n<td>NA</td>\n</tr>\n<tr>\n<td>0</td>\n<td>i am happy today ...</td>\n<td>NA</td>\n<td>0.4</td>\n<td>NA</td>\n</tr>\n<tr>\n<td>1</td>\n<td>i am confused because ...</td>\n<td>NA</td>\n<td>0.3</td>\n<td>NA</td>\n</tr>\n<tr>\n<td>...</td>\n<td>...</td>\n<td>...</td>\n<td>...</td>\n<td>...</td>\n</tr>\n<tr>\n<td>1000000</td>\n<td>i am exhausted about ...</td>\n<td>NA</td>\n<td>0.41</td>\n<td>NA</td>\n</tr>\n<tr>\n<td>0</td>\n<td>i am happy today ...</td>\n<td>NA</td>\n<td>NA</td>\n<td>0.238</td>\n</tr>\n<tr>\n<td>1</td>\n<td>i am confused because ...</td>\n<td>NA</td>\n<td>NA</td>\n<td>0.64</td>\n</tr>\n<tr>\n<td>...</td>\n<td>...</td>\n<td>...</td>\n<td>...</td>\n<td>...</td>\n</tr>\n<tr>\n<td>1000000</td>\n<td>i am exhausted about ...</td>\n<td>NA</td>\n<td>NA</td>\n<td>0.8</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>As you can see, the numbers are correctly aligned to the index but the rows basically repeat 3 times (or more times if there are more 'scores').</p>\n<p>I have the constraints that cannot load all the rows into memory at once so I have to do them in batches. Moreover, I cannot load a model, do 100 rows, then load another model and do the same 100 rows again as this takes too long due to model loading time).</p>\n<p>I Have tried several solutions, such as adding `total_df = pd.concat(new_df, axis = 1) to the concat, but that doesn't work as it just appends sideways.</p>\n<p>Is there any way to fix this and get the desired result?</p>\n",
    "is_answered": true,
    "view_count": 53,
    "answer_count": 2
  },
  {
    "title": "Webscraping images from google images",
    "link": "https://stackoverflow.com/questions/76467917/webscraping-images-from-google-images",
    "tags": [
      "python",
      "web-scraping",
      "beautifulsoup",
      "data-science"
    ],
    "body": "<p>I have recently been trying to scrape images of a specific type from Google through beautiful soup, but most of the markup seems hidden or not displayed when fetching the data from the URL. But when I inspect the URL I am able to see everything. Is there anything Google is doing to prevent scraping or am I making a mistake somewhere? Here is the code:</p>\n<pre><code>from bs4 import BeautifulSoup\nimport numpy as np\nimport requests\n\nurl=&quot;https://www.google.com/search?site=&amp;tbm=isch&amp;source=hp&amp;biw=1873&amp;bih=990&amp;&quot;\n# u_agent={\n#     'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'\n#     # 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n#     # 'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n#     # 'Accept-Encoding': 'none',\n#     # 'Accept-Language': 'en-US,en;q=0.8',\n#     # 'Connection': 'keep-alive'\n# }\npage=requests.get(url + 'q='+'person-inside-car')\nsoup=BeautifulSoup(page.text, 'html.parser')\nprint(soup)\n</code></pre>\n<p>I have commented on that header definition as that was giving me a very bizarre result when used.\nthe result when I used the header:\n<a href=\"https://i.sstatic.net/u7zOH.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/u7zOH.png\" alt=\"enter image description here\" /></a></p>\n<p>I was expecting image hypertext with a large number of links for images.\nFor info, none of the body is being lost due to js, i have already checked by disabling the js on browser, the entire hypertext is visible.</p>\n<p>One more thing i noticed was the URL is just retrieving info according to the amount of div's I'm inspecting in the website. The more div's i inspect the more information i am retrieving. please guide on this</p>\n",
    "is_answered": false,
    "view_count": 507,
    "answer_count": 1
  },
  {
    "title": "ValueError: Need 2 classes but I have 0",
    "link": "https://stackoverflow.com/questions/76465909/valueerror-need-2-classes-but-i-have-0",
    "tags": [
      "python",
      "dataframe",
      "machine-learning",
      "scikit-learn",
      "data-science"
    ],
    "body": "<p>I need two classes but I have just one class so I got ValueError:</p>\n<p>For Logistic Regression to understand, I converted string data types to integer data type. While there are Abnormal and Normal string values in the Class column, I updated them to 1 and 0. When I tried to fit Logistic Regression it gave an error. It says I need at least 2 classes but I have 1 class. I can't go back to the master data because I didn't copy the data before how can i fix it ?</p>\n<pre><code>from sklearn.linear_model import LogisticRegression\n</code></pre>\n<pre><code>data[&quot;class&quot;] = [1 if each == &quot;Abnormal&quot; else 0 for each in data [&quot;class&quot;]]\n</code></pre>\n<pre><code>data.head()\n</code></pre>\n<pre><code>lr = LogisticRegression()\nlr.fit(x_train.T, y_train.T)\n</code></pre>\n<pre><code>ValueError                                Traceback (most recent call last)\n~\\AppData\\Local\\Temp\\ipykernel_16508\\1381464910.py in &lt;module&gt;\n      1 #E\u011fitim\n      2 lr = LogisticRegression()\n----&gt; 3 lr.fit(x_train.T, y_train.T)\n\n~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py in fit(self, X, y, sample_weight)\n   1552         classes_ = self.classes_\n   1553         if n_classes &lt; 2:\n-&gt; 1554             raise ValueError(\n   1555                 &quot;This solver needs samples of at least 2 classes&quot;\n   1556                 &quot; in the data, but the data contains only one&quot;\n\nValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n</code></pre>\n<p>I wanted to return to the original data, but I cannot return to the main data because I did not copy the master data before. I tried changing the data in the Class column to 1 and 0 but they are all corrected to 1 i.e. Abnormal.</p>\n",
    "is_answered": false,
    "view_count": 401,
    "answer_count": 2
  },
  {
    "title": "unable to install lap==0.4.0 library in python",
    "link": "https://stackoverflow.com/questions/76463707/unable-to-install-lap-0-4-0-library-in-python",
    "tags": [
      "python",
      "machine-learning",
      "data-science",
      "object-detection",
      "cvzone"
    ],
    "body": "<p>I was trying to install lap for object detection but i cloud not do it.</p>\n<p>I have try to installed it on python version 9 , 10 and 11 also but i cant do it</p>\n<p>I am learning object detection from\nMurtaza's Workshop - Robotics and AI.</p>\n<p>or is  there any way to fix this problem</p>\n<pre><code>C:\\\\Users\\\\ACER\\\\Desktop\\\\Object-Detection-Yolo\\\\venv39\\\\Scripts\\\\activate.bat\npip install lap==0.4.0\n</code></pre>\n<p>the error:</p>\n<pre><code>Collecting lap==0.4.0\n  Using cached lap-0.4.0.tar.gz (1.5 MB)\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nBuilding wheels for collected packages: lap\n  Building wheel for lap (setup.py): started\n  Building wheel for lap (setup.py): finished with status 'error'\n  Running setup.py clean for lap\nFailed to build lap\nInstalling collected packages: lap\n  Running setup.py install for lap: started\n  Running setup.py install for lap: finished with status 'error'\n\n  error: subprocess-exited-with-error\n  \n  python setup.py bdist_wheel did not run successfully.\n  exit code: 1\n  \n  [39 lines of output]\n  Partial import of lap during the build process.\n  C:\\Users\\ACER\\AppData\\Local\\Temp\\pip-install-a82najcq\\lap_e14dabff97d544e59d3633db1f44d15e\\setup.py:223: DeprecationWarning:\n  \n    `numpy.distutils` is deprecated since NumPy 1.23.0, as a result\n    of the deprecation of `distutils` itself. It will be removed for\n    Python &gt;= 3.12. For older Python versions it will remain present.\n    It is recommended to use `setuptools &lt; 60.0` for those Python versions.\n    For more details, see:\n      https://numpy.org/devdocs/reference/distutils_status_migration.html\n  \n  \n    from numpy.distutils.core import setup\n  Generating cython files\n  running bdist_wheel\n  running build\n  running config_cc\n  INFO: unifing config_cc, config, build_clib, build_ext, build commands --compiler options\n  running config_fc\n  INFO: unifing config_fc, config, build_clib, build_ext, build commands --fcompiler options\n  running build_src\n  INFO: build_src\n  INFO: building extension &quot;lap._lapjv&quot; sources\n  INFO: building data_files sources\n  INFO: build_src: building npy-pkg config files\n  C:\\Users\\ACER\\Desktop\\Object-Detection-Yolo\\venv39\\lib\\site-packages\\setuptools\\command\\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n    warnings.warn(\n  running build_py\n  creating build\n  creating build\\lib.win-amd64-cpython-39\n  creating build\\lib.win-amd64-cpython-39\\lap\n  copying lap\\lapmod.py -&gt; build\\lib.win-amd64-cpython-39\\lap\n  copying lap\\__init__.py -&gt; build\\lib.win-amd64-cpython-39\\lap\n  running build_ext\n  INFO: No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils\n  INFO: customize MSVCCompiler\n  INFO: customize MSVCCompiler using build_ext\n  INFO: CCompilerOpt.cc_test_flags[1077] : testing flags (/O2)\n  error: Microsoft Visual C++ 14.0 or greater is required. Get it with &quot;Microsoft C++ Build Tools&quot;: https://visualstudio.microsoft.com/visual-cpp-build-tools/\n  INFO: CCompilerOpt.cache_flush[857] : write cache to path -&gt; C:\\Users\\ACER\\AppData\\Local\\Temp\\pip-install-a82najcq\\lap_e14dabff97d544e59d3633db1f44d15e\\build\\temp.win-amd64-cpython-39\\Release\\ccompiler_opt_cache_ext.py\n  [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\n  ERROR: Failed building wheel for lap\n  error: subprocess-exited-with-error\n  \n  Running setup.py install for lap did not run successfully.\n  exit code: 1\n  \n  [39 lines of output]\n  Partial import of lap during the build process.\n  C:\\Users\\ACER\\AppData\\Local\\Temp\\pip-install-a82najcq\\lap_e14dabff97d544e59d3633db1f44d15e\\setup.py:223: DeprecationWarning:\n  \n    `numpy.distutils` is deprecated since NumPy 1.23.0, as a result\n    of the deprecation of `distutils` itself. It will be removed for\n    Python &gt;= 3.12. For older Python versions it will remain present.\n    It is recommended to use `setuptools &lt; 60.0` for those Python versions.\n    For more details, see:\n      https://numpy.org/devdocs/reference/distutils_status_migration.html\n  \n  \n    from numpy.distutils.core import setup\n  Generating cython files\n  running install\n  C:\\Users\\ACER\\Desktop\\Object-Detection-Yolo\\venv39\\lib\\site-packages\\setuptools\\command\\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n    warnings.warn(\n  running build\n  running config_cc\n  INFO: unifing config_cc, config, build_clib, build_ext, build commands --compiler options\n  running config_fc\n  INFO: unifing config_fc, config, build_clib, build_ext, build commands --fcompiler options\n  running build_src\n  INFO: build_src\n  INFO: building extension &quot;lap._lapjv&quot; sources\n  INFO: building data_files sources\n  INFO: build_src: building npy-pkg config files\n  running build_py\n  creating build\n  creating build\\lib.win-amd64-cpython-39\n  creating build\\lib.win-amd64-cpython-39\\lap\n  copying lap\\lapmod.py -&gt; build\\lib.win-amd64-cpython-39\\lap\n  copying lap\\__init__.py -&gt; build\\lib.win-amd64-cpython-39\\lap\n  running build_ext\n  INFO: No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils\n  INFO: customize MSVCCompiler\n  INFO: customize MSVCCompiler using build_ext\n  INFO: CCompilerOpt.cc_test_flags[1077] : testing flags (/O2)\n  error: Microsoft Visual C++ 14.0 or greater is required. Get it with &quot;Microsoft C++ Build Tools&quot;: https://visualstudio.microsoft.com/visual-cpp-build-tools/\n  INFO: CCompilerOpt.cache_flush[857] : write cache to path -&gt; C:\\Users\\ACER\\AppData\\Local\\Temp\\pip-install-a82najcq\\lap_e14dabff97d544e59d3633db1f44d15e\\build\\temp.win-amd64-cpython-39\\Release\\ccompiler_opt_cache_ext.py\n  [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: legacy-install-failure\n\nEncountered error while trying to install package.\n\nlap\n\nnote: This is an issue with the package mentioned above, not pip.\nhint: See above for output from the failure.\n\n[notice] A new release of pip available: 22.3.1 -&gt; 23.1.2\n[notice] To update, run: python.exe -m pip install --upgrade pip\n</code></pre>\n",
    "is_answered": true,
    "view_count": 6658,
    "answer_count": 5
  },
  {
    "title": "Fast estimation of lagged covariance",
    "link": "https://stackoverflow.com/questions/76463018/fast-estimation-of-lagged-covariance",
    "tags": [
      "python",
      "python-3.x",
      "statistics",
      "data-science",
      "signal-processing"
    ],
    "body": "<p>I am working on a bigger function whose main bottleneck is the estimation of lagged covariance. I couldn't find any function, so I had to make my own:</p>\n<pre><code>def crosscov(signal1, signal2):\n    lags = numpy.arange(-signal1.size+1,signal1.size)\n    signal1 = copy.deepcopy(pandas.Series(signal1))\n    signal2 = copy.deepcopy(pandas.Series(signal2))\n    c = numpy.zeros(len(lags))\n    counter = -1\n    for lag in lags:\n        counter = counter + 1\n        cov_matrix = numpy.cov(signal1,signal2.shift(lag,fill_value=0), bias=True)\n        c[counter] = copy.deepcopy(cov_matrix[0,1])\n    return c, lags\n</code></pre>\n<p>In its current form about 90% of the main function is taken up from just crosscov. Is there a way to speed this up? A similar code in MATLAB with its built-in xcov function takes about 4 min, while in Python takes 3+ hours.</p>\n<p>I have to estimate the lagged covariance between several pairs of vectors, so the optimal would be to input a matrix and get all possible covariance values for all possible lags.</p>\n<p><strong>Edit</strong>:</p>\n<p>I found a better solution:</p>\n<pre><code>def crosscov(signal1, signal2):\n       lags = numpy.arange(-signal1.size+1,signal1.size)\n       c = numpy.correlate(signal1,signal2,mode = &quot;full&quot;) \n       c = c/len(signal1)\n       return c, lags\n</code></pre>\n<p>But I think I can speed up my code even more if instead of doing it for pair of signals, I vectorize the whole thing.\nFor example a 12x32 matrix (12 datapoints, 32 signals) should return a 23x1024 matrix(12x2-1,32x32). This is because it uses all possible lags in all possible combination of channels.</p>\n<p>Any ideas what I could do?</p>\n",
    "is_answered": false,
    "view_count": 127,
    "answer_count": 0
  },
  {
    "title": "How do I convert a Vec &lt;&amp;str&gt; to Vec &lt;String&gt;?",
    "link": "https://stackoverflow.com/questions/76461002/how-do-i-convert-a-vec-str-to-vec-string",
    "tags": [
      "rust",
      "data-science",
      "arff"
    ],
    "body": "<pre><code>let parts: Vec&lt;String&gt; = line.split_whitespace().collect();\nlet parts: Vec&lt;String&gt; = map(AsRef::as_ref).line.split_whitespace().collect();\nif parts.len() &gt;= 3 {\n    let attribute_name = parts[1].to_string();\n    let attribute_type = parts[2].to_lowercase();\n    let attribute = match attribute_type.as_str() {\n        &quot;numeric&quot; =&gt; Attribute::Numeric,\n        &quot;date&quot; =&gt; Attribute::Date(parts.get(3).cloned()),\n        &quot;string&quot; =&gt; Attribute::String,\n        &quot;nominal&quot; =&gt; {\n            let nominal_values: Vec&lt;String&gt; = parts[3..]\n                .iter()\n                .map(|s| s.trim_matches(|c| c == '{' || c == '}' || c == ',').to_string())\n                .collect();\n            Attribute::Nominal(nominal_values)\n        }\n</code></pre>\n<p><a href=\"https://i.sstatic.net/QFxHX.jpg\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/QFxHX.jpg\" alt=\"enter image description here\" /></a></p>\n<p>My goal is to implement an algorithm in Rust for loading and reading .arff files. It reads an .arff file, extract information from header, attributes and instances, then prints it on the output. Each attribute is represented as a pair (name, type) in array attributes, and instances are stored as arrays of attribute values \u200b\u200bin array instances.</p>\n",
    "is_answered": true,
    "view_count": 4709,
    "answer_count": 1
  },
  {
    "title": "Feature Engineering in Python",
    "link": "https://stackoverflow.com/questions/76451829/feature-engineering-in-python",
    "tags": [
      "python",
      "data-science",
      "one-hot-encoding",
      "feature-engineering",
      "label-encoding"
    ],
    "body": "<p>How do I know when to apply LabelEncoder() or OneHotEncoder()?</p>\n<p>I have used LabelEncoder to encode categorical variable for RandomForestRegressor model and it gives a extremely high mean squared error. I have tried hyperparameter tuning with GridSearchCV and it still gives the same value</p>\n",
    "is_answered": false,
    "view_count": 77,
    "answer_count": 2
  },
  {
    "title": "How to iterate through a website that shows no index in the URL",
    "link": "https://stackoverflow.com/questions/76449810/how-to-iterate-through-a-website-that-shows-no-index-in-the-url",
    "tags": [
      "python",
      "dataframe",
      "web-scraping",
      "data-science"
    ],
    "body": "<p>I'm trying to get data from &quot;https://chainplay.gg/&quot; but I'm stuck and I can't get data other than that on page one.\nThis is the code I wrote and it returns the .csv I'm looking for but, without getting me the other 44 pages of data I can't do much with it:</p>\n<pre><code>from bs4 import BeautifulSoup as bs \nimport json\nimport csv\n\nr = requests.get('https://chainplay.gg/') \n\nsoup = bs(r.content)\n\n#print(soup.prettify()) \n\ndata = json.loads(soup.find('script', id=&quot;__NEXT_DATA__&quot;, type=&quot;application/json&quot;).text) \n#print(data)\ndata = data[&quot;props&quot;][&quot;pageProps&quot;][&quot;game&quot;]\n#print(data)\n\njson_object = json.dumps(data, indent=4)\n \nwith open(&quot;games.json&quot;, &quot;w&quot;) as outfile:\n    outfile.write(json_object)\n\nwith open('games.json') as json_file:\n    data = json.load(json_file)\n \ngames = data['games']\n \ndata_file = open('data_file.csv', 'w')\n \ncsv_writer = csv.writer(data_file)\n\ncount = 0\n \nfor game in games:\n    if count == 0:\n \n        # Writing headers of CSV file\n        header = game.keys()\n        csv_writer.writerow(header)\n        count += 1\n \n    # Writing data of CSV file\n    csv_writer.writerow(game.values())\n\ndata_file.close()\n</code></pre>\n<p>The one above is only the code that I know for sure is working, every other attempt resulted in empty dataframes/continuous error generation.\nI tried using Selenium and looking or looking for pages indexes but to no avail. Is there someone that could please help me?</p>\n",
    "is_answered": true,
    "view_count": 42,
    "answer_count": 1
  },
  {
    "title": "How to convert a complex datasheet into a usabale dataframe?",
    "link": "https://stackoverflow.com/questions/76444291/how-to-convert-a-complex-datasheet-into-a-usabale-dataframe",
    "tags": [
      "python",
      "dataframe",
      "data-science"
    ],
    "body": "<p>So I have the following google sheet link:\n<a href=\"https://docs.google.com/spreadsheets/d/1GAH6Cb8JuloDAiTIGnEr9GtPPyHWXMpOLV6nHNSgcoM/edit#gid=1324085625\" rel=\"nofollow noreferrer\">https://docs.google.com/spreadsheets/d/1GAH6Cb8JuloDAiTIGnEr9GtPPyHWXMpOLV6nHNSgcoM/edit#gid=1324085625</a></p>\n<p>This sheet is a school timetable and what I have to do is read some fitered data like for example name of the teacher and when does he has classrom or name of the classroom and when it's ocupied. I'm free to filter how ever I want then I have to converted the filtered result into a pdf. My problem is how do I make this data into a dataframe that can be fitered! I tried with the libraries gspread and pandas and made the dataframe but the data is unusable and can't manipulate it. From the research I've done usualy the dataframes have a header and the data is nicely organized, in my case the timetable I think it's not structered to be used that way! How would you aproache a problem like this and make the data in these sheet easy to play with? I need an ideea!</p>\n<p><a href=\"https://i.sstatic.net/Kp79C.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/Kp79C.png\" alt=\"csv example\" /></a>What I did is I converted the sheet into a data frame using pandas and gspread but it is not usabale in my situation!</p>\n",
    "is_answered": false,
    "view_count": 52,
    "answer_count": 0
  },
  {
    "title": "Normallity Tests and Data Science",
    "link": "https://stackoverflow.com/questions/76439597/normallity-tests-and-data-science",
    "tags": [
      "data-science",
      "statistical-test"
    ],
    "body": "<p>I have a dataset and I wanted to check if some variables follow a normal distribution. So, I have read about normallity test but it seem to be sensitive when the amout of data is large and, apparantly when I plot the histogram and the QQ-Plot, the variable seem to be normal. Should I only relly on the Histogram plus QQ-Plot or  it is the best practice to do a normallity test as well?</p>\n<p>I have plotted histograms, QQ-Plot, Kurtosis, Skewness and also a dataframe with 3 normallity tests</p>\n<p>1 - Shapiro-Wilk.\n2 - Lilliefors.\n3 - D'Agostino_K2</p>\n<p>My conclusion was that the variable is normal based on QQ-Plot, Histogrm and, Kurtosis and Skewness between (-1, +1).</p>\n",
    "is_answered": true,
    "view_count": 45,
    "answer_count": 1
  },
  {
    "title": "Getting RuntimeError : configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml not available in Model Zoo! while Running Detectron2 for Object Detection",
    "link": "https://stackoverflow.com/questions/76436619/getting-runtimeerror-configs-coco-detection-faster-rcnn-r-50-fpn-3x-yaml-not-a",
    "tags": [
      "python",
      "data-science",
      "pre-trained-model",
      "faster-rcnn",
      "detectron"
    ],
    "body": "<p>Want to train a custom Image Dataset by Detectron2 Faster_RCNN model . I am using a <strong>wsl2 ubuntu terminal</strong> and a VScode in my windows os. In my train.py, I initiate a config_file_path with &quot;configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml&quot; for modelzoo.py. In the Model Zoo Directory -&gt; configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml file is also available but this error happens<a href=\"https://i.sstatic.net/zgoUZ.png\" rel=\"nofollow noreferrer\">enter image description here</a></p>\n<pre><code>#train.py\n\nimport numpy as np\nfrom detectron2.utils.logger import setup_logger\n\nsetup_logger()\n\nfrom detectron2.data.datasets import register_coco_instances\nfrom detectron2.engine import DefaultTrainer\n\nimport os\nimport pickle\n\nfrom utils import *\n\nconfig_file_path = &quot;configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml&quot;\ncheckpoint_url = &quot;configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml&quot;\n\noutput_dir = &quot;./output/object_detection&quot;\nnum_classes = 1\n\ndevice = &quot;cuda&quot;\n\ntrain_dataset_name = &quot;LP_train&quot;\ntrain_image_path = &quot;train&quot;\ntrain_json_annot_path = &quot;train.json&quot;\n\ntest_dataset_name = &quot;LP_test&quot;\ntest_image_path = &quot;test&quot;\ntest_json_annot_path = &quot;test.json&quot;\n\n###############################################\nregister_coco_instances(name = train_dataset_name, metadata = {},\njson_file=train_json_annot_path, image_root=train_image_path)\n\nregister_coco_instances(name=test_dataset_name, metadata={},json_file=test_json_annot_path, image_root=test_image_path)\n\n#plot_samples(dataset_name= train_dataset_name, n = 2)\n\n###############################################\n\n\ndef main():\n    cfg = get_train_cfg(config_file_path, checkpoint_url, train_dataset_name, test_dataset_name, num_classes,device, output_dir)\n    \n    #saving cfg\n    with open(cfg_save_path, 'wb') as f:\n        pickle.dump(cfg, f, protocol= pickle.HIGHEST_PROTOCOL)\n        \n    os.makedirs(cfg.OUTPUT_DIR, exist_ok= True)\n    \n    trainer = DefaultTrainer(cfg)\n    trainer.resume_or_load(resume= False)\n    \n    trainer.train()\n    \nif __name__ == '__main__':\n    main( )\n\n</code></pre>\n<pre><code>#utlis.py\n\nfrom detectron2.data import DatasetCatalog, MetadataCatalog\nfrom detectron2.utils.visualizer import Visualizer\nfrom detectron2.config import get_cfg\nfrom detectron2 import model_zoo\n\nfrom detectron2.utils.visualizer import ColorMode\n\nimport random\nimport cv2\nimport matplotlib.pyplot as plt\n\ndef plot_samples(dataset_name, n=1):\n    dataset_custom = DatasetCatalog.get(dataset_name)\n    dataset_custom_metadata = MetadataCatalog.get(dataset_name)\n    \n    for s in random.sample(dataset_custom, n):\n        img = cv2.imread(s[&quot;file_name&quot;])\n        v = Visualizer(img[:,:,::-1], metadata=dataset_custom_metadata, scale= 0.5)\n        v = v.draw_dataset_dict(s)\n        plt.figure(figsize=(15,20))\n        plt.imshow(v.get_image())\n        plt.show()\n        #plt.savefig(&quot;matplotlib.png&quot;) #save config , don't show\n        \ndef get_train_cfg(self,config_file_path, checkpoint_url, train_dataset_name, num_classes, device, output_dir):\n    cfg = get_cfg()\n    \n    cfg.merge_from_file(model_zoo.get_config_file(config_file_path))\n    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(checkpoint_url)\n    cfg.DATASETS.TRAIN = (train_dataset_name,)\n    cfg.DATASETS.TEST = (train_dataset_name,)\n    \n    cfg.DATALOADER.NUM_WORKERS = 5\n    \n    cfg.SOLVER.IMS_PER_BATCH = 5\n    cfg.SOLVER.BASE_LR = 0.00025\n    cfg.SOLVER.MAX_ITER = 1000\n    cfg.SOLVER.STEPS = []\n    \n    cfg.MODEL.ROI_HEADS.NUM_CLASSES = num_classes\n    cfg.MODEL.DEVICE = device\n    cfg.OUTPUT_DIR = output_dir\n    \n    return cfg\n</code></pre>\n<p>i tried to change the path -\nconfig_file_path = &quot;COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml&quot;\ncheckpoint_url = &quot;COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml&quot;</p>\n<p>but than kind of same type of error happened <a href=\"https://i.sstatic.net/3aYYs.png\" rel=\"nofollow noreferrer\">enter image description here</a>, btw I'm a begginer , I'm Expecting by Running this Pre-trained model from Zoo , I'll built a custom dataset of IMAGE for Object recognition!</p>\n",
    "is_answered": false,
    "view_count": 1228,
    "answer_count": 1
  },
  {
    "title": "Transform Data Set Using Pandas",
    "link": "https://stackoverflow.com/questions/76436143/transform-data-set-using-pandas",
    "tags": [
      "pandas",
      "numpy",
      "scikit-learn",
      "data-science"
    ],
    "body": "<p>I'm trying to transform how my data looks. I am trying to transform it from this:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Feature 1</th>\n<th>Feature 2</th>\n<th>Feature 3</th>\n<th>Feature 4</th>\n<th>Target</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>10.2</td>\n<td>0.5</td>\n<td>2.1</td>\n<td>7.8</td>\n<td>15.3</td>\n</tr>\n<tr>\n<td>11.5</td>\n<td>0.7</td>\n<td>1.8</td>\n<td>7.0</td>\n<td>16.2</td>\n</tr>\n<tr>\n<td>9.8</td>\n<td>0.6</td>\n<td>2.2</td>\n<td>8.5</td>\n<td>14.5</td>\n</tr>\n<tr>\n<td>...</td>\n<td>...</td>\n<td>...</td>\n<td>...</td>\n<td>...</td>\n</tr>\n<tr>\n<td>12.3</td>\n<td>0.9</td>\n<td>1.4</td>\n<td>6.2</td>\n<td>17.6</td>\n</tr>\n<tr>\n<td>11.7</td>\n<td>0.8</td>\n<td>1.7</td>\n<td>6.5</td>\n<td>16.8</td>\n</tr>\n<tr>\n<td>10.9</td>\n<td>0.7</td>\n<td>2.0</td>\n<td>7.3</td>\n<td>15.9</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>to this:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Date/Time</th>\n<th>Target Variable</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>2000-01-01 00:00:00</td>\n<td>10.2</td>\n</tr>\n<tr>\n<td>2000-01-01 01:00:00</td>\n<td>11.5</td>\n</tr>\n<tr>\n<td>2000-01-01 02:00:00</td>\n<td>9.8</td>\n</tr>\n<tr>\n<td>...</td>\n<td>...</td>\n</tr>\n<tr>\n<td>2017-12-31 21:00:00</td>\n<td>12.3</td>\n</tr>\n<tr>\n<td>2017-12-31 22:00:00</td>\n<td>11.7</td>\n</tr>\n<tr>\n<td>2017-12-31 23:00:00</td>\n<td>10.9</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p><a href=\"https://i.sstatic.net/34BRv.png\" rel=\"nofollow noreferrer\">Here is a link to what my current data looks like.</a></p>\n<p>I looked at the pandas documentation, but I had no luck figuring out how to achieve this result with my data set.</p>\n",
    "is_answered": false,
    "view_count": 43,
    "answer_count": 1
  },
  {
    "title": "Comparing data to a mapping file in R",
    "link": "https://stackoverflow.com/questions/76431885/comparing-data-to-a-mapping-file-in-r",
    "tags": [
      "r",
      "compare",
      "mapping",
      "data-science"
    ],
    "body": "<p>I am quite new in R and need help at a project for work.\nMy company is switching from single role assignment to multi-role assigment in our system.\nThe mapping file for two roles could look like this:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>old role</th>\n<th>new role</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>teacher</td>\n<td>literature expert</td>\n</tr>\n<tr>\n<td>teacher</td>\n<td>caretaker</td>\n</tr>\n<tr>\n<td>teacher</td>\n<td>nice adult</td>\n</tr>\n<tr>\n<td>janitor</td>\n<td>cleaner</td>\n</tr>\n<tr>\n<td>janitor</td>\n<td>plumber</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>My task is to write a script which will check if everything was migrated correctly: every employee that used to be assigned \u201eteacher\u201c should now be assigned \u201eliterature expert\u201c, \u201ecaretaker\u201c and \u201enice adult\u201c instead.</p>\n<p>So far I did a full outer join with the data from the old system with the old roles and the new system with the new roles.\nI also have an Excel Sheet with the mapping data, consisting of the columns \u201eold_role\u201c and \u201enew_role\u201c.</p>\n<p><code>df &lt;- merge(x = newdata, y = olddata, by \u201eemployee_ID, all = TRUE)</code></p>\n<p>Now I have a data frame like this:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>employee_id</th>\n<th>old_role</th>\n<th>new_role</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>teacher</td>\n<td>literature expert</td>\n</tr>\n<tr>\n<td>1</td>\n<td>teacher</td>\n<td>caretaker</td>\n</tr>\n<tr>\n<td>1</td>\n<td>teacher</td>\n<td>nice adult</td>\n</tr>\n<tr>\n<td>2</td>\n<td>teacher</td>\n<td>caretaker</td>\n</tr>\n<tr>\n<td>2</td>\n<td>teacher</td>\n<td>nice adult</td>\n</tr>\n<tr>\n<td>3</td>\n<td>janitor</td>\n<td>cleaner</td>\n</tr>\n<tr>\n<td>3</td>\n<td>janitor</td>\n<td>plumber</td>\n</tr>\n<tr>\n<td>4</td>\n<td>janitor</td>\n<td>cleaner</td>\n</tr>\n<tr>\n<td>4</td>\n<td>janitor</td>\n<td>plumber</td>\n</tr>\n<tr>\n<td>4</td>\n<td>janitor</td>\n<td>nice adult</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>Employee 1 and 3 are migrated correctly. Employee 2 misses one role, employee 4 has been falsely assigned a role.</p>\n<p>Now I am not sure how to continue.\nI need R to iterate through every employee and check if the new roles are assigned correctly according to the old role and print feedback &quot;ok&quot; or &quot;not ok&quot; per employee.\nI thought about creating a new column in my df containing this output, but I am not sure if this would work, because it should relate to multiple rows - only if every necessary role is assigned and none additionally added, it should print &quot;ok&quot;.\nAn output in the Terminal is also fine. I am not picky about the format.</p>\n<p>What's the best way to compare my data to the mapping data?</p>\n<p>Thanks a lot in advance!</p>\n",
    "is_answered": false,
    "view_count": 67,
    "answer_count": 1
  },
  {
    "title": "When using scikit-learn K-Means Clustering, how can you extract the centroids in original data domain?",
    "link": "https://stackoverflow.com/questions/76426328/when-using-scikit-learn-k-means-clustering-how-can-you-extract-the-centroids-in",
    "tags": [
      "python",
      "data-science",
      "cluster-analysis",
      "k-means"
    ],
    "body": "<p>I am using the <code>sklearn KMeans</code> k-means clustering algorithm. Before clustering, I normalize my data from <code>[0,1]</code> using</p>\n<pre><code>scaler = MinMaxScaler()\nscaled_features = scaler.fit_transform(data)\n</code></pre>\n<p>Now, I can run the K-means algorithm.</p>\n<pre><code>kmeans = KMeans(\n        init=&quot;random&quot;,\n        n_clusters=3,\n        n_init=10,\n        max_iter=3000,\n    )\n    kmeans.fit(scaled_features)\n</code></pre>\n<p>Then, I can extract the 3 cluster centroids using <code>kmeans.cluster_centers_</code>. However, these centroids are in the <em>normalized domain [0,1].</em> How can I re-transform these to the <em>original data domain?</em></p>\n",
    "is_answered": true,
    "view_count": 458,
    "answer_count": 1
  },
  {
    "title": "Anaconda new environment error: Paths don&#39;t have the same drive",
    "link": "https://stackoverflow.com/questions/76425016/anaconda-new-environment-error-paths-dont-have-the-same-drive",
    "tags": [
      "anaconda",
      "data-science"
    ],
    "body": "<p>I want to create a new environment in Anaconda, but I get an error:</p>\n<blockquote>\n<p>ValueError(&quot;Paths don't have the same drive&quot;)</p>\n</blockquote>\n<pre><code>file &quot;Q:\\CI_Anlisten\\Users\\lkh004\\Conda\\lib\\ntpath.py, line 804 in commonpath raise ValueError(&quot;Paths don't have the same drive)\n</code></pre>\n",
    "is_answered": false,
    "view_count": 312,
    "answer_count": 0
  },
  {
    "title": "Sum of column that resets to zero through out a process",
    "link": "https://stackoverflow.com/questions/76423496/sum-of-column-that-resets-to-zero-through-out-a-process",
    "tags": [
      "python",
      "pandas",
      "dataframe",
      "data-science"
    ],
    "body": "<p>Is there an easy way to go about a total for a column that increments but can reset back to zero through out the dataset? I have started to go down the path of a for loop and keeping track of previous value if it isn't a zero and using multiple variables, but wanted to check if there is a better way to go about it. Here is an example of the data I'm trying to work with:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th style=\"text-align: left;\">id</th>\n<th style=\"text-align: center;\">Time</th>\n<th style=\"text-align: center;\">Extruder1</th>\n<th style=\"text-align: right;\">LineSpeed</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\">157</td>\n<td style=\"text-align: center;\">5/22/2023 10:14:09.229 PM</td>\n<td style=\"text-align: center;\">1560.0</td>\n<td style=\"text-align: right;\">0.0</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">158</td>\n<td style=\"text-align: center;\">5/22/2023 10:16:28.582 PM</td>\n<td style=\"text-align: center;\">1563.0</td>\n<td style=\"text-align: right;\">0.0</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">159</td>\n<td style=\"text-align: center;\">5/23/2023 7:17:37.831 AM</td>\n<td style=\"text-align: center;\">1563.0</td>\n<td style=\"text-align: right;\">0.0</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">160</td>\n<td style=\"text-align: center;\">5/23/2023 7:19:57.184 AM</td>\n<td style=\"text-align: center;\">0.0</td>\n<td style=\"text-align: right;\">0.0</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">161</td>\n<td style=\"text-align: center;\">5/23/2023 7:33:53.302 AM</td>\n<td style=\"text-align: center;\">2.0</td>\n<td style=\"text-align: right;\">0.0</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">162</td>\n<td style=\"text-align: center;\">5/23/2023 7:36:12.655 AM</td>\n<td style=\"text-align: center;\">4.0</td>\n<td style=\"text-align: right;\">0.0</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">163</td>\n<td style=\"text-align: center;\">5/23/2023 7:38:32.008 AM</td>\n<td style=\"text-align: center;\">6.0</td>\n<td style=\"text-align: right;\">0.0</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">334</td>\n<td style=\"text-align: center;\">5/23/2023 2:15:41.371 PM</td>\n<td style=\"text-align: center;\">789.0</td>\n<td style=\"text-align: right;\">0.0</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">335</td>\n<td style=\"text-align: center;\">5/23/2023 2:18:00.724 PM</td>\n<td style=\"text-align: center;\">792.0</td>\n<td style=\"text-align: right;\">0.0</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">336</td>\n<td style=\"text-align: center;\">5/23/2023 2:20:20.077 PM</td>\n<td style=\"text-align: center;\">794.0</td>\n<td style=\"text-align: right;\">0.0</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">349</td>\n<td style=\"text-align: center;\">5/23/2023 2:50:31.666 PM</td>\n<td style=\"text-align: center;\">2.0</td>\n<td style=\"text-align: right;\">0.0</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>I would need to get a total of 3 from the first three rows and then get 6 from the next four, and so on. Is there a method in Pandas that will work for this type of column?</p>\n",
    "is_answered": true,
    "view_count": 60,
    "answer_count": 1
  },
  {
    "title": "Getting a ModuleNotFoundError with librosa",
    "link": "https://stackoverflow.com/questions/76418309/getting-a-modulenotfounderror-with-librosa",
    "tags": [
      "python",
      "python-3.x",
      "data-science",
      "librosa",
      "modulenotfounderror"
    ],
    "body": "<p>I am trying to load the audio files into the NumPy array using this code</p>\n<pre><code>#%%\nimport librosa\nimport matplotlib.pyplot as plt\nimport IPython.display as ipd\nimport os, os.path\nimport time\nimport joblib\nimport numpy as np\n\n#%%\nfname = 'archive\\\\Actor_01\\\\03-01-01-01-01-01-01.wav'\n\ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveshow(data, sr=sampling_rate)\n\nipd.Audio(fname)\n\n# %%\nlst = []\n\nfor subdir, dirs, files in os.walk('archive'):\n    for file in files:\n        try:\n            X, sample_rate = librosa.load(os.path.join(subdir, file), res_type='kaiser_fast')\n            mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n            file_class = int(file[7:8]) - 1\n            arr = mfccs, file_class\n            lst.append(arr)\n        except ValueError as err:\n            print(err)\n            continue\n</code></pre>\n<p>Everything runs fine except I am getting an error at line 25</p>\n<pre><code>ModuleNotFoundError                       Traceback (most recent call last)\nc:\\Users\\powellt1\\Documents\\COMP5500\\NonGitSED\\algorithm.py in line 7\n      23 for file in files:\n      24     try:\n----&gt; 25         X, sample_rate = librosa.load(os.path.join(subdir, file), res_type='kaiser_fast')\n      26         mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n      27         file_class = int(file[7:8]) - 1\nFile c:\\Users\\powellt1\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\librosa\\core\\audio.py:193, in load(path, sr, mono, offset, duration, dtype, res_type)\n    190     y = to_mono(y)\n    192 if sr is not None:\n--&gt; 193     y = resample(y, orig_sr=sr_native, target_sr=sr, res_type=res_type)\n    195 else:\n    196     sr = sr_native\n\nFile c:\\Users\\powellt1\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\librosa\\core\\audio.py:684, in resample(y, orig_sr, target_sr, res_type, fix, scale, axis, **kwargs)\n    675     y_hat = np.apply_along_axis(\n    676         soxr.resample,\n    677         axis=axis,\n   (...)\n    681         quality=res_type,\n    682     )\n    683 else:\n--&gt; 684     y_hat = resampy.resample(y, orig_sr, target_sr, filter=res_type, axis=axis)\n</code></pre>\n<p>I know I have librosa installed properly and it works in the previous cells.</p>\n<p>I've tried to reinstall and verify the installation of librosa and I've tried using</p>\n<pre><code>from librosa.core import load\n</code></pre>\n<p>but nothing seems to be working to fix the error.</p>\n",
    "is_answered": true,
    "view_count": 985,
    "answer_count": 3
  },
  {
    "title": "pandas series mark all the rows between two values",
    "link": "https://stackoverflow.com/questions/76415088/pandas-series-mark-all-the-rows-between-two-values",
    "tags": [
      "pandas",
      "dataframe",
      "data-science",
      "data-munging"
    ],
    "body": "<p>I have a series ( a single col in a df) with 3 possible values:</p>\n<pre><code>Stable, Increase, Decresae\n</code></pre>\n<p>, and I want to mark all the areas between a Increase to the subsequent Decrease. So for the values:</p>\n<pre><code>Stable\nStable\nStable\nIncrease\nIncrease\nStable\nStable\nDecrease\nStable\nIncrease\nStable\nDecrease\n</code></pre>\n<p>I will get: <code>-,-,-,+,+,+,+,-,-,+,+,-</code>\nWhat is the best way to do so?</p>\n",
    "is_answered": true,
    "view_count": 68,
    "answer_count": 3
  },
  {
    "title": "sentiment classification using doc2vec and LSTM Models",
    "link": "https://stackoverflow.com/questions/76401941/sentiment-classification-using-doc2vec-and-lstm-models",
    "tags": [
      "python",
      "nlp",
      "data-science",
      "lstm",
      "doc2vec"
    ],
    "body": "<p>I am building a text classification model based on sentiment analysis,\nthe data contains text and sentiment[Positive, Natural, Negative]<br />\nAs first step, I clean the data and normalize it,\nthen create doc2vec embedding:</p>\n<pre><code># Convert the data to TaggedDocument format for Doc2Vec\ndocuments = [TaggedDocument(words=text.split(), tags=[label]) for text, label in zip(data[&quot;text&quot;], data[&quot;sentiment&quot;])]\nprint(documents)\nmodel = Doc2Vec(vector_size=10, window=2, min_count=1, workers=4, epochs=100)\nmodel.build_vocab(documents)\nmodel.train(documents, total_examples=model.corpus_count, epochs=model.epochs)\n</code></pre>\n<p>then split the data:</p>\n<pre><code>X_train = [model.infer_vector(text.split()) for text in data[&quot;text&quot;]]\nprint(X_train)\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nlabel_encoder = LabelEncoder()\ny_trainEmbedding = label_encoder.fit_transform(data['sentiment'])\nonehot_encoder = OneHotEncoder(sparse=False)\ny_trainEmbedding = onehot_encoder.fit_transform(y_trainEmbedding.reshape(-1, 1))\n</code></pre>\n<p>then build LSTM model:</p>\n<pre><code>import numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense\n\nnum_classes = len(np.unique(data[&quot;sentiment&quot;]))\nmodel_lstm = Sequential()\nmodel_lstm.add(LSTM(64, input_shape=(10, 1)))\nmodel_lstm.add(Dense(32, activation=&quot;relu&quot;))\nmodel_lstm.add(Dense(num_classes, activation=&quot;softmax&quot;))\nmodel_lstm.compile(loss=&quot;categorical_crossentropy&quot;, optimizer=&quot;adam&quot;, metrics=[&quot;accuracy&quot;])\nX_train_lstm = np.array(X_train).reshape(-1, 10, 1)\ny_train_lstm = np.array(y_trainEmbedding)\nmodel_lstm.fit(X_train_lstm, y_train_lstm, epochs=100, batch_size=32)\n</code></pre>\n<p>the result is good and the accuracy is 0.99</p>\n<p>but when I try to predict the label of new text such as below:</p>\n<pre><code># Use the trained model to predict the sentiment of new texts\ntext = &quot;\u0647\u0630\u0627 \u0627\u0644\u0628\u064a\u062a \u062c\u0645\u064a\u0644 &quot;\ntext=remove_punctuations(text)\ntext=remove_repeating_char(text)\ntext=remove_english_char(text)\ntext=remove_diacritics(text)\ntext=remove_noise_char(text)\ntext=tokenizer(text)\ntext=remove_stop_word(text)\ntext=stemming(text) \nnew_embedding = model.infer_vector(text.split())\nprint(new_embedding)\nnew_embedding_lstm = np.array(new_embedding).reshape(-1, 10, 1)\nprint(new_embedding)\n\ny_pred = model_lstm.predict(new_embedding_lstm)\nprint(y_pred)\n\npredicted_label = label_encoder.inverse_transform(np.argmax(y_pred))\nprint(predicted_label)\n</code></pre>\n<p>this error occured:</p>\n<pre><code> 18 \n---&gt; 19 predicted_label = label_encoder.inverse_transform(np.argmax(y_pred))\n     20 print(predicted_label)\n\n1 frames\n/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py in column_or_1d(y, dtype, warn)\n   1200         return _asarray_with_order(xp.reshape(y, -1), order=&quot;C&quot;, xp=xp)\n   1201 \n-&gt; 1202     raise ValueError(\n   1203         &quot;y should be a 1d array, got an array of shape {} instead.&quot;.format(shape)\n   1204     )\n\nValueError: y should be a 1d array, got an array of shape () instead.\n</code></pre>\n<p>is my process correct?\nand Anyone can help me solve it?</p>\n",
    "is_answered": false,
    "view_count": 97,
    "answer_count": 0
  },
  {
    "title": "Prevent tidymodels from turning numeric to chr",
    "link": "https://stackoverflow.com/questions/76400397/prevent-tidymodels-from-turning-numeric-to-chr",
    "tags": [
      "r",
      "types",
      "data-science",
      "tidymodels",
      "r-recipes"
    ],
    "body": "<p>Apologies if this has been asked before, but I was unable to find the corresponding info.</p>\n<p>I am using the <code>recipe</code> from <code>tidymodels</code> and trying to create a model (eventually).</p>\n<p>As I prepped my this is effectively what it looked like (apologies, I can't include the actual data)</p>\n<p>Import:</p>\n<p><code>myData = read.csv('file')</code></p>\n<p>which gives me something along the lines of</p>\n<pre><code>ID NumericField1 StringField1... NumericFieldN StringFieldN\n</code></pre>\n<p>and all the data types are correct for numeric/non-numeric data</p>\n<p>I then go on to do some data manipulation and whatnot and get to the point where I split my data</p>\n<pre><code>train_test_split &lt;- initial_split(data=myManipulatedData, prop=MySplitPct)\n\ntrain_data &lt;- train_test_split %&gt;% training()\ntest_data &lt;- train_test_split %&gt;% testing()\n</code></pre>\n<p>at this point I double checked the data-types on the <code>test_data</code> before I put it in my recipe and did make sure all my numeric data was still numeric etc... And verified the data has all the correct data types</p>\n<p>so then I create my recipe:</p>\n<pre><code>my_recipe &lt;- recipe(outcome ~ numeric_1 + numeric_2 + ... + string_1 + ..., data=test_data) %&gt;%\n  update_role(numeric_n, numeric_m, new_role=&quot;ID&quot;)\n</code></pre>\n<p>however after doing so this is what it spits out in the summary:</p>\n<pre><code>&gt; summary(my_recipe)\n# A tibble: 15 \u00d7 4\n   variable              type      role      source  \n   &lt;chr&gt;                 &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 numeric_1             &lt;chr [2]&gt; predictor original\n 2 numeric_2             &lt;chr [2]&gt; predictor original\n 3 string_1              &lt;chr [3]&gt; predictor original\n 4 numeric_3             &lt;chr [2]&gt; predictor original\n 5 numeric_4             &lt;chr [2]&gt; predictor original\n 6 string_2              &lt;chr [3]&gt; predictor original\n 7 string_3              &lt;chr [3]&gt; predictor original\n 8 numeric_5             &lt;chr [2]&gt; predictor original\n 9 string_4              &lt;chr [3]&gt; predictor original\n10 string_5              &lt;chr [3]&gt; predictor original\n11 string_6              &lt;chr [3]&gt; predictor original\n12 numeric_6             &lt;chr [2]&gt; predictor original\n13 numeric_7             &lt;chr [2]&gt; ID        original\n14 numeric_8             &lt;chr [2]&gt; ID        original\n15 outcome_1             &lt;chr [3]&gt; outcome   original\n</code></pre>\n<p>Just to tie it together since I know it will be difficult for anyone to debug without the actual data here is one column which I can share:</p>\n<p><code>numeric_1 = y_coordinate</code></p>\n<p>In our <code>summary(test_data)</code> this is how it shows:</p>\n<pre><code>y_coordinate\nMin.   :-42.0000\n1st Qu.:-15.0000   \nMedian :  0.0000   \nMean   : -0.5718\n3rd Qu.: 13.0000\nMax.   : 42.0000\n</code></pre>\n<p>So it is clear to me the test_data is aware that the field is numeric, but I don't understand why the recipe continues to use the chr type?</p>\n<p>TIA</p>\n",
    "is_answered": false,
    "view_count": 72,
    "answer_count": 0
  },
  {
    "title": "Getting error while parsing through driver.find_element()",
    "link": "https://stackoverflow.com/questions/76400223/getting-error-while-parsing-through-driver-find-element",
    "tags": [
      "python",
      "selenium-webdriver",
      "data-science",
      "analysis"
    ],
    "body": "<p>I am getting below error even after correcting the query, I have tried all the given solutions but the error is still same.</p>\n<p>I am using 2 jupyter files, one is imported into the other and is used to extract data from the web</p>\n<p>Attached error screenshot and the code snippet where the error is generated.</p>\n<p>I have tried the possible solution from the web.</p>\n<p>Code :</p>\n<pre class=\"lang-py prettyprint-override\"><code>def get_jobs(keyword, num_jobs, verbose, path, slp_time):\n    \n    '''Gathers jobs as a dataframe, scraped from Glassdoor'''\n    \n    #Initializing the webdriver\n    options = webdriver.ChromeOptions()\n    \n    #Uncomment the line below if you'd like to scrape without a new Chrome window every time.\n    #options.add_argument('headless')\n    \n    #Change the path to where chromedriver is in your home folder.\n    driver = webdriver.Chrome(executable_path=path, options=options)\n    driver.set_window_size(1120, 1000)\n\n    url = 'https://www.glassdoor.com/Job/jobs.htm?sc.keyword=&quot;' + keyword + '&quot;&amp;locT=C&amp;locId=1147401&amp;locKeyword=San%20Francisco,%20CA&amp;jobType=all&amp;fromAge=-1&amp;minSalary=0&amp;includeNoSalaryJobs=true&amp;radius=100&amp;cityId=-1&amp;minRating=0.0&amp;industryId=-1&amp;sgocId=-1&amp;seniorityType=all&amp;companyId=-1&amp;employerSizes=0&amp;applicationType=0&amp;remoteWorkType=0'\n    driver.get(url)\n    jobs = []\n\n    while len(jobs) &lt; num_jobs:  #If true, should be still looking for new jobs.\n\n        #Let the page load. Change this number based on your internet speed.\n        #Or, wait until the webpage is loaded, instead of hardcoding it.\n        time.sleep(slp_time)\n\n        #Test for the &quot;Sign Up&quot; prompt and get rid of it.\n        try:\n            driver.find_element(By.CSS_SELECTOR, &quot;[alt='Close']&quot;).click()\n        except ElementClickInterceptedException:\n            pass\n\n        time.sleep(.1)\n\n        try:\n            driver.find_element(By.CSS_SELECTOR, &quot;[alt = 'Close']&quot;).click()#clicking to the X.\n        except NoSuchElementException:\n            pass\n\n        \n        #Going through each job in this page\n        job_buttons = driver.find_element(By.CLASS_NAME, &quot;jl&quot;)  #jl for Job Listing. These are the buttons we're going to click.\n        for job_button in job_buttons:  \n\n            print(&quot;Progress: {}&quot;.format(&quot;&quot; + str(len(jobs)) + &quot;/&quot; + str(num_jobs)))\n            if len(jobs) &gt;= num_jobs:\n                break\n\n            job_button.click()  #You might \n            time.sleep(1)\n            collected_successfully = False\n</code></pre>\n<p>Call made here :</p>\n<pre class=\"lang-py prettyprint-override\"><code>df = get_jobs(&quot;data scientist&quot;, 15, False, path, 10)\n</code></pre>\n<p>Error Encoutered:</p>\n<pre><code>AttributeError                            Traceback (most recent call last)\n~\\AppData\\Local\\Temp\\ipykernel_5364\\40834992.py in &lt;module&gt;\n----&gt; 1 df = get_jobs(&quot;data scientist&quot;, 15, False, path, 10)\n\n~\\Documents\\ds_salary_proj\\glassdoor_scrapper.ipynb in get_jobs(keyword, num_jobs, verbose, path, slp_time)\n     36     &quot;    driver.set_window_size(1120, 1000)\\n&quot;,\n     37     &quot;\\n&quot;,\n---&gt; 38     &quot;    url = 'https://www.glassdoor.com/Job/jobs.htm?sc.keyword=\\&quot;' + keyword + '\\&quot;&amp;locT=C&amp;locId=1147401&amp;locKeyword=San%20Francisco,%20CA&amp;jobType=all&amp;fromAge=-1&amp;minSalary=0&amp;includeNoSalaryJobs=true&amp;radius=100&amp;cityId=-1&amp;minRating=0.0&amp;industryId=-1&amp;sgocId=-1&amp;seniorityType=all&amp;companyId=-1&amp;employerSizes=0&amp;applicationType=0&amp;remoteWorkType=0'\\n&quot;,\n     39     &quot;    driver.get(url)\\n&quot;,\n     40     &quot;    jobs = []\\n&quot;,\n\nAttributeError: 'list' object has no attribute 'click'\n</code></pre>\n",
    "is_answered": false,
    "view_count": 39,
    "answer_count": 0
  },
  {
    "title": "Why does the algorithm sometimes not behave as intended?",
    "link": "https://stackoverflow.com/questions/76397456/why-does-the-algorithm-sometimes-not-behave-as-intended",
    "tags": [
      "python",
      "pandas",
      "dataframe",
      "algorithm",
      "data-science"
    ],
    "body": "<p>we are currently working on a college project. We have been tasked to optimize the maintenance schedule for repairs on bikes from a bike sharing service. The bikes can only be rented from and returned to bike docking stations. We need to calculate the idle time, which is defined as follows:</p>\n<blockquote>\n<p>the time period from when bike with bike id = x was dropped off at station y until any other bike at station y is booked</p>\n</blockquote>\n<p>We implemented our solution as follows:</p>\n<pre><code>import pandas as pd\ncsv_file = '../Data_Cleanup/outCSV/Clean_Metro_Set.csv'\nmetro = pd.read_csv(csv_file)\nmetro['start_time'] = pd.to_datetime(metro['start_time'])\nmetro ['end_time'] = pd.to_datetime(metro['end_time'])\nmetro = metro.sort_values(by='start_time')\nmetro['idle_time'] = None\n\nBigDict = {\n    # station_id: {\n    #     bike_id: (transaction_id ,end_time)\n    # }\n}\n\nfor i, row in metro.iterrows():\n    current_start_time = row[&quot;start_time&quot;]\n    current_end_time = row[&quot;end_time&quot;]\n    current_end_station_id = row[&quot;end_station_id&quot;]\n    current_start_station_id = row[&quot;start_station_id&quot;]\n    current_bike_id = row[&quot;bike_id&quot;]\n    current_index = i\n\n    if current_start_station_id in BigDict:\n        for bike in list(BigDict[current_start_station_id]):  # Create a copy of the keys\n            idle_time = current_start_time - BigDict[current_start_station_id][bike][1]\n            metro.at[BigDict[current_start_station_id][bike][0], &quot;idle_time&quot;] = idle_time\n            if idle_time.total_seconds() &gt;= 0:\n                del BigDict[current_start_station_id][bike]\n\n    if current_end_station_id not in BigDict:\n        BigDict[current_end_station_id] = {current_bike_id: (current_index, current_end_time)}\n\n    BigDict[current_end_station_id][current_bike_id] = (current_index, current_end_time)\n\nmetro.to_csv('../Data_Cleanup/outCSV/Metro_Set_with_IdleTime.csv')\n</code></pre>\n<p>The Input data looks like this:</p>\n<p><a href=\"https://i.sstatic.net/maLyO.png\" rel=\"nofollow noreferrer\">input data</a></p>\n<p>Expected output:</p>\n<p><a href=\"https://i.sstatic.net/EVxAf.png\" rel=\"nofollow noreferrer\">expected output</a></p>\n<p>Although some of the values don't get calculated correctly.\nE.g.\n<a href=\"https://i.sstatic.net/kjY8q.png\" rel=\"nofollow noreferrer\">error 1</a></p>\n<p><a href=\"https://i.sstatic.net/LvEB5.png\" rel=\"nofollow noreferrer\">error 2</a></p>\n<p>As you can see, in the first picture there is a row with a negative idle time. Because we sorted the dataframe by end time, we sometimes run into the issue that a transaction at a later row has an earlier start time than the end time of the previous transaction(c.f error 1). In this case the idle time should be updated whenever transaction meets the following two conditions:</p>\n<ol>\n<li>the end_station_id of the transaction, for which the idle time is being calculated, is the same as the start station id of the transaction, over which the for loop is currently iterating.</li>\n<li>the transaction, over which the for loop is currently iterating, has a later start time than the end_time of the transaction, for which we calulate the idle time</li>\n</ol>\n<p>In the error above(c.f. screen snippets) this does not occur and we cannot figure out the reason. Any help would be appreciated</p>\n",
    "is_answered": true,
    "view_count": 71,
    "answer_count": 1
  },
  {
    "title": "ModuleNotFoundError: No module named &#39;sklearn.ensemble._bagging&#39;",
    "link": "https://stackoverflow.com/questions/76394823/modulenotfounderror-no-module-named-sklearn-ensemble-bagging",
    "tags": [
      "github",
      "data-science",
      "python-3.7",
      "data-science-experience",
      "github-issues"
    ],
    "body": "<blockquote>\n<p>ModuleNotFoundError: No module named 'sklearn.ensemble._bagging'</p>\n</blockquote>\n<p>Which version is suitable of <code>scikit</code> learn for the above error?</p>\n<p>I am facing this issue when I am using the python 3.7 version. And I can't update the version.</p>\n",
    "is_answered": false,
    "view_count": 102,
    "answer_count": 0
  },
  {
    "title": "Create tables using OptBinning with custom bins",
    "link": "https://stackoverflow.com/questions/76392184/create-tables-using-optbinning-with-custom-bins",
    "tags": [
      "python",
      "data-science",
      "analytics",
      "optbinning"
    ],
    "body": "<p>I want to use the library <a href=\"/questions/tagged/optbinning\" class=\"post-tag\" title=\"show questions tagged &#39;optbinning&#39;\" aria-label=\"show questions tagged &#39;optbinning&#39;\" rel=\"tag\" aria-labelledby=\"tag-optbinning-tooltip-container\">optbinning</a> to create tables with all the metrics, but under the assumption that I already have all the bins. I don't want to optimize the binning process, I just want the tables with my current bins. Despite the fact that I've found a &quot;solution&quot;, not sure if there's a bug or something I'm missing in the parameters. Here is my example:\nFirst, I create a fake dataset:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import random\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport pandas as pd\nfrom optbinning import BinningProcess, OptimalBinning\n\n\n# Set seed for reproducibility\nrandom.seed(42)\n\n# Generate fake data\ndata = {\n    'GB': [random.choice([0, 1]) for _ in range(2000)],\n    'Period': [(datetime(2021, 1, 1) + timedelta(days=random.randint(0, 731))).strftime(&quot;%m/%Y&quot;) for _ in range(2000)],\n    'Age': [random.randint(18, 80) if random.random() &gt; 0.2 else None for _ in range(2000)],\n    'L6ag': [random.randint(0, 9) if random.random() &gt; 0.2 else None for _ in range(2000)],\n    'L_3M': [chr(random.randint(65, 90)) if random.random() &gt; 0.2 else None for _ in range(2000)],\n    'M36m': [random.randint(0, 1000) for _ in range(2000)],\n    'Balance': [random.randint(0, 100000) for _ in range(2000)]\n}\n\n# Create DataFrame\ndf = pd.DataFrame(data)\ndf\n\nThen, I want to for example create a table for Age using the following bins: custom_bins = [28, 37, 63, 67]\nSo, i use the following code:\n# Define your custom bins\ncustom_bins = [28, 37, 63, 67]\n\n# Define the binning object\noptb = OptimalBinning(name=&quot;Age&quot;, dtype=&quot;numerical&quot;, user_splits=custom_bins)\n\n# Fit the binning object\noptb.fit(df[&quot;Age&quot;], df[&quot;GB&quot;]) # GB is your target variable\n\noptb.binning_table.build()\n\n</code></pre>\n<p>And I get the following table which miss the first bin (-inf to 28):\n<img src=\"https://i.sstatic.net/TBfNt.png\" alt=\"enter image description here\" /></p>\n<p>If I try using the user_splits_fixed parameter to &quot;force&quot; each value on the bins, the result is even worse</p>\n<pre><code># Define your custom bins\ncustom_bins = [28, 37, 63, 67]\nuser_splits_fixed = [True, True,  True, True] \n\n# Define the binning object\noptb = OptimalBinning(name=&quot;Age&quot;, dtype=&quot;numerical&quot;, user_splits=custom_bins, user_splits_fixed=user_splits_fixed)\n\n# Fit the binning object\noptb.fit(df[&quot;Age&quot;], df[&quot;GB&quot;]) # GB is your target variable\n\noptb.binning_table.build()\n</code></pre>\n<p><img src=\"https://i.sstatic.net/ZpZqD.png\" alt=\"enter image description here\" /></p>\n<p>Any help would be more than appreciated</p>\n<p>I would love to get a proper code to produce the table maintaining the original bins provided by the user</p>\n",
    "is_answered": false,
    "view_count": 917,
    "answer_count": 0
  },
  {
    "title": "How do I implement a 30-day cutoff in the Chow test to analyze changes in time series data after a significant event?",
    "link": "https://stackoverflow.com/questions/76384940/how-do-i-implement-a-30-day-cutoff-in-the-chow-test-to-analyze-changes-in-time-s",
    "tags": [
      "python",
      "time-series",
      "data-science",
      "data-analysis"
    ],
    "body": "<p>I am using the <a href=\"https://github.com/David-Woroniuk/chowtest\" rel=\"nofollow noreferrer\">Chow test</a> python library to analyze structural change in time series data potentially caused by a significant event. Since each data point represents a day, I set <code>last_index</code> to the day before the event and <code>first_index</code> to the event. This means the data is broken into two chunks:</p>\n<ul>\n<li>Chunk 1: All data occurring before the event</li>\n<li>Chunk 2: All data occurring on and after the event\nThis is working fine, but I also want to analyze potential changes in the 30-day period following the significant event. In other words, I want to use a cutoff of 30 days rather than a single day, if that makes sense.</li>\n</ul>\n<p>So far, I have tried to create a larger period cutoff by setting <code>first_index</code> to 30 days after the event. This means the data is broken into two chunks:</p>\n<ul>\n<li>Chunk 1: All data occurring before the event</li>\n<li>Chunk 2: All data occurring 30 days later and beyond\nThis excludes data occurring in days 1-29. Is it better to exclude the data in this period, or is it better to keep the data when conducting the Chow test with a 30-day cutoff? Should I be using multi-cutoff analysis, where the first cutoff is the event and the second cutoff is 30 days following the event?\ntl;dr: If I want to use the Chow test to analyze changes that occurred 30 days after an event in the data, what's the best way to go about this?</li>\n</ul>\n",
    "is_answered": false,
    "view_count": 80,
    "answer_count": 0
  },
  {
    "title": "How can I separate data in a MATLAB dat file into proper columns?",
    "link": "https://stackoverflow.com/questions/76384247/how-can-i-separate-data-in-a-matlab-dat-file-into-proper-columns",
    "tags": [
      "dataframe",
      "matlab",
      "data-science"
    ],
    "body": "<p>I am trying to read a dat file in MATLAB but my data is not correctly separated into columns. All of the data is in one single column.</p>\n<p>I am not sure how to separate them into proper columns</p>\n<p>What I am trying to do is to have the data separated into column like so:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Column A</th>\n<th>Column B</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>2</td>\n</tr>\n<tr>\n<td>3</td>\n<td>4</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>but what my file shows is:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Column A</th>\n<th>Column B</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1 2</td>\n<td></td>\n</tr>\n<tr>\n<td>3 4</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n</div>",
    "is_answered": false,
    "view_count": 62,
    "answer_count": 1
  },
  {
    "title": "Generating means from bivariate gaussian distribution",
    "link": "https://stackoverflow.com/questions/76375459/generating-means-from-bivariate-gaussian-distribution",
    "tags": [
      "statistics",
      "data-science",
      "gaussian-process",
      "gaussian-mixture-model"
    ],
    "body": "<p>I am reading the book &quot;The elements of statistical learning&quot; Ch 2 and on page 16 there is this line - 'First we generate 10 means m_k from a bivariate gaussian distribution N((1,0),I) and labelled BLUE. Similarly, 10 more were drawn from N((0,1),I) and labelled ORANGE. Then for each class we generate 100 observations,...'</p>\n<p>I m unable to understand this paragraph. I have following questions:\nQ1 What does generating 10 means from a bivariate gaussian distribution implies ? How can we generate mean ? If there is any mathematical formula please do tell.</p>\n<p>Q2 Difference between N((1,0),I) and N((0,1),I) ?\nDoes 1st implies mean = 1 and variance = 0 and second one's vice-versa ?</p>\n<p>I don't know about clustering yet since I thought I was going through supervised learning and clustering comes under the category of unsupervised learning. Should I learn about clustering first to understand this paragraph ?</p>\n",
    "is_answered": false,
    "view_count": 43,
    "answer_count": 1
  },
  {
    "title": "How to add a fixed value to the Y axis in Altair?",
    "link": "https://stackoverflow.com/questions/76361820/how-to-add-a-fixed-value-to-the-y-axis-in-altair",
    "tags": [
      "python",
      "data-science",
      "altair"
    ],
    "body": "<p>I'm new to Altair and I'd like to add a two rule lines on the Y axis of a stock progression in my line chart.</p>\n<p>The DataFrame <code>symbol_data</code> has Date, Symbol, Name, Close, and I added another column &quot;StdDev&quot; which is the standard deviation for the entire data set.</p>\n<pre class=\"lang-py prettyprint-override\"><code>chart = alt.Chart(symbol_data).mark_line(point=True).encode(\n    x=alt.X(&quot;Date:T&quot;, title=&quot;Dates&quot;),\n    y=alt.X(&quot;Close&quot;, title=&quot;Close price&quot;),\n  )\n)\n\nline = alt.Chart(symbol_data).mark_rule(\n    color=&quot;red&quot;, \n  ).encode(\n    y=alt.Y(&quot;mean(Close)&quot; + StdDev),\n)\n\nline2 = alt.Chart(symbol_data).mark_rule(\n    color=&quot;blue&quot;,\n  ).encode(\n    y=alt.Y(&quot;mean(Close)&quot; - StdDev),\n)\n\nchart + line + line2\n</code></pre>\n<p>I'm trying to add the standard deviation lines by adding and subtracting from the Y axis values.  I know this isn't valid but is there a way to do what I'm trying to do here?</p>\n<pre class=\"lang-py prettyprint-override\"><code>...  \n    y=alt.Y(&quot;mean(Close)&quot; + StdDev),\n... \n</code></pre>\n<p>I've tried adding a mark area which is conceptually close to what I want but I want two rule lines.</p>\n<pre class=\"lang-py prettyprint-override\"><code>line2 = alt.Chart(symbol_data).mark_area(\n    opacity=0.5, color=&quot;gray&quot;\n      ).encode(\n      x=alt.X(&quot;Date:T&quot;),\n      y=alt.Y(&quot;max(Close):Q&quot;),\n      y2=alt.Y2(&quot;StdDev&quot;),\n  )\n</code></pre>\n<p>Update: I was able to get the desired result but I had to add columns to the DataFrame; one for the +1 standard deviation and another for -1 standard deviation.  I'd still like to know if I can add or subtract from the Y axis values.</p>\n<pre class=\"lang-py prettyprint-override\"><code>  upper_std = alt.Chart(symbol_data).mark_rule(\n      color=&quot;green&quot;,\n      strokeDash=(6, 2)).encode(\n      y=alt.Y(&quot;Std_plus:Q&quot;),\n  )\n      \n  line = alt.Chart(symbol_data).mark_rule(\n      color=&quot;red&quot;, \n      strokeWidth=2, \n      strokeDash=(5, 2)).encode(\n      y=alt.Y(&quot;mean(Close):Q&quot;),\n  )\n      \n  lower_std = alt.Chart(symbol_data).mark_rule(\n      color=&quot;blue&quot;,\n      strokeDash=(6, 2)).encode(\n      y=alt.Y(&quot;Std_minus:Q&quot;),\n  )\n</code></pre>\n",
    "is_answered": false,
    "view_count": 410,
    "answer_count": 1
  },
  {
    "title": "Why I can&#39;t load dataset from seaborn",
    "link": "https://stackoverflow.com/questions/76345909/why-i-cant-load-dataset-from-seaborn",
    "tags": [
      "python",
      "pandas",
      "numpy",
      "seaborn",
      "data-science"
    ],
    "body": "<p>I am trying to load the dataset iris from seaborn in my spyder\nMy code was simple:</p>\n<pre><code>import seaborn as sns\ndf = sns.load_dataset('iris')\nprint(df)\n</code></pre>\n<p>I am not getting output. I have tried in Jupiter notebook also, not getting any output</p>\n<p>What to do?</p>\n<p>I tried to load a dataset form seaborn named: 'iris' and also 'flights' but I can't.</p>\n",
    "is_answered": false,
    "view_count": 2049,
    "answer_count": 1
  },
  {
    "title": "import langchain =&gt; Error : TypeError: issubclass() arg 1 must be a class",
    "link": "https://stackoverflow.com/questions/76313592/import-langchain-error-typeerror-issubclass-arg-1-must-be-a-class",
    "tags": [
      "python",
      "nlp",
      "data-science",
      "chatbot",
      "langchain"
    ],
    "body": "<p>I want to use langchain for my project.</p>\n<p>so I installed it using following command : <code>pip install langchain</code></p>\n<p>but While importing &quot;langchain&quot; I am facing following Error:</p>\n<pre><code>File /usr/lib/python3.8/typing.py:774, in _GenericAlias.__subclasscheck__(self, cls)\n    772 if self._special:\n    773     if not isinstance(cls, _GenericAlias):\n--&gt; 774         return issubclass(cls, self.__origin__)\n    775     if cls._special:\n    776         return issubclass(cls.__origin__, self.__origin__)\n\nTypeError: issubclass() arg 1 must be a class\n</code></pre>\n<p>Any one who can solve this error ?</p>\n",
    "is_answered": true,
    "view_count": 33189,
    "answer_count": 7
  },
  {
    "title": "How can I fix the &#39;arrays must be in the same length&#39; error in my Python web scraping code using Selenium and BeautifulSoup?",
    "link": "https://stackoverflow.com/questions/76306008/how-can-i-fix-the-arrays-must-be-in-the-same-length-error-in-my-python-web-scr",
    "tags": [
      "python",
      "selenium-webdriver",
      "web-scraping",
      "beautifulsoup",
      "data-science"
    ],
    "body": "<pre><code>import bs4\nimport pandas as pd\nimport re\nfrom bs4 import BeautifulSoup\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.common.action_chains import ActionChains\nimport time\nimport requests\n\n#Function that open the url with selenium, and return the page source\ndef getPageBySel(url):\n    options = webdriver.ChromeOptions()\n    options.add_argument(&quot;--disable-dev-shm-usage&quot;)\n    options.add_argument(&quot;--remote-debugging-port=9222&quot;)\n    options.add_argument(&quot;--window-size=1920x1080&quot;)\n    driver = webdriver.Chrome(options=options)\n    driver.get(url)\n    page = driver.page_source\n    driver.quit()\n    return page\n\n\noffset=0\nhotelsArr = { f'https://www.booking.com/searchresults.he.html?aid=397594&amp;label=gog235jc-1DCAEoggI46AdIDlgDaGqIAQGYAQ64ARfIAQzYAQPoAQH4AQKIAgGoAgO4ArvKrqIGwAIB0gIkMWJjMjhhNzItNDZhNC00NDZmLTk1YzgtNjhiOWM0NmM0NDA42AIE4AIB&amp;dest_id=-2601889&amp;dest_type=city&amp;group_adults=2&amp;req_adults=2&amp;no_rooms=1&amp;group_children=0&amp;checkin=2024-01-04&amp;checkout=2024-01-07&amp;req_children=0&amp;offset={offset}',\n            f'https://www.booking.com/searchresults.he.html?ss=Manchester%2C+Greater+Manchester%2C+United+Kingdom&amp;ssne=%D7%9E%D7%A0%D7%A6%27%D7%A1%D7%98%D7%A8&amp;ssne_untouched=%D7%9E%D7%A0%D7%A6%27%D7%A1%D7%98%D7%A8&amp;efdco=1&amp;label=gog235jc-1DCAEoggI46AdIDlgDaGqIAQGYAQ64ARfIAQzYAQPoAQH4AQKIAgGoAgO4Ar3Xs6IGwAIB0gIkMWU3MTc2OTUtZDZkNi00NzFhLTk2NWYtMDczNjk5MDNhN2U52AIE4AIB&amp;aid=397594&amp;lang=he&amp;sb=1&amp;src_elem=sb&amp;src=index&amp;dest_id=-2602512&amp;dest_type=city&amp;ac_position=0&amp;ac_click_type=b&amp;ac_langcode=en&amp;ac_suggestion_list_length=5&amp;search_selected=true&amp;search_pageview_id=39e046de839803a2&amp;ac_meta=GhAzOWUwNDZkZTgzOTgwM2EyIAAoATICZW46Ck1hbmNoZXN0ZXJAAEoAUAA%3D&amp;group_adults=2&amp;checkin=2024-01-04&amp;checkout=2024-01-07&amp;no_rooms=1&amp;group_children=0&amp;sb_travel_purpose=leisure&amp;offset={offset}',\n            f'https://www.booking.com/searchresults.he.html?aid=7961375&amp;lang=he&amp;sid=ff4607c90e3e0d79763672e65389c94b&amp;sb=1&amp;sb_lp=1&amp;src=index&amp;src_elem=sb&amp;error_url=https%3A%2F%2Fwww.booking.com%2Findex.he.html%3Faid%3D7961375%26sid%3Dff4607c90e3e0d79763672e65389c94b%26sb_price_type%3Dtotal%26%26&amp;ss=Liverpool%2C+Merseyside%2C+United+Kingdom&amp;is_ski_area=&amp;checkin_year=&amp;checkin_month=&amp;checkout_year=&amp;checkout_month=&amp;efdco=1&amp;group_adults=2&amp;group_children=0&amp;no_rooms=1&amp;b_h4u_keep_filters=&amp;from_sf=1&amp;ss_raw=Liverpool&amp;ac_position=0&amp;ac_langcode=en&amp;ac_click_type=b&amp;ac_meta=GhBhNTQ5NGFhODM2MTgwMjFkIAAoATICZW46CUxpdmVycG9vbEAASgBQAA%3D%3D&amp;dest_id=-2601422&amp;dest_type=city&amp;iata=LPL&amp;place_id_lat=53.4109&amp;place_id_lon=-2.97811&amp;search_pageview_id=a5494aa83618021d&amp;search_selected=true&amp;search_pageview_id=a5494aa83618021d&amp;checkin=2024-01-04&amp;checkout=2024-01-07&amp;ac_suggestion_list_length=5&amp;ac_suggestion_theme_list_length=0&amp;offset={offset}',\n            f'https://www.booking.com/searchresults.he.html?ss=Birmingham%2C+West+Midlands%2C+United+Kingdom&amp;ssne=%D7%91%D7%A8%D7%9E%D7%99%D7%A0%D7%92%D7%94%D7%90%D7%9D&amp;ssne_untouched=%D7%91%D7%A8%D7%9E%D7%99%D7%A0%D7%92%D7%94%D7%90%D7%9D&amp;efdco=1&amp;label=gen173nr-1BCAEoggI46AdIM1gEaGqIAQGYAQ64ARfIAQzYAQHoAQGIAgGoAgO4AsL2s6IGwAIB0gIkM2VlZjk2YjEtMDJhYi00YmExLTg1NmEtOTIxYTNhNzdhMWQ22AIF4AIB&amp;sid=ff4607c90e3e0d79763672e65389c94b&amp;aid=304142&amp;lang=he&amp;sb=1&amp;src_elem=sb&amp;src=index&amp;dest_id=-2589989&amp;dest_type=city&amp;ac_position=0&amp;ac_click_type=b&amp;ac_langcode=en&amp;ac_suggestion_list_length=5&amp;search_selected=true&amp;search_pageview_id=acea4ea102160498&amp;ac_meta=GhBhY2VhNGVhMTAyMTYwNDk4IAAoATICZW46BWJpcm1pQABKAFAA&amp;group_adults=2&amp;checkin=2024-01-04&amp;checkout=2024-01-07&amp;no_rooms=1&amp;group_children=0&amp;sb_travel_purpose=leisure&amp;offset={offset}',\n            f'https://www.booking.com/searchresults.he.html?ss=%D7%90%D7%93%D7%99%D7%A0%D7%91%D7%95%D7%A8%D7%95%2C+%D7%A1%D7%A7%D7%95%D7%98%D7%9C%D7%A0%D7%93%2C+%D7%91%D7%A8%D7%99%D7%98%D7%A0%D7%99%D7%94&amp;ssne=%D7%92%D7%9C%D7%90%D7%96%D7%92%D7%95&amp;ssne_untouched=%D7%92%D7%9C%D7%90%D7%96%D7%92%D7%95&amp;efdco=1&amp;label=gen173nr-1BCAEoggI46AdIM1gEaGqIAQGYAQ64ARfIAQzYAQHoAQGIAgGoAgO4AoP4s6IGwAIB0gIkZmQ0YjhjNTMtZTc2ZS00NDZkLThmMmEtNmUyZDk3YTAwZWJl2AIF4AIB&amp;sid=ff4607c90e3e0d79763672e65389c94b&amp;aid=304142&amp;lang=he&amp;sb=1&amp;src_elem=sb&amp;src=index&amp;dest_id=-2595386&amp;dest_type=city&amp;ac_position=0&amp;ac_click_type=b&amp;ac_langcode=he&amp;ac_suggestion_list_length=5&amp;search_selected=true&amp;search_pageview_id=ec504f01a010003c&amp;ac_meta=GhBlYzUwNGYwMWEwMTAwMDNjIAAoATICaGU6A2VkaUAASgBQAA%3D%3D&amp;group_adults=2&amp;checkin=2024-01-04&amp;checkout=2024-01-07&amp;no_rooms=1&amp;group_children=0&amp;sb_travel_purpose=leisure&amp;offset={offset}',\n            f'https://www.booking.com/searchresults.he.html?ss=%D7%99%D7%95%D7%A8%D7%A7&amp;ssne=%D7%99%D7%95%D7%A8%D7%A7&amp;ssne_untouched=%D7%99%D7%95%D7%A8%D7%A7&amp;label=gen173nr-1BCAEoggI46AdIM1gEaGqIAQGYAQ64ARfIAQzYAQHoAQGIAgGoAgO4Auj6s6IGwAIB0gIkM2IwYTU3ODgtNDJiYS00ZDk0LWI0MDAtNGU3M2U3ZDlkNzM42AIF4AIB&amp;sid=ff4607c90e3e0d79763672e65389c94b&amp;aid=304142&amp;lang=he&amp;sb=1&amp;src_elem=sb&amp;src=index&amp;dest_id=-2612321&amp;dest_type=city&amp;group_adults=2&amp;no_rooms=1&amp;group_children=0&amp;sb_travel_purpose=leisure&amp;offset={offset}'\n}\nx=0\nhotels=[]\nlinks=[]\nprices=[]\nfor x in hotelsArr:\n    offset=0\n    page = x\n# Loop that run on the first 40 pages (offset+25 each time)\n    while offset &lt; 980:\n    \n        \n        while True: #Loop that make sure that the page loaded successfully \n            temp=getPageBySel(page)\n            soup = BeautifulSoup(temp, 'html.parser')\n            if len(soup(&quot;h3&quot;,{&quot;class&quot;:&quot;a4225678b2&quot;}))&gt;0:\n                break\n            else:\n                time.sleep(1.5)\n                \n        for element in soup.select(&quot;.fcab3ed991.fbd1d3018c.e729ed5ab6&quot;):\n            # Find the price\n            price = element.get_text(strip=True) if element else 'N/A'\n            \n\n            # Append the price to the prices list\n           \n            prices.append(price)\n            # Extract the hotel name, link, and price\n        for element in soup(&quot;h3&quot;, {&quot;class&quot;: &quot;a4225678b2&quot;}):\n            # Find the hotel link\n            link = element('a')[0]['href']\n            \n            # Find the hotel name\n            name = element.select_one('.fcab3ed991.a23c043802').get_text() if element else 'N/A'\n            \n         \n          \n            # Append the data to the respective lists\n            hotels.append(name)\n            links.append(link)\n            \n\n        \n        offset= offset+25 #Move to the next page\n    \n# Ensure all lists have the same length\nlength = len(hotels)\nif len(links) &lt; length:\n    length = len(links)\nif len(prices) &lt; length:\n    length = len(prices)\n#Take the data into dataframe\ndf = pd.DataFrame({\n    'Hotel':hotels, 'Link':links, 'Prices':prices\n})\nprint(df)\n\n#Df to csv\ndf.to_csv('hotels_list30.csv', index=True)\n\n\n</code></pre>\n<p>I've get an error in this code, all the arrys must be in the same length, how can i fix this problem? tried everythins, the condition in the end and try to put 'NA' in the black cells.</p>\n<p>I've tried to solve this but it didn't work, i need to get a df with the hotels, link and price cols, through scrapping. maybe i've got a mistake there. this is why i also put the booking links.</p>\n",
    "is_answered": true,
    "view_count": 72,
    "answer_count": 1
  },
  {
    "title": "Load Teachable Machine Model in HTML and JavaScript Code",
    "link": "https://stackoverflow.com/questions/76304762/load-teachable-machine-model-in-html-and-javascript-code",
    "tags": [
      "javascript",
      "html",
      "tensorflow",
      "data-science",
      "teachable-machine"
    ],
    "body": "<p>I created a teachable machine audio project and uploaded it and used url in html code that is also provided by teachable machine and when I run in the browser it is working. But now I want to use downloaded model instead of uploaded url:\nwhat I tried:\nin place of\nconst URL = &quot;https://teachablemachine.withgoogle.com/models/7cEccJReh/&quot;;\nI used</p>\n<ol>\n<li>const URL = &quot;./&quot;;\nError: Uncaught (in promise) Error: Unsupported URL scheme in metadata URL: ./metadata.json. Supported schemes are: http://, https://, and (node.js-only) file://</li>\n<li>const URL = &quot;file://&quot;;</li>\n<li>const URL = &quot;file:///&quot;;\nError: Uncaught (in promise) ReferenceError: require is not defined</li>\n</ol>\n<p>My project files: index.html, metadata.json, model.json, weights.bin are all in same folder.</p>\n<p>here is my code</p>\n<pre><code>&lt;div&gt;Teachable Machine Audio Model&lt;/div&gt;\n&lt;button type=&quot;button&quot; onclick=&quot;init()&quot;&gt;Start&lt;/button&gt;\n&lt;div id=&quot;label-container&quot;&gt;&lt;/div&gt;\n&lt;script src=&quot;https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.3.1/dist/tf.min.js&quot;&gt;&lt;/script&gt;\n&lt;script src=&quot;https://cdn.jsdelivr.net/npm/@tensorflow-models/speech-commands@0.4.0/dist/speech-commands.min.js&quot;&gt;&lt;/script&gt;\n\n&lt;script type=&quot;text/javascript&quot;&gt;\n    // more documentation available at\n    // https://github.com/tensorflow/tfjs-models/tree/master/speech-commands\n\n    // the link to your model provided by Teachable Machine export panel\n    // const URL = &quot;https://teachablemachine.withgoogle.com/models/7cEccJReh/&quot;;\n    const URL = &quot;file:///&quot;\n\n    async function createModel() {\n        const checkpointURL = URL + &quot;model.json&quot;; // model topology\n        const metadataURL = URL + &quot;metadata.json&quot;; // model metadata\n\n        const recognizer = speechCommands.create(\n            &quot;BROWSER_FFT&quot;, // fourier transform type, not useful to change\n            undefined, // speech commands vocabulary feature, not useful for your models\n            checkpointURL,\n            metadataURL);\n\n        // check that model and metadata are loaded via HTTPS requests.\n        await recognizer.ensureModelLoaded();\n\n        return recognizer;\n    }\n\n    async function init() {\n        const recognizer = await createModel();\n        const classLabels = recognizer.wordLabels(); // get class labels\n        const labelContainer = document.getElementById(&quot;label-container&quot;);\n        for (let i = 0; i &lt; classLabels.length; i++) {\n            labelContainer.appendChild(document.createElement(&quot;div&quot;));\n        }\n\n        // listen() takes two arguments:\n        // 1. A callback function that is invoked anytime a word is recognized.\n        // 2. A configuration object with adjustable fields\n        recognizer.listen(result =&gt; {\n            const scores = result.scores; // probability of prediction for each class\n            // render the probability scores per class\n            for (let i = 0; i &lt; classLabels.length; i++) {\n                const classPrediction = classLabels[i] + &quot;: &quot; + result.scores[i].toFixed(2);\n                labelContainer.childNodes[i].innerHTML = classPrediction;\n            }\n        }, {\n            includeSpectrogram: true, // in case listen should return result.spectrogram\n            probabilityThreshold: 0.75,\n            invokeCallbackOnNoiseAndUnknown: true,\n            overlapFactor: 0.50 // probably want between 0.5 and 0.75. More info in README\n        });\n\n        // Stop the recognition in 5 seconds.\n        // setTimeout(() =&gt; recognizer.stopListening(), 5000);\n    }\n&lt;/script&gt;\n\n</code></pre>\n",
    "is_answered": false,
    "view_count": 1835,
    "answer_count": 1
  },
  {
    "title": "Mysterious &quot;Segmentation fault&quot; error when running a Fortran program",
    "link": "https://stackoverflow.com/questions/76303952/mysterious-segmentation-fault-error-when-running-a-fortran-program",
    "tags": [
      "fortran",
      "data-science",
      "hdf5",
      "netcdf4"
    ],
    "body": "<p>P.P.P.P.S\nFinally, I have found out the reason of the problem.</p>\n<p>Errors occurs on the variable <code>vbuf</code>, which is a <code>Pointer</code> type and its shape was defined previously. The shape of <code>vbuf</code> supposed to be <code>(\\nland,tidx2-tidx1+1\\)</code> but it was uncorrectly defined. The <code>SIGSEGV, segmentation fault</code> may not be useful to resolve the problem.</p>\n<hr />\n<p>I encountered a &quot;SIGSEGV, segmentation fault&quot; error when running a Fortran program that was reading a netCDF file created by another programming language (Julia or Python). The program was compiled by ifort (intel Fortran Compiler). The error messages were:</p>\n<pre><code>forrtl: severe (174): SIGSEGV, segmentation fault occurred\nImage              PC                Routine            Line        Source\nclm.x              000000000086F103  Unknown               Unknown  Unknown\nlibpthread-2.17.s  00002B0BB1AD65E0  Unknown               Unknown  Unknown\nlibhdf5.so.103.1.  00002B0BB296CB60  H5SL_item             Unknown  Unknown\nlibhdf5.so.103.1.  00002B0BB279B825  Unknown               Unknown  Unknown\nlibhdf5.so.103.1.  00002B0BB27C6960  H5D__read             Unknown  Unknown\nlibhdf5.so.103.1.  00002B0BB27C5FFF  H5Dread               Unknown  Unknown\nlibnetcdf.so.15.0  00002B0BB103F4B6  NC4_get_vars          Unknown  Unknown\nlibnetcdf.so.15.0  00002B0BB103E6E6  NC4_get_vara          Unknown  Unknown\nlibnetcdf.so.15.0  00002B0BB0FBF78F  NC_get_vara           Unknown  Unknown\nlibnetcdff.so.6.2  00002B0BB1350CE2  nf_get_vara_real_     Unknown  Unknown\nlibnetcdff.so.6.2  00002B0BB13AABB9  netcdf_mp_nf90_ge     Unknown  Unknown\nclm.x              00000000005BB7B7  gswp3data_mp_ncdf         640  gswp3data.f90\nclm.x              00000000005B7EC5  gswp3data_mp_read         538  gswp3data.f90\nclm.x              00000000005A2086  gswp3data_mp_metd         325  gswp3data.f90\nclm.x              00000000005FC968  forcedata_mp_read          58  forcedata.f90\nclm.x              00000000007679B2  MAIN__                     83  CLM.f90\nclm.x              0000000000410922  Unknown               Unknown  Unknown\nlibc-2.17.so       00002B0BB1F08C05  __libc_start_main     Unknown  Unknown\nclm.x              0000000000410829  Unknown               Unknown  Unknown\n</code></pre>\n<p>I tracked this error and I am confident that it occurs when I call the <code>nf90_get_var</code> function:</p>\n<pre><code>   SUBROUTINE ncdf_read(fid,vid,tidx1,tidx2,vbuf)\n\n      integer,  intent(in) :: fid\n      integer,  intent(in) :: vid\n      integer,  intent(in) :: tidx1      ! begin time record to read\n      integer,  intent(in) :: tidx2      ! end time record to read\n      real(r4), intent(out):: vbuf(:,:)  ! pre-read buffer for time record between &lt;tidx1:tidx2&gt;\n\n      if (fid &gt; 0) then\n         write(6,*) 'debug', tidx1, tidx2, nland\n         call sanity(nf90_get_var(fid,vid,vbuf,start=(/1,tidx1/),count=(/nland,tidx2-tidx1+1/)))\n      end if\n\n   END SUBROUTINE ncdf_read\n\n</code></pre>\n<p>The <code>sanity</code> is a irrelated function, which shows a error message. It seems that there is something wrong with the NetCDF or HDF5 library. However, I have carefully checked the HDF5 and NetCDF libraries, and I am sure they are correctly installed. (As described below, I can use a simple Fortran program to read this data.)</p>\n<p>Also, a &quot;SIGSEGV segmentation fault&quot; always occurs when there is a memory overflow or a pointer out of bounds. This reason has also been rejected because the input NetCDF field contains only thousands of float numbers.</p>\n<p>This program is part of a large model, so I must use Fortran to read data. Now I believe that this issue <strong>is related to some of my environmental variables</strong> or some mysterious errors in Julia/Python script, and I will elaborate on the reasons for making this judgment below. But I can't find the exact reason</p>\n<h2>What did I tried</h2>\n<p>Firstly, I wrote a small Fortran program to read the same data. No errors occurred and the program was able to read the data correctly.</p>\n<p>My friend compiled the program and generated the entire model in his environment (we both have the same version of netCDF and HDF lib). He uses the same version ifort compiler as me. However, he was able to read the netCDF data normally, and when <strong>I tried to read the netCDF file, which was created by his Julia script, my Fortran program ran normally</strong>.</p>\n<p>Now I have no idea where the problem lies or what ways to correct it. I would greatly appreciate it if someone could provide me with advice</p>\n<p>P.S. The Julia script I used to create the input netCDF data are as follows. By the way, I have tried the NetCDF4 or NetCDF4_classic (which do not use HDF lib). The error occurs both way.</p>\n<pre><code>#!julia\n\nusing NCDatasets\nusing Dates\nusing DataStructures\n\n...\n\n    fo = Dataset(foname,&quot;c&quot;)\n\n    defDim(fo, &quot;lon&quot;, 1)\n    defDim(fo, &quot;lat&quot;, 1)\n    defDim(fo, &quot;land&quot;, 1)\n    defDim(fo, &quot;time&quot;, Inf)\n\n    xvar = defVar(fo, &quot;lon&quot;, Float32, (&quot;lon&quot;,), attrib=OrderedDict(&quot;units&quot;=&gt;&quot;degrees_E&quot;))\n    yvar = defVar(fo, &quot;lat&quot;, Float32, (&quot;lat&quot;,), attrib=OrderedDict(&quot;units&quot;=&gt;&quot;degrees_N&quot;))\n    lvar = defVar(fo, &quot;land&quot;, Int32, (&quot;land&quot;,), attrib=OrderedDict(&quot;compress&quot;=&gt;&quot;y x&quot;))\n    tvar = defVar(fo, &quot;time&quot;, Float32, (&quot;time&quot;,), attrib=OrderedDict(&quot;units&quot;=&gt;timunits))\n    vvar = defVar(fo, vname, Float32, (&quot;land&quot;,&quot;time&quot;), attrib=OrderedDict(&quot;units&quot;=&gt;varunits,&quot;_FillValue&quot;=&gt;convert(Float32,fillvalue)))\n\n    xvar[:] = clon\n    yvar[:] = clat\n    lvar[:] = 1 # only digger one grid\n    tvar[:] = tim[:]\n    vvar[:,:] = var[:,:]\n\n    close(fo)\n\n  end\nend\n\n</code></pre>\n<p>P.P.S I used dbg to trace the error message. The output massage are:</p>\n<pre><code>Program received signal SIGSEGV, Segmentation fault.\n0x00002aaaac6afb60 in H5SL_item () from /ddn_lustre/weiliren/apps/hdf5-2019/lib/libhdf5.so.103\nMissing separate debuginfos, use: debuginfo-install cyrus-sasl-lib-2.1.26-21.el7.x86_64 glibc-2.17-196.el7.x86_64 keyutils-libs-1.5.8-3.el7.x86_64 krb5-libs-1.15.1-8.el7.x86_64 libcom_err-1.42.9-10.el7.x86_64 libcurl-7.29.0-42.el7.x86_64 libgcc-4.8.5-16.el7.x86_64 libidn-1.28-4.el7.x86_64 libselinux-2.5-11.el7.x86_64 libssh2-1.4.3-10.el7_2.1.x86_64 nspr-4.13.1-1.0.el7_3.x86_64 nss-3.28.4-8.el7.x86_64 nss-softokn-freebl-3.28.3-6.el7.x86_64 nss-util-3.28.4-3.el7.x86_64 openldap-2.4.44-5.el7.x86_64 openssl-libs-1.0.2k-8.el7.x86_64 pcre-8.32-17.el7.x86_64 zlib-1.2.7-17.el7.x86_64\n\n</code></pre>\n<p>and using <code>backtrace</code> it shows:</p>\n<pre><code>(gdb) backtrace\n#0  0x00002aaaac6afb60 in H5SL_item () from /ddn_lustre/weiliren/apps/hdf5-2019/lib/libhdf5.so.103\n#1  0x00002aaaac4de825 in ?? () from /ddn_lustre/weiliren/apps/hdf5-2019/lib/libhdf5.so.103\n#2  0x00002aaaac509960 in H5D__read () from /ddn_lustre/weiliren/apps/hdf5-2019/lib/libhdf5.so.103\n#3  0x00002aaaac508fff in H5Dread () from /ddn_lustre/weiliren/apps/hdf5-2019/lib/libhdf5.so.103\n#4  0x00002aaaaad824b6 in NC4_get_vars () from /ddn_lustre/weiliren/apps/netcdf-2019/lib/libnetcdf.so.15\n#5  0x00002aaaaad816e6 in NC4_get_vara () from /ddn_lustre/weiliren/apps/netcdf-2019/lib/libnetcdf.so.15\n#6  0x00002aaaaad0278f in NC_get_vara () from /ddn_lustre/weiliren/apps/netcdf-2019/lib/libnetcdf.so.15\n#7  0x00002aaaab093ce2 in nf_get_vara_real_ () from /ddn_lustre/weiliren/apps/netcdf-2019/lib/libnetcdff.so.6\n#8  0x00002aaaab0edbb9 in netcdf_mp_nf90_get_var_2d_fourbytereal_ () from /ddn_lustre/weiliren/apps/netcdf-2019/lib/libnetcdff.so.6\n#9  0x00000000005be393 in gswp3data::ncdf_read (fid=65536, vid=5, tidx1=1, tidx2=2928, vbuf=...) at gswp3data.f90:640\n#10 0x00000000005baaa1 in gswp3data::read_files () at gswp3data.f90:538\n#11 0x00000000005a4c62 in gswp3data::metdata_read (lon_points=1, lat_points=1, gmask=0xc811c0, lonw=0xc822c0, lats=0xc82280,\n    longxy=0xc819c0, latixy=0xc81960, mtair=0xc9a9c0, mqair=0xc9aa00, mpres=0xc9aa40, mrainc=0xc9aa80, mrainl=0xc9aac0,\n    mwindu=0xc9ab00, mwindv=0xc9ab40, mdswrf=0xc9ab80, mdlwrf=0xc9abc0, tair_z=0xc9ac00, qair_z=0xc9ac40, wind_z=0xc9ac80)\n    at gswp3data.f90:325\n#12 0x00000000005ff544 in forcedata::read_forcedata () at forcedata.f90:58\n#13 0x00000000007728c8 in colm () at CLM.f90:83\n#14 0x0000000000410922 in main ()\n#15 0x00002aaaabc4bc05 in __libc_start_main () from /lib64/libc.so.6\n#16 0x0000000000410829 in _start ()\n</code></pre>\n<p>P.P.P.S\nForgive me for not replying to everyone again, because the problem that has been bothering me for a week suddenly disappeared without any modifications, and I cannot even reproduce it. But I still want to thank @lastchance for his advice, which is very helpful for my other debugging processes in the future</p>\n",
    "is_answered": false,
    "view_count": 517,
    "answer_count": 0
  },
  {
    "title": "How does python apply filters to specific columns?",
    "link": "https://stackoverflow.com/questions/76298855/how-does-python-apply-filters-to-specific-columns",
    "tags": [
      "python",
      "numpy",
      "data-science"
    ],
    "body": "<p>The NumPy array data has a shape of (1000, ). More specifically, look at the code below:</p>\n<pre><code>data = np.genfromtext(&quot;data.csv&quot;, delimiter=&quot;,&quot;, skip_header=1, names=(&quot;Score&quot;, &quot;GPA&quot;, &quot;Research&quot;))\n</code></pre>\n<p>I want to find out how many students have research experience. I apply this filter(The score column is a binary column):</p>\n<pre><code>data[data[&quot;Research&quot;] == 1]\n</code></pre>\n<p>This returns a new array containing the data of all students with research experience.\nI don't understand how Python applies the boolean mask to only the &quot;Score&quot; column. It should apply the mask to the entire array; what happens if there exists more than one binary column?</p>\n<p>I try to understand the code by separating a new variable for the mask and then passing the mask to the data; the results are the same.</p>\n",
    "is_answered": false,
    "view_count": 62,
    "answer_count": 0
  },
  {
    "title": "hoe to applied example in Sentiment analysis using textblob and naive bayes",
    "link": "https://stackoverflow.com/questions/76298214/hoe-to-applied-example-in-sentiment-analysis-using-textblob-and-naive-bayes",
    "tags": [
      "dataset",
      "data-science",
      "sentiment-analysis",
      "naivebayes",
      "textblob"
    ],
    "body": "<p>Hi i am using amazon food dataset and my project about sentiment analysis i used TextBlob and naive Bayes models and after i get the accuracy for both</p>\n<p>i wants t to applied each model in example like this\n<a href=\"https://i.sstatic.net/Sieyb.png\" rel=\"nofollow noreferrer\">enter image description here</a>\nthis is the code for predicting in TextBlob\n<a href=\"https://i.sstatic.net/HW4jk.png\" rel=\"nofollow noreferrer\">enter image description here</a></p>\n<p>and this for Naive Bayes\n<a href=\"https://i.sstatic.net/4CFQT.png\" rel=\"nofollow noreferrer\">enter image description here</a></p>\n",
    "is_answered": false,
    "view_count": 64,
    "answer_count": 1
  }
]