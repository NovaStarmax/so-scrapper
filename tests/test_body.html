"<p>I’m trying to use qKnowledgeGradient with a fully Bayesian SAAS (Sparse Axis Aligned Subspace) GP  (Gaussian Process) (using SaasFullyBayesianSingleTaskGP) in BoTorch. I'm doing so by writing a new class that inherits from both SaasFullyBayesianSingleTaskGP and FantasizeMixin. Then, I override the fantasize() method to define how fantasy data is generated for this model. I have used 256 samples, warmup of 512, thinning of 1, num_fantasies=2. However, on running the code, I keep getting a shape mismatch error even with raw_samples=1 and num_restarts=1. The error looks like this:</p>\n<pre><code>RuntimeError: shape '[2, 1, 16, 1]' is invalid for input of size 64\n</code></pre>\n<p>I created a custom SAAS GP by inheriting from both SaasFullyBayesianSingleTaskGP and FantasizeMixin, and I overrode the fantasize() method.  I then attempted to use this model with qKnowledgeGradient by setting num_fantasies=2 and reducing raw_samples and num_restarts to 1 (so only a single t‑batch is used).  I expected the acquisition to evaluate successfully and produce a candidate point, but instead, KG fails with the broadcast/reshape error above.</p>\n<p>The error occurs regardless of whether I use the default KG (Knowledge Gradient) implementation or a custom KG that loops over the batch dimension and manually averages over the ensemble and I haven’t been able to eliminate it by changing collapsing batch dimensions also. I even printed the tensor dimension and seems okay to me.</p>\n<p>Below is a minimal version of my code to reproduce the issue.</p>\n<p>Reproducible Minimal Example\nBranin function embedded in 100D</p>\n<pre><code>lb = np.hstack((-5 * np.ones(50), 0 * np.ones(50)))\nub = np.hstack((10 * np.ones(50), 15 * np.ones(50)))\n\ndef branin100(x):\n    assert (x &lt;= ub).all() and (x &gt;= lb).all()\n    x1, x2 = x[19], x[64]\n    t1 = x2 - 5.1 / (4 * math.pi ** 2) * x1 ** 2 + 5 / math.pi * x1 - 6\n    t2 = 10 * (1 - 1 / (8 * math.pi)) * np.cos(x1)\n    return t1 ** 2 + t2 + 10\n</code></pre>\n<p>SAAS GP with custom fantasize() method</p>\n<pre><code>class SaasFullyBayesianSingleTaskGPWithFantasy(SaasFullyBayesianSingleTaskGP, FantasizeMixin):\n    def fantasize(\n        self,\n        X: torch.Tensor,\n        sampler: Optional[MCSampler] = None,\n        num_fantasies: int = 2,\n        **kwargs,\n    ) -&gt; Model:\n        if sampler is None:\n            sampler = SobolQMCNormalSampler(\n                sample_shape=torch.Size([num_fantasies]),\n                collapse_batch_dims=True,\n            )\n        X = torch.as_tensor(\n            X, dtype=self.train_inputs[0].dtype, device=self.train_inputs[0].device\n        )\n        return FantasizeMixin.fantasize(self, X, sampler=sampler, **kwargs)\n</code></pre>\n<p>Running SAASBO with KG</p>\n<pre><code>\ndef run_saasbo_botorch():\n    torch.manual_seed(0)\n    dtype = torch.double\n    device = &quot;cpu&quot;\n    dim = 100\n    lb_torch = torch.zeros(dim, dtype=dtype)\n    ub_torch = torch.ones(dim, dtype=dtype)\n    bounds = torch.stack([lb_torch, ub_torch])\n\n    def f(x): return branin100(x)\n\n    # Initial Sobol samples\n    sobol = SobolEngine(dim, scramble=True, seed=0)\n    X = sobol.draw(4).to(dtype=dtype)  # 4 initial points\n    Y = torch.tensor(\n        [f(lb + (ub - lb) * x.cpu().numpy()) for x in X],\n        dtype=dtype\n    ).unsqueeze(-1)\n\n    train_Y = (Y - Y.mean()) / Y.std()\n\n    # Fit SAAS GP\n    model = SaasFullyBayesianSingleTaskGPWithFantasy(X, train_Y)\n    fit_fully_bayesian_model_nuts(\n        model, warmup_steps=512, num_samples=256, thinning=16\n    )\n\n    # Define posterior transform\n    weights = torch.ones(2, dtype=dtype) / 2\n    post_tf = ScalarizedPosteriorTransform(weights=weights)\n\n    # Define KG acquisition\n    qkg = qKnowledgeGradient(\n        model=model,\n        num_fantasies=2,\n        current_value=train_Y.min(),\n        posterior_transform=post_tf,\n    )\n\n    # Optimize acquisition\n    candidate, _ = optimize_acqf(\n        acq_function=qkg,\n        bounds=bounds,\n        q=1,\n        raw_samples=1,\n        num_restarts=1,\n    )\n\nrun_saasbo_botorch()\n</code></pre>\n<p>Error</p>\n<pre><code>RuntimeError: shape '[2, 1, 16, 1]' is invalid for input of size 64\n</code></pre>\n<p>I don't understand why I keep getting this error and where it is coming from. Any guidance on what might be causing this and how to properly structure the fantasy model in this context would be greatly appreciated!</p>\n<p>Thanks in advance.</p>\n<p>EDIT: I overrode condition_on_observations and changed num fantasies to 64,( code below ) but now I get another error :</p>\n<pre><code>Output shape not equal to that of weights. Output shape is 1 and weights are torch.Size([64]\n</code></pre>\n<p>Code for condition_on_observation -:</p>\n<pre><code>def condition_on_observations(self, X: torch.Tensor, Y: torch.Tensor, **kwargs):\nmodel_batch_ndim = len(self.batch_shape)\n\n    if X.ndim == 2 and Y.ndim == 2:\n        X = X.repeat(self.batch_shape + (1, 1)).contiguous()\n        Y = Y.repeat(self.batch_shape + (1, 1)).contiguous()\n        return super().condition_on_observations(X, Y, **kwargs)\n\n    \n    start_idx = Y.ndim - (2 + model_batch_ndim)\n    model_batch_indices = list(range(start_idx, start_idx + model_batch_ndim))\n    extra_indices = list(range(0, start_idx))\n    remaining_indices = list(range(start_idx + model_batch_ndim, Y.ndim - 2))\n    permute_order = model_batch_indices + extra_indices + remaining_indices + [Y.ndim - 2, Y.ndim - 1]\n    Y_perm = Y.permute(*permute_order).contiguous()\n    \n\n    \n    if X.shape[:model_batch_ndim] != self.batch_shape:\n        X = X.expand(self.batch_shape + X.shape[-2:])  \n\n   \n    extra_dims = len(extra_indices) + len(remaining_indices)\n    for _ in range(extra_dims):\n        X = X.unsqueeze(model_batch_ndim)\n\n    \n    expand_shape = list(X.shape)\n    for i in range(extra_dims):\n        expand_shape[model_batch_ndim + i] = Y_perm.shape[model_batch_ndim + i]\n    X_expanded = X.expand(*expand_shape).contiguous()\n\n    \n    flat_size = int(torch.tensor(Y_perm.shape[model_batch_ndim:-1]).prod())\n    X_flat = X_expanded.reshape(*X_expanded.shape[:model_batch_ndim], flat_size, X_expanded.shape[-1]).clone()\n    Y_flat = Y_perm.reshape(*Y_perm.shape[:model_batch_ndim], flat_size, Y_perm.shape[-1]).clone()\n\n    return super().condition_on_observations(X_flat, Y_flat, **kwargs)\n</code></pre>\n"