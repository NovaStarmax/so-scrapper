# ğŸ“Š SO-Scrapper

Un projet de scraping de Stack Overflow pour extraire, stocker et analyser les derniÃ¨res tendances en **Data Science**. Le projet combine **scraping manuel**, **appel API**, **MongoDB**, **data mining**, **visualisation**, et **tests automatisÃ©s**.

---

## ğŸš€ Objectif

Ce projet a pour but de :
- RÃ©cupÃ©rer des donnÃ©es depuis Stack Overflow (scrapping manuel avec BeautifulSoup + API officielle),
- Stocker les donnÃ©es dans une base MongoDB locale,
- Nettoyer et analyser les textes,
- Visualiser les grandes tendances liÃ©es Ã  la Data Science.

---

## ğŸ—ï¸ Arborescence du projet

```
so-scrapper/
â”‚
â”œâ”€â”€ eda/                        # Notebooks d'analyse
â”‚   â”œâ”€â”€ datascience.ipynb       # Analyses des posts API
â”‚   â”œâ”€â”€ newest.ipynb            # Analyses des posts scrappÃ©s
â”‚   â””â”€â”€ text_mining.py          # Classe de traitement NLP
â”‚
â”œâ”€â”€ handle_io/
â”‚   â”œâ”€â”€ api.json                # DonnÃ©es API localement stockÃ©es
â”‚   â”œâ”€â”€ scrapper.json           # DonnÃ©es Scrapper localement stockÃ©es
â”‚   â””â”€â”€ sync_db.py              # Fonction de synchronisation JSON <-> MongoDB
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api.py                  # RÃ©cupÃ©ration via API StackOverflow
â”‚   â”œâ”€â”€ database.py             # Connexion MongoDB + fonctions DB
â”‚   â””â”€â”€ scrapper.py             # Scraping manuel avec BeautifulSoup
â”‚
â”œâ”€â”€ tests/                      # Tests Pytest (API & Scrapper)
â”‚   â”œâ”€â”€ test_api.py
â”‚   â””â”€â”€ test_scrapper.py
â”‚
â”œâ”€â”€ main.py                     # Script de synchronisation entre MongoDB et JSON
â”œâ”€â”€ setup.py                    # Script de setup initial (DB, index, nltk)
â”œâ”€â”€ .env                        # Variables d'environnement (Mongo URI, DB, etc.)
â”œâ”€â”€ pyproject.toml              # DÃ©pendances & config Pytest
â”œâ”€â”€ README.md
â””â”€â”€ uv.lock
```

---

## âš™ï¸ PrÃ©requis

- Python â‰¥ 3.10
- [`uv`](https://github.com/astral-sh/uv) pour gÃ©rer les dÃ©pendances
- [MongoDB](https://www.mongodb.com/try/download/community) installÃ© localement

---

## ğŸ› ï¸ Installation

1. Clone le repo :
   ```bash
   git clone https://github.com/tonio/so-scrapper.git
   cd so-scrapper
   ```

2. CrÃ©e un environnement et installe les dÃ©pendances :
   ```bash
   uv sync
   # or
   uv pip install -r requirements.txt  # ou utilise pyproject.toml avec uv install
   ```

3. Remplis ton fichier `.env` :
   ```env
   CLIENT=mongodb://localhost:27017/
   DB=so_scrapper_db
   COLLECTION_API=api
   COLLECTION_SCRAPPER=scrapper
   ```

4. Lance le setup initial :
   ```bash
   uv run setup.py
   ```

   > Cela crÃ©e les collections MongoDB nÃ©cessaires, les indexes uniques sur `"link"`, synchronise les fichiers `*.json`, et tÃ©lÃ©charge les ressources NLTK (`stopwords`, `wordnet`).

---

## ğŸ“¦ Commandes principales

- Scraping manuel :
  ```bash
  uv run src/scrapper.py
  ```

- RÃ©cupÃ©ration via API :
  ```bash
  uv run src/api.py
  ```

- Lancer les tests :
  ```bash
  uv run pytest -m scrapper
  uv run pytest -m api
  ```

---

## ğŸ§ª EDA et Text Mining

- Lâ€™analyse exploratoire est faite dans `eda/`
- La classe `TextMining` (`text_mining.py`) permet :
  - Nettoyage
  - Tokenisation
  - Suppression des stopwords
  - Lemmatisation
  - Extraction de mots-clÃ©s
  - GÃ©nÃ©ration de nuages de mots et graphes

---

## ğŸ“Œ Ã€ venir

- Export dâ€™un dashboard interactif
- DÃ©ploiement sur un service cloud

---

## ğŸ“ Contexte pÃ©dagogique

Ce projet a Ã©tÃ© rÃ©alisÃ© dans un cadre **pÃ©dagogique** et mobilise :

- ğŸ§© **Scraping Web** avec `BeautifulSoup` et `requests`
- ğŸ› ï¸ **MongoDB** pour stocker les donnÃ©es
- ğŸ“Š **Exploration et visualisation** avec `matplotlib`, `seaborn`, `wordcloud`, `networkx`
- ğŸ§ª **Tests unitaires** avec `pytest` et `pytest.mark`
- ğŸ§  **Text mining** et prÃ©traitement NLP avec `NLTK`, `sklearn`
- âš™ï¸ Gestion de projet Python avec `uv`, `dotenv`, `pyproject.toml`